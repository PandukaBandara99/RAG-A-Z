{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Simple Workflow or Graph Using LangGraph\n",
    "\n",
    "**State**\n",
    "First, define the State of the graph.\n",
    "\n",
    "The State schema serves as the input schema for all Nodes and Edges in the graph.\n",
    "\n",
    "Let's use the TypedDict class from python's typing module as our schema, which provides type hints for the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "class State(TypedDict):\n",
    "    graph_info:str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes\n",
    "Nodes are just python functions.\n",
    "\n",
    "The first positional argument is the state, as defined above.\n",
    "\n",
    "Because the state is a TypedDict with schema as defined above, each node can access the key, graph_state, with state['graph_state'].\n",
    "\n",
    "Each node returns a new value of the state key graph_state.\n",
    "\n",
    "By default, the new value returned by each node will override the prior state value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_play(state:State):\n",
    "    print(\"Start_Play node has been called\")\n",
    "    return {\"graph_info\":state['graph_info'] + \" I am planning to play\"}\n",
    "\n",
    "def cricket(state:State):\n",
    "    print(\"My Cricket node has been called\")\n",
    "    return {\"graph_info\":state['graph_info'] + \" Cricket\"}\n",
    "\n",
    "def badminton(state:State):\n",
    "    print(\"My badminton node has been called\")\n",
    "    return {\"graph_info\":state['graph_info'] + \" Badminton\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Literal\n",
    "\n",
    "def random_play(state:State)-> Literal['cricket','badminton']:\n",
    "    graph_info=state['graph_info']\n",
    "\n",
    "    if random.random()>0.5:\n",
    "        return \"cricket\"\n",
    "    else:\n",
    "        return \"badminton\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Construction\n",
    "Now, we build the graph from our components defined above.\n",
    "\n",
    "The StateGraph class is the graph class that we can use.\n",
    "\n",
    "First, we initialize a StateGraph with the State class we defined above.\n",
    "\n",
    "Then, we add our nodes and edges.\n",
    "\n",
    "We use the START Node, a special node that sends user input to the graph, to indicate where to start our graph.\n",
    "\n",
    "The END Node is a special node that represents a terminal node.\n",
    "\n",
    "Finally, we compile our graph to perform a few basic checks on the graph structure.\n",
    "\n",
    "We can visualize the graph as a Mermaid diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image,display\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "\n",
    "## Build Graph\n",
    "graph=StateGraph(State)\n",
    "\n",
    "## Adding the nodes\n",
    "graph.add_node(\"start_play\",start_play)\n",
    "graph.add_node(\"cricket\",cricket)\n",
    "graph.add_node(\"badminton\",badminton)\n",
    "\n",
    "## Schedule the flow of the graph\n",
    "\n",
    "graph.add_edge(START,\"start_play\")\n",
    "graph.add_conditional_edges(\"start_play\",random_play)\n",
    "graph.add_edge(\"cricket\",END)\n",
    "graph.add_edge(\"badminton\",END)\n",
    "\n",
    "## Compile the graph\n",
    "graph_builder=graph.compile()\n",
    "\n",
    "## View\n",
    "display(Image(graph_builder.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAPH Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start_Play node has been called\n",
      "My Cricket node has been called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'graph_info': 'Hey My name is Krish I am planning to play Cricket'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.invoke({\"graph_info\":\"Hey My name is Krish\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction to LangGraph**\n",
    "\n",
    "LangGraph is an evolution of the LangChain framework that brings **graph-based orchestration** to large language model (LLM) workflows. It allows developers to design **modular, dynamic, and stateful AI applications** using a **node-and-edge execution model**, enabling more control, flexibility, and reliability compared to traditional linear chain structures.\n",
    "\n",
    "**What is LangGraph?**\n",
    "\n",
    "**LangGraph** is a **graph-based framework** built on top of LangChain that enables you to define and execute **complex, multi-agent or multi-step reasoning workflows** using **directed graphs**.\n",
    "\n",
    "Each **node** represents a computation (such as an LLM call, retriever, or custom function), and **edges** define the **flow of execution** between nodes.\n",
    "\n",
    "LangGraph gives developers fine-grained control over:\n",
    "\n",
    "* How data flows between steps.\n",
    "* When nodes execute (based on conditions or events).\n",
    "* How to handle state and memory across a long conversation or computation.\n",
    "\n",
    "LangGraph can be thought of as **“LangChain + Graph Execution + Persistent State”**, making it ideal for **RAG pipelines, autonomous agents, and decision-driven reasoning systems.**\n",
    "\n",
    "---\n",
    "\n",
    "**The Motivation Behind LangGraph**\n",
    "\n",
    "LangChain’s **Sequential Chains** and **Agent Executors** are powerful but limited:\n",
    "\n",
    "* They assume a **linear execution flow**.\n",
    "* Handling **branching logic**, **loops**, or **stateful agents** requires complex custom code.\n",
    "* Scaling to multi-agent workflows or conditional execution becomes cumbersome.\n",
    "\n",
    "LangGraph solves these challenges by introducing a **graph execution engine** that allows:\n",
    "\n",
    "1. **Conditional routing** – e.g., if the model is unsure, send the query to a secondary verifier.\n",
    "2. **Parallel execution** – run multiple LLMs or retrievers simultaneously.\n",
    "3. **Persistent state management** – carry context across multiple graph steps or sessions.\n",
    "4. **Event-based orchestration** – trigger actions based on model responses or external inputs.\n",
    "\n",
    "In short, LangGraph gives developers **a declarative way to design LLM applications** that resemble **workflow engines** more than simple pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "**LangChain vs. LangGraph**\n",
    "\n",
    "| Feature                | **LangChain**                      | **LangGraph**                                   |\n",
    "| ---------------------- | ---------------------------------- | ----------------------------------------------- |\n",
    "| **Execution Model**    | Linear or nested chains            | Directed graph with nodes and edges             |\n",
    "| **State Management**   | Transient (limited to memory type) | Persistent and shared across nodes              |\n",
    "| **Branching Logic**    | Hard-coded                         | Natively supported                              |\n",
    "| **Concurrency**        | Sequential                         | Parallel or asynchronous                        |\n",
    "| **Complexity**         | Best for simple pipelines          | Best for dynamic, multi-agent systems           |\n",
    "| **Use Case Example**   | Question answering, summarization  | Multi-step reasoning, RAG + planning agents     |\n",
    "| **Memory Integration** | Local or chain-level               | Global graph memory with node-level updates     |\n",
    "| **Error Handling**     | Manual                             | Built-in retries, fallbacks, and event triggers |\n",
    "\n",
    "LangGraph doesn’t replace LangChain — it extends it.\n",
    "You still use **LangChain components** (like `LLMChain`, `Retrievers`, `PromptTemplates`, etc.) inside LangGraph **nodes**. The difference is **how you connect and execute** them.\n",
    "\n",
    "---\n",
    "\n",
    "**Real-World Applications and Use Cases**\n",
    "\n",
    "LangGraph is designed for **complex AI systems** where linear execution isn’t enough.\n",
    "\n",
    "1. **Autonomous Multi-Agent Systems**\n",
    "\n",
    "* Multiple agents collaborate and share state via the graph.\n",
    "* Example: Researcher agent → Retriever agent → Summarizer agent → Evaluator agent.\n",
    "\n",
    "2. **Dynamic RAG Pipelines**\n",
    "\n",
    "* Different retrieval or reasoning paths triggered by query type.\n",
    "* Example: Technical query → Documentation retriever; General query → Knowledge summarizer.\n",
    "\n",
    "3. **Customer Support Bots**\n",
    "\n",
    "* Manage long conversations with context, decision points, and fallbacks.\n",
    "* Example: “Escalate to human” branch when confidence < threshold.\n",
    "\n",
    "4. **Workflow Automation**\n",
    "\n",
    "* Combine LLM reasoning with API calls, database queries, and conditional logic.\n",
    "* Example: Query → Generate SQL → Run Query → Explain Results.\n",
    "\n",
    "5. **Multi-Modal and Multi-Model Systems**\n",
    "\n",
    "* Integrate text, image, or code models in one orchestrated graph.\n",
    "* Example: CLIP-based image retrieval → LLM-based captioning → Summarization.\n",
    "\n",
    "---\n",
    "**Core Concepts**\n",
    "\n",
    "LangGraph introduces several new primitives for graph-based reasoning.\n",
    "\n",
    "**Nodes**\n",
    "\n",
    "A **node** is a discrete unit of computation. It can represent:\n",
    "\n",
    "* An LLM call (`ChatOpenAI`, `ChatAnthropic`)\n",
    "* A tool or API call\n",
    "* A retriever or database query\n",
    "* A custom Python function\n",
    "\n",
    "Each node can receive **inputs** and produce **outputs**, which are passed through edges to other nodes.\n",
    "\n",
    "```python\n",
    "from langgraph.graph import Node\n",
    "\n",
    "def summarize_text(inputs):\n",
    "    return {\"summary\": f\"Summary: {inputs['text'][:100]}...\"}\n",
    "\n",
    "summary_node = Node(name=\"summarizer\", func=summarize_text)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**Edges**\n",
    "\n",
    "Edges connect nodes and define how outputs from one node become inputs to the next.\n",
    "Edges can include **conditions**, allowing for **dynamic branching**.\n",
    "\n",
    "```python\n",
    "graph.add_edge(\"retriever\", \"generator\", condition=lambda x: len(x['docs']) > 0)\n",
    "```\n",
    "\n",
    "Edges are what transform LangGraph from a simple chain into a **decision-capable network**.\n",
    "\n",
    "\n",
    "\n",
    "**Graph Execution**\n",
    "\n",
    "Graphs are executed by a **GraphExecutor**, which manages:\n",
    "\n",
    "* Node dependencies\n",
    "* Execution order\n",
    "* Parallelism\n",
    "* State propagation\n",
    "\n",
    "```python\n",
    "from langgraph.graph import Graph\n",
    "\n",
    "graph = Graph()\n",
    "graph.add_node(\"retriever\", retriever_node)\n",
    "graph.add_node(\"generator\", generator_node)\n",
    "graph.add_edge(\"retriever\", \"generator\")\n",
    "\n",
    "result = graph.run({\"query\": \"Explain LangGraph\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**State Management and Transitions**\n",
    "\n",
    "LangGraph introduces a **shared state store**, where nodes can:\n",
    "\n",
    "* Read and update state variables.\n",
    "* Retain context between executions.\n",
    "* Transition state based on events (e.g., user input or model output).\n",
    "\n",
    "This allows for **persistent reasoning**, where an agent can “remember” facts or past interactions across multiple runs.\n",
    "\n",
    "```python\n",
    "state.update({\"conversation_history\": new_messages})\n",
    "```\n",
    "\n",
    "State transitions define **when and how the graph moves forward** — essential for **multi-turn dialogues** or **multi-step reasoning**.\n",
    "\n",
    "---\n",
    "\n",
    "**Graph Memory vs. LangChain Memory**\n",
    "\n",
    "| Feature          | **LangChain Memory**              | **LangGraph Memory**                                  |\n",
    "| ---------------- | --------------------------------- | ----------------------------------------------------- |\n",
    "| **Scope**        | Per-chain or per-agent            | Global across the graph                               |\n",
    "| **Persistence**  | Typically in-memory               | Can be persisted (e.g., Redis, Chroma)                |\n",
    "| **Usage**        | Short-term conversational context | Full state persistence and coordination               |\n",
    "| **Granularity**  | Tracks messages                   | Tracks variables, documents, and intermediate results |\n",
    "| **Applications** | Chatbots                          | Multi-agent RAG systems, workflows                    |\n",
    "\n",
    "LangGraph’s **graph memory** is more structured and long-lived, making it ideal for **autonomous systems** that evolve state over time.\n",
    "\n",
    "---\n",
    "\n",
    "**Event-Driven Architecture**\n",
    "\n",
    "LangGraph supports an **event-driven model**, allowing external triggers or node outputs to **fire specific actions or transitions** in the graph.\n",
    "\n",
    "Event Types:\n",
    "\n",
    "* **On Node Success / Failure** → Trigger retries or alternate paths.\n",
    "* **On User Input** → Resume paused graph with new context.\n",
    "* **On Timer / External Event** → Schedule future tasks or agent handoffs.\n",
    "\n",
    "This event model enables **asynchronous, real-time** agent workflows:\n",
    "\n",
    "* Example: Agent waits for new data or user response before continuing.\n",
    "* Example: If retrieval fails, trigger a “search expansion” node automatically.\n",
    "\n",
    "```python\n",
    "graph.on_event(\"no_results\", lambda: graph.run({\"query\": \"Rephrase and retry\"}))\n",
    "```\n",
    "\n",
    "This architecture makes LangGraph suitable for **long-running AI processes** like research pipelines, monitoring agents, and conversational memory systems.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* **LangGraph = LangChain + Graph-Oriented Workflow Engine**\n",
    "* It introduces **nodes**, **edges**, **graph memory**, and **event-driven execution**.\n",
    "* Ideal for **multi-agent**, **multi-step**, **stateful**, and **dynamic reasoning pipelines**.\n",
    "* Provides **parallelism**, **conditional routing**, and **persistent state**, far beyond what linear chains can achieve.\n",
    "* Perfect for advanced systems such as **multi-model RAGs**, **autonomous research agents**, and **AI orchestration frameworks**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction to Groq**\n",
    "\n",
    "Groq is a **next-generation AI acceleration platform** designed to deliver **ultra-low latency inference** for large language models (LLMs), deep learning, and data processing workloads. Unlike traditional GPUs or CPUs, Groq uses a **novel architecture** optimized for deterministic, massively parallel computation — enabling **real-time inference** at unmatched speeds and efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "**What is Groq?**\n",
    "\n",
    "Groq is both:\n",
    "\n",
    "1. A **hardware company** that produces the **GroqChip™** — a tensor-streaming processor built for AI workloads.\n",
    "2. A **software platform (GroqCloud)** that enables **instant inference-as-a-service**, allowing developers to run models like **Llama, Mistral, and Whisper** directly on Groq hardware with sub-millisecond latency.\n",
    "\n",
    "Groq’s main focus is **accelerating LLMs and AI inference**, not training. The platform is designed to serve **chatbots, RAG pipelines, autonomous systems, and edge AI** with extremely low latency and high throughput.\n",
    "\n",
    "---\n",
    "\n",
    "**The Groq Architecture**\n",
    "\n",
    "Groq’s architecture is fundamentally different from traditional accelerators:\n",
    "\n",
    "1. **Tensor Streaming Processor (TSP)**\n",
    "\n",
    "* The **GroqChip™** is based on a **deterministic, single-core tensor-streaming processor**.\n",
    "* Unlike GPUs (which use thousands of threads with dynamic scheduling), Groq executes all instructions **in a predictable sequence**, ensuring **zero kernel launch overhead**.\n",
    "* This design makes Groq ideal for **low-latency inference** rather than high-throughput training.\n",
    "\n",
    "2. **Dataflow Execution Model**\n",
    "\n",
    "* Groq uses a **dataflow architecture**, where computations are represented as graphs of operations that “flow” through the processor.\n",
    "* This allows for **pipeline parallelism** — every stage of computation runs simultaneously on different parts of the chip.\n",
    "\n",
    "3. **On-Chip Memory and Determinism**\n",
    "\n",
    "* All memory accesses are **pre-scheduled**, so there are no cache misses or unpredictable stalls.\n",
    "* This ensures **consistent latency**, making Groq ideal for **real-time systems**.\n",
    "\n",
    "4. **Linear Scalability**\n",
    "\n",
    "* Groq chips can be connected in clusters where performance scales **linearly** with the number of chips.\n",
    "* Ideal for **large-scale inference workloads** across distributed data centers.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Groq is Revolutionary**\n",
    "\n",
    "| Feature               | **Groq**                                    | **GPU (e.g., NVIDIA)**    | **CPU**                 |\n",
    "| --------------------- | ------------------------------------------- | ------------------------- | ----------------------- |\n",
    "| **Architecture Type** | Deterministic, single-core tensor streaming | Parallel, thread-based    | General-purpose         |\n",
    "| **Primary Use Case**  | AI inference                                | AI training + inference   | General computation     |\n",
    "| **Latency**           | Microseconds–milliseconds                   | Milliseconds–seconds      | High                    |\n",
    "| **Throughput**        | Consistent and predictable                  | Varies by workload        | Low                     |\n",
    "| **Scalability**       | Linear                                      | Depends on interconnect   | Moderate                |\n",
    "| **Power Efficiency**  | High                                        | Moderate                  | Low                     |\n",
    "| **Programming Model** | Static dataflow graphs                      | Dynamic kernel scheduling | Sequential instructions |\n",
    "\n",
    "Groq’s **deterministic execution** means that every inference will always take the same amount of time, regardless of system load — a major advantage for **real-time AI** (e.g., conversational agents, RAG pipelines, or trading systems).\n",
    "\n",
    "---\n",
    "\n",
    "**Groq Software Stack**\n",
    "\n",
    "Groq provides a complete developer ecosystem to deploy and scale models efficiently:\n",
    "\n",
    "1. **Groq Compiler**\n",
    "\n",
    "* Converts machine learning models into optimized tensor graphs for the GroqChip.\n",
    "* Performs **graph-level optimizations**, unrolling loops and scheduling tensor operations to eliminate runtime overhead.\n",
    "\n",
    "2. **Groq API**\n",
    "\n",
    "* Provides Python and REST interfaces for model deployment.\n",
    "* Allows developers to connect models directly from Hugging Face or LangChain-based applications.\n",
    "\n",
    "3. **GroqCloud**\n",
    "\n",
    "* Groq’s cloud inference platform where you can **run models instantly** without provisioning GPUs.\n",
    "* Supports models like **Llama 3, Mistral 7B, Whisper, and T5**.\n",
    "* Offers **streaming inference** for chatbots and RAG (Retrieval-Augmented Generation) applications.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```python\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=\"your_api_key\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain Groq in simple terms.\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "This API structure is almost identical to OpenAI’s, making migration frictionless.\n",
    "\n",
    "---\n",
    "\n",
    "**Groq in RAG (Retrieval-Augmented Generation)**\n",
    "\n",
    "Groq accelerates RAG systems by reducing latency in two critical areas:\n",
    "\n",
    "1. **Vector Search + Retrieval**\n",
    "\n",
    "   * Rapid retrieval of top-k embeddings from databases like **Pinecone**, **Chroma**, or **FAISS**.\n",
    "2. **LLM Inference**\n",
    "\n",
    "   * Instant generation using low-latency models deployed on GroqCloud.\n",
    "\n",
    "In a LangChain RAG pipeline:\n",
    "\n",
    "* You can integrate Groq as the LLM backend via the `ChatGroq` wrapper.\n",
    "* This allows for **real-time conversational RAG**, where user queries, retrieval, and response generation all occur in **milliseconds**.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\", api_key=\"your_groq_api_key\")\n",
    "\n",
    "response = llm.invoke(\"What are the advantages of Groq over GPUs?\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "This integration can be combined with LangChain **retrievers** and **vector stores** to build ultra-fast RAG systems.\n",
    "\n",
    "---\n",
    "\n",
    "**Security and Data Privacy**\n",
    "\n",
    "* GroqCloud offers **secure API access** with encrypted data transmission.\n",
    "* No data persistence: input and output are not stored unless explicitly configured.\n",
    "* Supports **enterprise-grade compliance** for data-sensitive workloads (e.g., healthcare, finance).\n",
    "\n",
    "---\n",
    "\n",
    "**Performance Benchmarks**\n",
    "\n",
    "Groq achieves **sub-millisecond inference latency** for models like:\n",
    "\n",
    "* **Llama 3 70B** — 30–40 tokens/s per user stream.\n",
    "* **Whisper** — 50× faster-than-real-time transcription.\n",
    "* **Mistral 7B** — 80–100 tokens/s with deterministic timing.\n",
    "\n",
    "This makes Groq ideal for:\n",
    "\n",
    "* **Conversational AI (chatbots, voice assistants)**\n",
    "* **Real-time RAG pipelines**\n",
    "* **Autonomous systems** (robots, trading bots)\n",
    "* **High-frequency data processing**\n",
    "\n",
    "---\n",
    "\n",
    "**Limitations and Challenges**\n",
    "\n",
    "While Groq is powerful, it’s not a universal solution.\n",
    "\n",
    "| Limitation                   | Explanation                                                                                                   |\n",
    "| ---------------------------- | ------------------------------------------------------------------------------------------------------------- |\n",
    "| **Model Support**            | Limited to supported transformer architectures (e.g., Llama, Mistral). Custom models may need re-compilation. |\n",
    "| **Not for Training**         | Groq is optimized for **inference only**; training still requires GPUs or TPUs.                               |\n",
    "| **Static Graph Requirement** | Models must be compiled into a fixed computation graph, reducing flexibility for dynamic shapes.              |\n",
    "| **Cloud-Centric**            | On-premises deployment options are currently limited.                                                         |\n",
    "| **Ecosystem Maturity**       | Compared to CUDA, developer tools and community support are still growing.                                    |\n",
    "\n",
    "---\n",
    "\n",
    "**Real-World Applications**\n",
    "\n",
    "1. **Conversational Agents** — Sub-millisecond response chatbots powered by Llama 3.\n",
    "2. **Streaming RAG Systems** — Real-time document retrieval and reasoning pipelines.\n",
    "3. **Autonomous Vehicles / Edge AI** — Deterministic execution for critical decision systems.\n",
    "4. **Finance and Trading** — Predictable inference for time-sensitive computations.\n",
    "5. **Healthcare AI** — Secure and low-latency inference for medical decision systems.\n",
    "\n",
    "---\n",
    "\n",
    " **Integration with LangChain and LangGraph**\n",
    "\n",
    "Groq integrates seamlessly with **LangChain** and **LangGraph** via custom LLM wrappers (`ChatGroq`).\n",
    "You can plug Groq models into any **LangChain pipeline**, **RAG retriever**, or **LangGraph node** to achieve:\n",
    "\n",
    "* Real-time multi-agent reasoning.\n",
    "* Event-driven RAG systems.\n",
    "* Deterministic multi-hop workflows.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* **Groq** redefines AI inference by replacing GPU-style parallelism with **deterministic tensor streaming**.\n",
    "* It enables **ultra-low latency**, **predictable**, and **scalable** LLM execution.\n",
    "* Ideal for **real-time**, **mission-critical**, and **interactive AI systems**.\n",
    "* Seamlessly integrates with frameworks like **LangChain**, **LangGraph**, and **RAG pipelines**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Understanding Graphs in LangGraph**\n",
    "\n",
    "LangGraph introduces a **graph-based programming model** for orchestrating and controlling the flow of **LLM-driven applications** (like RAG pipelines, multi-agent systems, or reasoning workflows).\n",
    "It borrows concepts from **graph theory**, **state machines**, and **event-driven systems** — making complex, branching AI logic easier to build, visualize, and debug.\n",
    "\n",
    "---\n",
    "\n",
    "**Graph Theory Basics Applied to LLM Workflows**\n",
    "\n",
    "**What is a Graph?**\n",
    "\n",
    "A **graph** in computer science consists of:\n",
    "\n",
    "* **Nodes (Vertices):** Represent computational units (like functions, models, or agents).\n",
    "* **Edges:** Represent the flow of information or control between nodes.\n",
    "\n",
    "In LangGraph:\n",
    "\n",
    "* Each **Node** = a *computational step* (e.g., a prompt to an LLM, a retrieval call, or a decision rule).\n",
    "* Each **Edge** = defines *what happens next* depending on outputs or conditions.\n",
    "\n",
    "💡 **Analogy:**\n",
    "Think of your AI application as a **conversation map** — every node is a thought, and every edge is a possible next thought.\n",
    "\n",
    "---\n",
    "\n",
    "Applying Graph Theory to LLM Workflows\n",
    "\n",
    "| Graph Concept        | LangGraph Equivalent             | Example                                                        |\n",
    "| -------------------- | -------------------------------- | -------------------------------------------------------------- |\n",
    "| **Node (Vertex)**    | A task or agent                  | “Summarize document”, “Generate query”, “Decide next step”     |\n",
    "| **Edge**             | Data or control flow             | “If summary incomplete → regenerate”, “Else → store to memory” |\n",
    "| **Directed Graph**   | Defined sequence of LLM actions  | “Retriever → Reranker → Generator”                             |\n",
    "| **Conditional Edge** | Branching logic                  | “If sentiment = negative → call mitigation agent”              |\n",
    "| **Path**             | Execution sequence               | Retriever → LLM → Evaluator                                    |\n",
    "| **Cycle**            | Feedback or self-reflection loop | LLM re-evaluates its output until condition met                |\n",
    "\n",
    "LangGraph uses these principles to define **deterministic**, **reproducible**, and **parallelizable** workflows.\n",
    "\n",
    "---\n",
    "\n",
    "**Directed and Conditional Edges**\n",
    "\n",
    "### **Directed Edges**\n",
    "\n",
    "* Represent a **one-way flow** of execution from one node to another.\n",
    "* Each edge defines *which node executes next* once the current one finishes.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Start → Retrieve Documents → Summarize → Output\n",
    "```\n",
    "\n",
    "Here, the graph defines a *fixed* order: once the `Retrieve` node finishes, it always moves to `Summarize`.\n",
    "\n",
    "---\n",
    "**Conditional Edges**\n",
    "\n",
    "* Add **dynamic decision-making** to the graph.\n",
    "* The next node is chosen based on a **condition** or **node output**.\n",
    "* These edges make the graph **non-linear**, enabling branching or looping.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Analyze → (if \"needs fact-checking\") → VerifyFacts\n",
    "         → (else) → FinalizeAnswer\n",
    "```\n",
    "\n",
    "LangGraph supports **conditional transitions** using `ConditionalEdge`, which routes control flow based on runtime results — similar to `if-else` logic in programming.\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph()\n",
    "\n",
    "@graph.node()\n",
    "def classify(input_text):\n",
    "    if \"error\" in input_text:\n",
    "        return \"handle_error\"\n",
    "    else:\n",
    "        return \"process_text\"\n",
    "\n",
    "@graph.node()\n",
    "def handle_error(_):\n",
    "    return \"Error handled\"\n",
    "\n",
    "@graph.node()\n",
    "def process_text(_):\n",
    "    return \"Processed successfully\"\n",
    "\n",
    "graph.edge(\"classify\", \"handle_error\", condition=lambda x: \"error\" in x)\n",
    "graph.edge(\"classify\", \"process_text\", condition=lambda x: \"error\" not in x)\n",
    "graph.edge(\"handle_error\", END)\n",
    "graph.edge(\"process_text\", END)\n",
    "```\n",
    "\n",
    "Here, the **conditional edge** determines whether the workflow routes to an error handler or a processing function.\n",
    "\n",
    "---\n",
    "\n",
    "**Node Execution Flow**\n",
    "\n",
    "Each **node** in LangGraph represents an **atomic operation** — often a function, an LLM call, or a retrieval step.\n",
    "The **execution flow** defines how data moves through nodes and how the graph decides what happens next.\n",
    "\n",
    "\n",
    "**Step-by-Step Flow**\n",
    "\n",
    "1. **Initialization**\n",
    "\n",
    "   * The graph starts from an entry node (e.g., `\"start\"`).\n",
    "   * Initial input (state) is passed to this node.\n",
    "\n",
    "2. **Node Execution**\n",
    "\n",
    "   * The node executes its logic (e.g., prompts an LLM, processes data).\n",
    "   * It can update the shared state (like conversation history, retrieved docs, or context).\n",
    "\n",
    "3. **Transition Decision**\n",
    "\n",
    "   * Once the node finishes, the graph evaluates **edges** from that node.\n",
    "   * Depending on conditions, one or more edges trigger the next nodes.\n",
    "\n",
    "4. **State Update**\n",
    "\n",
    "   * The state (a shared object) carries forward through nodes.\n",
    "   * Each node can read/write from this shared context — similar to memory passing.\n",
    "\n",
    "5. **Termination or Loop**\n",
    "\n",
    "   * The graph continues traversing edges until it reaches a terminal node (e.g., `END`)\n",
    "   * Or loops back (for self-reflection, retry, or iteration).\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Multi-Step RAG Flow**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph()\n",
    "\n",
    "@graph.node()\n",
    "def retrieve(state):\n",
    "    state[\"docs\"] = [\"AI is the simulation of human intelligence.\"]\n",
    "    return state\n",
    "\n",
    "@graph.node()\n",
    "def generate_answer(state):\n",
    "    state[\"answer\"] = f\"Answer: {state['docs'][0]}\"\n",
    "    return state\n",
    "\n",
    "@graph.node()\n",
    "def evaluate(state):\n",
    "    if \"AI\" in state[\"answer\"]:\n",
    "        state[\"valid\"] = True\n",
    "        return \"END\"\n",
    "    else:\n",
    "        return \"generate_answer\"\n",
    "\n",
    "graph.edge(\"retrieve\", \"generate_answer\")\n",
    "graph.edge(\"generate_answer\", \"evaluate\")\n",
    "graph.edge(\"evaluate\", END)\n",
    "\n",
    "workflow = graph.compile()\n",
    "\n",
    "final_state = workflow.invoke({})\n",
    "print(final_state[\"answer\"])\n",
    "```\n",
    "\n",
    "**Execution Flow:**\n",
    "`retrieve → generate_answer → evaluate → END`\n",
    "\n",
    "Each node reads/writes to `state`, and edges define the **execution path**.\n",
    "\n",
    "---\n",
    "\n",
    "**Parallel Execution**\n",
    "\n",
    "LangGraph can execute **multiple nodes in parallel** if they don’t depend on each other — leveraging concurrency for efficiency.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Retrieve Docs ─┬─> Summarize\n",
    "                └─> Extract Keywords\n",
    "```\n",
    "\n",
    "Both can execute simultaneously before merging their outputs.\n",
    "\n",
    "\n",
    "\n",
    "**Loops and Re-evaluation**\n",
    "\n",
    "Graphs can contain **feedback loops**, allowing iterative refinement:\n",
    "\n",
    "```\n",
    "Generate → Evaluate → (if poor quality) → Regenerate\n",
    "```\n",
    "\n",
    "This supports **reflection-based LLM systems**, similar to “agent self-correction.”\n",
    "\n",
    "---\n",
    "\n",
    "**Visualizing the Flow**\n",
    "\n",
    "LangGraph supports visualization (via `.get_graph()` or `graphviz`) so you can see how nodes connect:\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "workflow.get_graph().draw(\"workflow.png\")\n",
    "```\n",
    "\n",
    "Output visualization:\n",
    "\n",
    "```\n",
    "[Retrieve] → [Generate] → [Evaluate] → [END]\n",
    "```\n",
    "\n",
    "Such diagrams help debug logic and optimize flow.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Graphs Work So Well for LLM Pipelines**\n",
    "\n",
    "| Challenge in LLM Workflows | How Graphs Solve It                              |\n",
    "| -------------------------- | ------------------------------------------------ |\n",
    "| Multi-step reasoning       | Graph nodes separate reasoning into clear stages |\n",
    "| Conditional logic          | Directed + conditional edges handle branching    |\n",
    "| State handling             | Shared graph state manages memory and context    |\n",
    "| Error recovery             | Looping edges allow retry or fallback logic      |\n",
    "| Visualization              | Graph view helps understand and debug flow       |\n",
    "| Determinism                | Each path defines predictable execution order    |\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Concept              | Description                          | LangGraph Implementation              |\n",
    "| -------------------- | ------------------------------------ | ------------------------------------- |\n",
    "| **Graph**            | Workflow composed of connected nodes | `StateGraph()`                        |\n",
    "| **Node**             | Function / operation / model call    | `@graph.node()`                       |\n",
    "| **Directed Edge**    | Fixed transition between nodes       | `graph.edge(\"A\", \"B\")`                |\n",
    "| **Conditional Edge** | Decision-based transition            | `graph.edge(\"A\", \"B\", condition=...)` |\n",
    "| **Execution Flow**   | Order in which nodes run             | Defined by edges and conditions       |\n",
    "| **State**            | Shared data between nodes            | `state` object                        |\n",
    "| **End Node**         | Terminates flow                      | `END` constant                        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Graph Components in LangGraph**\n",
    "\n",
    "LangGraph is designed around **graph-based workflow orchestration**, where components such as **nodes**, **edges**, and **state management** define the behavior of an **LLM-driven application**.\n",
    "Understanding these components is essential to design scalable, interpretable, and fault-tolerant AI systems that combine **reasoning**, **tools**, and **data retrieval**.\n",
    "\n",
    "---\n",
    "\n",
    "**Node Types**\n",
    "\n",
    "Each **node** in LangGraph represents a computational step — similar to a function or module in a traditional program.\n",
    "Nodes are modular, allowing you to design workflows that mix **LLM calls**, **external tool executions**, and **logic-based decisions**.\n",
    "\n",
    "LangGraph primarily supports three conceptual node types:\n",
    "\n",
    "---\n",
    "\n",
    "A. **LLM Nodes**\n",
    "\n",
    "**Purpose:**\n",
    "Nodes that directly interact with **Large Language Models** (LLMs), such as GPT, Claude, or Gemini.\n",
    "These nodes are used for **reasoning**, **generation**, **summarization**, or **decision-making** based on natural language prompts.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "* Perform **prompt-based computation** (input → LLM → output).\n",
    "* Can use **context/state** (retrieved documents, previous messages).\n",
    "* May output structured results (JSON, text, or key-value pairs).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "graph = StateGraph()\n",
    "\n",
    "@graph.node()\n",
    "def summarize(state):\n",
    "    prompt = f\"Summarize the following text: {state['document']}\"\n",
    "    response = llm.invoke(prompt)\n",
    "    state[\"summary\"] = response\n",
    "    return state\n",
    "```\n",
    "\n",
    "💡 *Use Case:*\n",
    "LLM Nodes are central to tasks like **text summarization**, **semantic query reformulation**, or **chain-of-thought reasoning** in RAG systems.\n",
    "\n",
    "\n",
    "\n",
    "B. **Tool Nodes**\n",
    "\n",
    "**Purpose:**\n",
    "Nodes that **interface with external systems or APIs**, such as:\n",
    "\n",
    "* Databases (SQL, MongoDB)\n",
    "* Web services (REST APIs)\n",
    "* Vector stores (FAISS, Pinecone, Chroma)\n",
    "* Local utilities (Python REPL, calculators, file systems)\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "* Execute deterministic logic outside the LLM.\n",
    "* Typically return structured data.\n",
    "* Can run synchronously or asynchronously.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "@graph.node()\n",
    "def fetch_weather(state):\n",
    "    city = state[\"city\"]\n",
    "    data = requests.get(f\"https://api.weatherapi.com/{city}\").json()\n",
    "    state[\"weather\"] = data[\"current\"][\"temp_c\"]\n",
    "    return state\n",
    "```\n",
    "\n",
    "💡 *Use Case:*\n",
    "Tool nodes are used for **function calling**, **retrieval**, **computation**, or **external API integration** within a reasoning pipeline.\n",
    "\n",
    "\n",
    "\n",
    "C. **Decision Nodes**\n",
    "\n",
    "**Purpose:**\n",
    "Nodes that **evaluate conditions** and **route execution** dynamically based on logical or learned criteria.\n",
    "They act like **if-else** or **switch** statements within the graph, enabling **adaptive reasoning**.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "* Use LLM responses or computed values to decide next node.\n",
    "* Return a routing key (e.g., `\"yes\"`, `\"no\"`, `\"retry\"`).\n",
    "* Useful for error handling, validation, and reasoning branching.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "@graph.node()\n",
    "def validate_summary(state):\n",
    "    if len(state[\"summary\"].split()) < 50:\n",
    "        return \"regenerate\"\n",
    "    else:\n",
    "        return \"finalize\"\n",
    "```\n",
    "\n",
    " *Use Case:*\n",
    "Decision nodes power **conditional routing**, such as:\n",
    "\n",
    "* “If retrieval fails → retry.”\n",
    "* “If confidence < threshold → call fallback model.”\n",
    "* “If user intent = query → retrieve context; else → summarize.”\n",
    "\n",
    "---\n",
    "\n",
    "**Edges and Routing**\n",
    "\n",
    "Edges define **how nodes connect** and **how execution flows** through the graph.\n",
    "They are the “wiring” between components — determining sequence, conditions, and transitions.\n",
    "\n",
    "\n",
    "\n",
    "A. **Directed Edges**\n",
    "\n",
    "* Represent a **fixed, linear flow** between two nodes.\n",
    "* Used when the next operation is predetermined.\n",
    "\n",
    "```python\n",
    "graph.edge(\"summarize\", \"validate_summary\")\n",
    "```\n",
    "\n",
    "**Flow:**\n",
    "`Summarize → Validate Summary → Next Step`\n",
    "\n",
    "\n",
    "\n",
    "B. **Conditional Edges**\n",
    "\n",
    "* Route execution based on **output** or **state condition**.\n",
    "* Defined using a lambda or condition function.\n",
    "\n",
    "```python\n",
    "graph.edge(\"validate_summary\", \"regenerate\", condition=lambda state: len(state[\"summary\"].split()) < 50)\n",
    "graph.edge(\"validate_summary\", \"finalize\", condition=lambda state: len(state[\"summary\"].split()) >= 50)\n",
    "```\n",
    "\n",
    "**Flow:**\n",
    "If summary too short → Regenerate\n",
    "Else → Finalize\n",
    "\n",
    "\n",
    "\n",
    "C. **Dynamic Routing**\n",
    "\n",
    "Some nodes return **routing keys** directly — enabling **data-driven branching** without explicit condition functions.\n",
    "\n",
    "```python\n",
    "@graph.node()\n",
    "def classify_query(state):\n",
    "    if \"weather\" in state[\"query\"]:\n",
    "        return \"fetch_weather\"\n",
    "    else:\n",
    "        return \"search_facts\"\n",
    "```\n",
    "\n",
    "The returned string automatically determines the **next node** in the flow.\n",
    "\n",
    "💡 *Dynamic routing* is crucial for **multi-intent agents** or **workflow adaptivity**.\n",
    "\n",
    "---\n",
    "\n",
    "**Stateful vs. Stateless Execution**\n",
    "\n",
    "LangGraph workflows can be designed as **stateful** or **stateless**, depending on how data is shared across nodes.\n",
    "\n",
    "---\n",
    "\n",
    "A. **Stateful Execution**\n",
    "\n",
    "**Definition:**\n",
    "Each node reads from and writes to a shared **graph state object** that persists across the entire workflow.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "* State acts like **global memory** (similar to context or blackboard pattern).\n",
    "* Each node can modify, extend, or prune data in the state.\n",
    "* Enables long-running, memory-aware agents.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "state = {\n",
    "    \"query\": \"Explain LangGraph\",\n",
    "    \"docs\": [],\n",
    "    \"summary\": \"\"\n",
    "}\n",
    "```\n",
    "\n",
    "All nodes modify this shared object:\n",
    "\n",
    "* Retriever adds `\"docs\"`\n",
    "* LLM adds `\"summary\"`\n",
    "* Evaluator adds `\"confidence_score\"`\n",
    "\n",
    "💡 *Use Case:*\n",
    "Stateful design powers **context retention**, **multi-hop reasoning**, and **RAG pipelines** that rely on persistent intermediate results.\n",
    "\n",
    "\n",
    "B. **Stateless Execution**\n",
    "\n",
    "**Definition:**\n",
    "Each node operates independently — input and output are passed directly between nodes without global state.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "* Simpler and faster for small, isolated tasks.\n",
    "* No shared memory or context persistence.\n",
    "* Best for **single-step** or **deterministic pipelines**.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "@graph.node()\n",
    "def normalize(text):\n",
    "    return text.lower()\n",
    "```\n",
    "\n",
    "💡 *Use Case:*\n",
    "Used in **data preprocessing**, **embedding generation**, or **short-lived agents** where state continuity is unnecessary.\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison: Stateful vs. Stateless**\n",
    "\n",
    "| Feature              | Stateful                            | Stateless             |\n",
    "| -------------------- | ----------------------------------- | --------------------- |\n",
    "| **Data Persistence** | Shared across nodes                 | Independent per node  |\n",
    "| **Complexity**       | Higher (requires state management)  | Simpler               |\n",
    "| **Use Case**         | RAG, agents, memory-based reasoning | Simple pipelines      |\n",
    "| **Performance**      | Slightly slower                     | Faster                |\n",
    "| **Fault Tolerance**  | Easier to checkpoint and resume     | Easier to parallelize |\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Full Graph with All Components**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "graph = StateGraph()\n",
    "\n",
    "@graph.node()\n",
    "def retrieve(state):\n",
    "    state[\"docs\"] = [\"LangGraph is a framework for graph-based LLM orchestration.\"]\n",
    "    return state\n",
    "\n",
    "@graph.node()\n",
    "def summarize(state):\n",
    "    state[\"summary\"] = llm.invoke(f\"Summarize: {state['docs'][0]}\")\n",
    "    return state\n",
    "\n",
    "@graph.node()\n",
    "def validate_summary(state):\n",
    "    if len(state[\"summary\"].split()) < 10:\n",
    "        return \"regenerate\"\n",
    "    return \"finalize\"\n",
    "\n",
    "@graph.node()\n",
    "def regenerate(state):\n",
    "    state[\"summary\"] = llm.invoke(f\"Expand summary: {state['summary']}\")\n",
    "    return state\n",
    "\n",
    "@graph.node()\n",
    "def finalize(state):\n",
    "    print(\"✅ Final Summary:\", state[\"summary\"])\n",
    "    return state\n",
    "\n",
    "graph.edge(\"retrieve\", \"summarize\")\n",
    "graph.edge(\"summarize\", \"validate_summary\")\n",
    "graph.edge(\"validate_summary\", \"regenerate\")\n",
    "graph.edge(\"validate_summary\", \"finalize\")\n",
    "graph.edge(\"finalize\", END)\n",
    "graph.edge(\"regenerate\", \"validate_summary\")\n",
    "\n",
    "workflow = graph.compile()\n",
    "workflow.invoke({})\n",
    "```\n",
    "\n",
    "**Execution Flow:**\n",
    "\n",
    "1. **Retrieve** document →\n",
    "2. **Summarize** using LLM →\n",
    "3. **Validate** →\n",
    "\n",
    "   * If too short → Regenerate → Retry\n",
    "   * Else → Finalize\n",
    "\n",
    "This example demonstrates:\n",
    "\n",
    "* **LLM nodes** (`summarize`, `regenerate`)\n",
    "* **Decision node** (`validate_summary`)\n",
    "* **Directed & conditional edges**\n",
    "* **Stateful execution** with shared context\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Concept                 | Description                              | Example                              |\n",
    "| ----------------------- | ---------------------------------------- | ------------------------------------ |\n",
    "| **LLM Node**            | Performs language model operations       | `summarize`, `generate`              |\n",
    "| **Tool Node**           | Executes deterministic code or API calls | `fetch_weather`, `search_db`         |\n",
    "| **Decision Node**       | Evaluates conditions and routes flow     | `validate_summary`, `classify_query` |\n",
    "| **Directed Edge**       | Fixed transition between nodes           | `graph.edge(\"A\", \"B\")`               |\n",
    "| **Conditional Edge**    | Branches flow dynamically                | `condition=lambda s: s[\"flag\"]`      |\n",
    "| **Stateful Execution**  | Persistent shared memory                 | RAG workflows                        |\n",
    "| **Stateless Execution** | Independent node operation               | Preprocessing pipelines              |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **State Management in LangGraph**\n",
    "\n",
    "LangGraph’s true power lies in its **state management system**, which enables dynamic, context-aware workflows across multiple nodes.\n",
    "Unlike linear pipelines, LangGraph treats the **state** as a *shared memory object* that evolves as data passes between nodes — allowing for complex reasoning, decision-making, and adaptive behavior.\n",
    "\n",
    "---\n",
    "\n",
    "**The Role of State in LangGraph**\n",
    "\n",
    "In LangGraph, **state** acts as the single source of truth that represents the current context of execution.\n",
    "It holds all the intermediate results, inputs, and metadata as the graph progresses.\n",
    "\n",
    "Every node **reads from** and **writes to** the shared state, allowing data to persist and flow seamlessly between steps.\n",
    "\n",
    "**Analogy:**\n",
    "Think of the state as a *global whiteboard* — each node can read what's written, add new notes, or erase/update old ones before passing it on.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Responsibilities of the State:**\n",
    "\n",
    "1. **Data Sharing Between Nodes**\n",
    "\n",
    "   * Passes relevant data such as prompts, embeddings, or tool outputs across connected components.\n",
    "   * Eliminates the need for external variables or global storage.\n",
    "\n",
    "2. **Context Management**\n",
    "\n",
    "   * Stores user inputs, retrieved documents, intermediate reasoning, and decisions.\n",
    "   * Enables persistent context for long-running conversations or multi-hop reasoning.\n",
    "\n",
    "3. **Checkpointing and Recovery**\n",
    "\n",
    "   * Can be serialized or logged at any stage.\n",
    "   * Facilitates workflow resumption, debugging, and auditability.\n",
    "\n",
    "4. **Dynamic Adaptation**\n",
    "\n",
    "   * The state can evolve based on conditions, allowing nodes to modify or redirect execution dynamically.\n",
    "\n",
    "\n",
    "\n",
    "### **Example of State Flow:**\n",
    "\n",
    "```python\n",
    "state = {\n",
    "    \"user_query\": \"What is LangGraph?\",\n",
    "    \"retrieved_docs\": [],\n",
    "    \"summary\": \"\",\n",
    "    \"decision\": None\n",
    "}\n",
    "```\n",
    "\n",
    "* **Retriever Node** adds `retrieved_docs`\n",
    "* **LLM Node** generates `summary`\n",
    "* **Decision Node** updates `decision`\n",
    "* Each node contributes incrementally, shaping the global state until completion.\n",
    "\n",
    "---\n",
    "\n",
    "**Passing Memory Between Nodes**\n",
    "\n",
    "Each node in LangGraph receives the **current state** as input, processes it, and returns an updated state to the next node.\n",
    "\n",
    "This makes LangGraph *state-aware* — nodes can remember prior steps and adapt accordingly.\n",
    "\n",
    "\n",
    "**Mechanism:**\n",
    "\n",
    "1. **Input** – Node receives a dictionary representing the current state.\n",
    "2. **Processing** – Node performs computation or reasoning based on current values.\n",
    "3. **Output** – Node returns the same state dictionary with new or updated entries.\n",
    "\n",
    "\n",
    "**Code Example:**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "graph = StateGraph()\n",
    "\n",
    "@graph.node()\n",
    "def retrieve(state):\n",
    "    state[\"retrieved_docs\"] = [\"LangGraph is a framework for orchestrating LLM workflows.\"]\n",
    "    return state\n",
    "\n",
    "@graph.node()\n",
    "def summarize(state):\n",
    "    doc = state[\"retrieved_docs\"][0]\n",
    "    state[\"summary\"] = f\"Summary: {doc.split()[0:6]}\"\n",
    "    return state\n",
    "\n",
    "graph.edge(\"retrieve\", \"summarize\")\n",
    "\n",
    "workflow = graph.compile()\n",
    "result = workflow.invoke({\"user_query\": \"Explain LangGraph\"})\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Flow:**\n",
    "`retrieve()` adds documents → `summarize()` uses them → both operate on the same `state`.\n",
    "\n",
    "\n",
    "**State Update Patterns**\n",
    "\n",
    "| Pattern            | Description                                      | Example                                               |\n",
    "| ------------------ | ------------------------------------------------ | ----------------------------------------------------- |\n",
    "| **Additive**       | Node appends new keys                            | `state[\"embeddings\"] = model.embed(text)`             |\n",
    "| **Transformative** | Node modifies existing values                    | `state[\"summary\"] = refine_summary(state[\"summary\"])` |\n",
    "| **Pruning**        | Node removes unnecessary data to optimize memory | `del state[\"temp_data\"]`                              |\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practices for Passing Memory:**\n",
    "\n",
    "* Use **consistent key naming** (`query`, `docs`, `summary`) across nodes.\n",
    "* Avoid **overwriting keys** unless intentional.\n",
    "* Log or print intermediate states for debugging.\n",
    "* For long workflows, periodically **snapshot** the state for checkpointing.\n",
    "\n",
    "---\n",
    "\n",
    "**Dynamic Updates and Context Persistence**\n",
    "\n",
    "Dynamic workflows often require nodes to modify or enrich the state based on *real-time outcomes*, user inputs, or intermediate results.\n",
    "\n",
    "LangGraph’s state supports **mutability and persistence**, enabling continuous context updates during runtime.\n",
    "\n",
    "---\n",
    "\n",
    "**Dynamic Update Example**\n",
    "\n",
    "```python\n",
    "@graph.node()\n",
    "def validate(state):\n",
    "    summary = state[\"summary\"]\n",
    "    if len(summary.split()) < 10:\n",
    "        state[\"status\"] = \"retry\"\n",
    "    else:\n",
    "        state[\"status\"] = \"complete\"\n",
    "    return state\n",
    "```\n",
    "\n",
    "This node updates the state **based on computed logic** — if the summary is too short, it flags a retry.\n",
    "\n",
    "---\n",
    "\n",
    "**Persistent Context**\n",
    "\n",
    "* **Short-Term Context:**\n",
    "  Keeps immediate conversation or query data (e.g., last user message).\n",
    "\n",
    "* **Long-Term Context:**\n",
    "  Maintains accumulated memory across sessions — can be stored externally (e.g., in a database or vector store).\n",
    "\n",
    "**Example Use Case:**\n",
    "\n",
    "* In a multi-turn chatbot, the state can include:\n",
    "\n",
    "  ```python\n",
    "  {\n",
    "      \"conversation_history\": [\"Hi!\", \"Hello, how can I help you today?\"],\n",
    "      \"user_profile\": {\"name\": \"Alex\"},\n",
    "      \"intent\": \"GetWeather\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "  This allows the graph to *retain memory* and respond intelligently across turns.\n",
    "\n",
    "---\n",
    "\n",
    "**Context Persistence Techniques**\n",
    "\n",
    "| Technique               | Description                                            | Example              |\n",
    "| ----------------------- | ------------------------------------------------------ | -------------------- |\n",
    "| **In-Memory State**     | Default during graph execution                         | `state` dict         |\n",
    "| **External Database**   | Store states for recovery or long-term memory          | MongoDB, Redis       |\n",
    "| **Vector Store Memory** | Store embeddings of past states for semantic retrieval | FAISS, Pinecone      |\n",
    "| **Checkpointing**       | Save serialized state snapshots                        | JSON logs or pickles |\n",
    "\n",
    "---\n",
    "\n",
    "**Real-World Example: Adaptive RAG Pipeline**\n",
    "\n",
    "```python\n",
    "@graph.node()\n",
    "def retriever(state):\n",
    "    state[\"docs\"] = retrieve_from_vector_store(state[\"query\"])\n",
    "    return state\n",
    "\n",
    "@graph.node()\n",
    "def summarizer(state):\n",
    "    state[\"summary\"] = llm.invoke(f\"Summarize: {state['docs']}\")\n",
    "    return state\n",
    "\n",
    "@graph.node()\n",
    "def reviewer(state):\n",
    "    if \"incomplete\" in state[\"summary\"]:\n",
    "        state[\"feedback\"] = \"Needs more detail\"\n",
    "        return \"refine\"\n",
    "    return \"done\"\n",
    "```\n",
    "\n",
    "The **state evolves** as follows:\n",
    "\n",
    "```\n",
    "Initial State → Retrieved Docs → Summary → Review → (Refine or Done)\n",
    "```\n",
    "\n",
    "Each stage refines the same `state`, ensuring **data continuity** throughout the workflow.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Concept                 | Description                               | Example                                  |\n",
    "| ----------------------- | ----------------------------------------- | ---------------------------------------- |\n",
    "| **State**               | Shared memory holding data across nodes   | `state = {\"query\": \"Explain LangGraph\"}` |\n",
    "| **Passing Memory**      | Each node reads/modifies the shared state | LLM nodes use `state[\"docs\"]`            |\n",
    "| **Dynamic Updates**     | State evolves as workflow executes        | Add keys like `\"status\"`, `\"decision\"`   |\n",
    "| **Context Persistence** | Retain short-term or long-term memory     | Save state snapshots or embeddings       |\n",
    "| **Checkpointing**       | Enables debugging and workflow recovery   | Save state to JSON/logs                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Workflows\n",
    "\n",
    "---\n",
    "\n",
    "**Creating Sequential and Conditional Graphs**\n",
    "\n",
    "Sequential and conditional graphs form the foundation of LangGraph-based pipelines.\n",
    "A **sequential graph** executes nodes one after another, while a **conditional graph** introduces branching logic — executing different nodes based on runtime conditions or decisions.\n",
    "\n",
    "**Text-Based Architecture Diagram (Sequential Graph)**\n",
    "\n",
    "```\n",
    "[Input Node] → [LLM Node] → [Output Node]\n",
    "```\n",
    "\n",
    "**Text-Based Architecture Diagram (Conditional Graph)**\n",
    "\n",
    "```\n",
    "             ┌──> [Summarizer Node]\n",
    "[Input Node] ─┤\n",
    "             └──> [Answer Generator Node]\n",
    "                     ↓\n",
    "               [Final Output Node]\n",
    "```\n",
    "\n",
    "**Example Use Case:**\n",
    "\n",
    "* Sequential: Processing a question → retrieving documents → generating a final answer.\n",
    "* Conditional: If user query requires factual data → go to retriever; else → go to reasoning LLM.\n",
    "\n",
    "**Code Example: Creating a Sequential Graph**\n",
    "\n",
    "```python\n",
    "# Importing required LangGraph components\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM node\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "\n",
    "# Define workflow graph\n",
    "workflow = StateGraph()\n",
    "\n",
    "# Step 1: Input Node\n",
    "def input_node(state):\n",
    "    \"\"\"Takes user query and prepares context.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    print(f\"Received query: {query}\")\n",
    "    return {\"context\": f\"Processing query: {query}\"}\n",
    "\n",
    "# Step 2: LLM Node\n",
    "def llm_node(state):\n",
    "    \"\"\"Generates a response using the LLM.\"\"\"\n",
    "    context = state[\"context\"]\n",
    "    response = llm.invoke(f\"Answer this query: {context}\")\n",
    "    return {\"response\": response}\n",
    "\n",
    "# Step 3: Output Node\n",
    "def output_node(state):\n",
    "    \"\"\"Formats and returns final output.\"\"\"\n",
    "    print(\"Generating final output...\")\n",
    "    return {\"final_answer\": state[\"response\"]}\n",
    "\n",
    "# Add nodes to workflow\n",
    "workflow.add_node(\"Input\", input_node)\n",
    "workflow.add_node(\"LLM\", llm_node)\n",
    "workflow.add_node(\"Output\", output_node)\n",
    "\n",
    "# Define execution flow (Sequential)\n",
    "workflow.add_edge(\"Input\", \"LLM\")\n",
    "workflow.add_edge(\"LLM\", \"Output\")\n",
    "workflow.add_edge(\"Output\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run the workflow\n",
    "result = app.invoke({\"query\": \"What is LangGraph?\"})\n",
    "print(result[\"final_answer\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Question Answering Graph**\n",
    "\n",
    "This workflow demonstrates how LangGraph can orchestrate **retrieval + generation** in a structured pipeline.\n",
    "\n",
    "**Architecture Overview**\n",
    "\n",
    "```\n",
    "[Question Input] \n",
    "     ↓\n",
    "[Retriever Node] \n",
    "     ↓\n",
    "[LLM Answer Node] \n",
    "     ↓\n",
    "[Output Node]\n",
    "```\n",
    "\n",
    "**Code Example: Question Answering Graph**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Initialize LLM and retriever\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "retriever = FAISS.load_local(\"knowledge_base\", OpenAIEmbeddings())\n",
    "\n",
    "# Define nodes\n",
    "def retrieve_node(state):\n",
    "    \"\"\"Retrieve context documents based on user question.\"\"\"\n",
    "    query = state[\"question\"]\n",
    "    docs = retriever.similarity_search(query, k=3)\n",
    "    return {\"docs\": docs}\n",
    "\n",
    "def answer_node(state):\n",
    "    \"\"\"Generate an answer using retrieved context.\"\"\"\n",
    "    docs = \"\\n\".join([d.page_content for d in state[\"docs\"]])\n",
    "    question = state[\"question\"]\n",
    "    response = llm.invoke(f\"Use the following context:\\n{docs}\\n\\nQuestion: {question}\")\n",
    "    return {\"answer\": response}\n",
    "\n",
    "# Create graph\n",
    "qa_graph = StateGraph()\n",
    "qa_graph.add_node(\"Retriever\", retrieve_node)\n",
    "qa_graph.add_node(\"Answer\", answer_node)\n",
    "qa_graph.add_edge(\"Retriever\", \"Answer\")\n",
    "qa_graph.add_edge(\"Answer\", END)\n",
    "qa_graph.set_entry_point(\"Retriever\")\n",
    "\n",
    "# Compile and run\n",
    "qa_app = qa_graph.compile()\n",
    "result = qa_app.invoke({\"question\": \"Explain Retrieval-Augmented Generation\"})\n",
    "print(result[\"answer\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Document Retrieval Graph**\n",
    "\n",
    "**Architecture Overview**\n",
    "\n",
    "```\n",
    "[Input Document Node]\n",
    "       ↓\n",
    "[Chunking Node]\n",
    "       ↓\n",
    "[Embedding Node]\n",
    "       ↓\n",
    "[Vector Store Node]\n",
    "       ↓\n",
    "[Completion Node]\n",
    "```\n",
    "\n",
    "**Code Example: Document Retrieval Graph**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Define processing nodes\n",
    "def input_node(state):\n",
    "    \"\"\"Receive and load document text.\"\"\"\n",
    "    return {\"text\": state[\"document\"]}\n",
    "\n",
    "def chunk_node(state):\n",
    "    \"\"\"Split the document into manageable chunks.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_text(state[\"text\"])\n",
    "    return {\"chunks\": chunks}\n",
    "\n",
    "def embed_node(state):\n",
    "    \"\"\"Generate embeddings for each chunk.\"\"\"\n",
    "    embedder = OpenAIEmbeddings()\n",
    "    embeddings = [embedder.embed_query(chunk) for chunk in state[\"chunks\"]]\n",
    "    return {\"embeddings\": embeddings, \"chunks\": state[\"chunks\"]}\n",
    "\n",
    "def store_node(state):\n",
    "    \"\"\"Store embeddings in FAISS vector index.\"\"\"\n",
    "    db = FAISS.from_embeddings(state[\"embeddings\"], state[\"chunks\"])\n",
    "    db.save_local(\"doc_vector_store\")\n",
    "    return {\"status\": \"Index saved\"}\n",
    "\n",
    "# Create graph\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Input\", input_node)\n",
    "graph.add_node(\"Chunk\", chunk_node)\n",
    "graph.add_node(\"Embed\", embed_node)\n",
    "graph.add_node(\"Store\", store_node)\n",
    "\n",
    "# Define flow\n",
    "graph.add_edge(\"Input\", \"Chunk\")\n",
    "graph.add_edge(\"Chunk\", \"Embed\")\n",
    "graph.add_edge(\"Embed\", \"Store\")\n",
    "graph.add_edge(\"Store\", END)\n",
    "\n",
    "# Compile and execute\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"document\": \"LangGraph enables complex AI workflows using graph-based orchestration.\"})\n",
    "print(result[\"status\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "🔹 **Key Takeaways**\n",
    "\n",
    "* Sequential graphs are ideal for **linear data pipelines**.\n",
    "* Conditional graphs allow **adaptive logic and branching**.\n",
    "* LangGraph makes it possible to **combine LLMs, retrievers, and tools** in modular, stateful workflows.\n",
    "* Graph visualization helps **debug and optimize** AI pipelines efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tool Integration**\n",
    "\n",
    "\n",
    "**Adding External Tools to LangGraph**\n",
    "\n",
    "LangGraph allows seamless integration of external tools such as APIs, databases, and vector stores, enabling your graph to perform real-world tasks.\n",
    "Each tool can be encapsulated as a **node** that performs an operation (e.g., search, retrieve, compute) and passes results to the next node.\n",
    "\n",
    " **Architecture Diagram — Tool-Integrated Graph**\n",
    "\n",
    "```\n",
    "[User Input] \n",
    "     ↓\n",
    "[Tool Node: Web Search/API Call]\n",
    "     ↓\n",
    "[LLM Node: Summarize Results]\n",
    "     ↓\n",
    "[Output Node]\n",
    "```\n",
    "\n",
    "**Example: Integrating a Web Search Tool**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize external tool\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "\n",
    "# Define tool node\n",
    "def web_search_node(state):\n",
    "    \"\"\"Perform a real-time web search.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    print(f\"Searching the web for: {query}\")\n",
    "    results = search_tool.run(query)\n",
    "    return {\"search_results\": results}\n",
    "\n",
    "# Define LLM node\n",
    "def summarize_node(state):\n",
    "    \"\"\"Summarize search results using the LLM.\"\"\"\n",
    "    context = state[\"search_results\"]\n",
    "    summary = llm.invoke(f\"Summarize the following information:\\n{context}\")\n",
    "    return {\"summary\": summary}\n",
    "\n",
    "# Build graph\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Search\", web_search_node)\n",
    "graph.add_node(\"Summarize\", summarize_node)\n",
    "graph.add_edge(\"Search\", \"Summarize\")\n",
    "graph.add_edge(\"Summarize\", END)\n",
    "graph.set_entry_point(\"Search\")\n",
    "\n",
    "# Compile and run\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"query\": \"Latest advancements in generative AI\"})\n",
    "print(result[\"summary\"])\n",
    "```\n",
    "\n",
    "**Key Benefit:**\n",
    "Tools act as plug-and-play nodes — you can easily integrate APIs, SQL databases, vector retrievers, or computational services.\n",
    "\n",
    "---\n",
    "\n",
    "**Using LangChain Tools Inside LangGraph**\n",
    "\n",
    "LangGraph extends LangChain’s modularity — any `Tool` from LangChain (like `WikipediaQueryRun`, `SerpAPIWrapper`, or `SQLDatabaseChain`) can be embedded as a graph node.\n",
    "\n",
    "**Architecture Diagram — LangChain Tool Workflow**\n",
    "\n",
    "```\n",
    "[User Query]\n",
    "     ↓\n",
    "[LangChain Tool Node]\n",
    "     ↓\n",
    "[LLM Node]\n",
    "     ↓\n",
    "[Output]\n",
    "```\n",
    "\n",
    "**Example: Using LangChain’s Wikipedia Tool**\n",
    "\n",
    "```python\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize tool and LLM\n",
    "wiki_tool = WikipediaQueryRun()\n",
    "llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "\n",
    "def wiki_node(state):\n",
    "    \"\"\"Retrieve Wikipedia content for a topic.\"\"\"\n",
    "    topic = state[\"topic\"]\n",
    "    data = wiki_tool.run(topic)\n",
    "    return {\"context\": data}\n",
    "\n",
    "def llm_node(state):\n",
    "    \"\"\"Summarize the content using GPT.\"\"\"\n",
    "    context = state[\"context\"]\n",
    "    answer = llm.invoke(f\"Summarize this content:\\n{context}\")\n",
    "    return {\"summary\": answer}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Wikipedia\", wiki_node)\n",
    "graph.add_node(\"Summarize\", llm_node)\n",
    "graph.add_edge(\"Wikipedia\", \"Summarize\")\n",
    "graph.add_edge(\"Summarize\", END)\n",
    "graph.set_entry_point(\"Wikipedia\")\n",
    "\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"topic\": \"LangChain\"})\n",
    "print(result[\"summary\"])\n",
    "```\n",
    "\n",
    "🧩 **Note:** LangGraph handles input/output passing automatically, maintaining **state consistency** across nodes.\n",
    "\n",
    "---\n",
    "\n",
    "**Custom Tool Nodes**\n",
    "\n",
    "You can create **custom tools** by wrapping any Python function or API endpoint in a node.\n",
    "This is useful when integrating proprietary services, ML models, or internal APIs.\n",
    "\n",
    "**Architecture Diagram — Custom Tool Integration**\n",
    "\n",
    "```\n",
    "[User Input]\n",
    "     ↓\n",
    "[Custom API Tool Node]\n",
    "     ↓\n",
    "[LLM Processing Node]\n",
    "     ↓\n",
    "[Final Output]\n",
    "```\n",
    "\n",
    "**Example: Custom Sentiment Analysis Tool Node**\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a custom API function\n",
    "def sentiment_api(text):\n",
    "    \"\"\"Mock sentiment analysis API call.\"\"\"\n",
    "    # Simulated API response\n",
    "    return {\"sentiment\": \"Positive\" if \"good\" in text else \"Neutral\"}\n",
    "\n",
    "def sentiment_node(state):\n",
    "    \"\"\"Call custom sentiment analysis tool.\"\"\"\n",
    "    text = state[\"text\"]\n",
    "    result = sentiment_api(text)\n",
    "    return {\"sentiment\": result[\"sentiment\"]}\n",
    "\n",
    "def llm_node(state):\n",
    "    \"\"\"Explain the sentiment result using LLM.\"\"\"\n",
    "    sentiment = state[\"sentiment\"]\n",
    "    llm = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "    explanation = llm.invoke(f\"Explain why this text is {sentiment}.\")\n",
    "    return {\"analysis\": explanation}\n",
    "\n",
    "# Build graph\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"SentimentTool\", sentiment_node)\n",
    "graph.add_node(\"ExplainLLM\", llm_node)\n",
    "graph.add_edge(\"SentimentTool\", \"ExplainLLM\")\n",
    "graph.add_edge(\"ExplainLLM\", END)\n",
    "graph.set_entry_point(\"SentimentTool\")\n",
    "\n",
    "# Run graph\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"text\": \"The product quality is good and I love it!\"})\n",
    "print(result[\"analysis\"])\n",
    "```\n",
    "\n",
    "**Tip:**\n",
    "Custom nodes can handle complex logic such as **data validation**, **API error handling**, or **asynchronous operations**.\n",
    "\n",
    "---\n",
    "\n",
    "**LLM Integration**\n",
    "\n",
    "\n",
    "\n",
    "**Connecting GPT, Anthropic, and Other Models**\n",
    "\n",
    "LangGraph supports **multi-LLM integration**, allowing you to connect models like **OpenAI GPT**, **Anthropic Claude**, **Mistral**, or **Gemini** within a single workflow.\n",
    "\n",
    "**Architecture Diagram — Multi-LLM Graph**\n",
    "\n",
    "```\n",
    "[User Input]\n",
    "     ↓\n",
    "[Claude Node] → reasoning\n",
    "     ↓\n",
    "[GPT Node] → final answer\n",
    "     ↓\n",
    "[Output Node]\n",
    "```\n",
    "\n",
    "**Example: Multi-LLM Integration**\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Initialize models\n",
    "gpt = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "claude = ChatAnthropic(model=\"claude-3-opus-2025\")\n",
    "\n",
    "def reasoning_node(state):\n",
    "    \"\"\"Claude performs analytical reasoning.\"\"\"\n",
    "    question = state[\"query\"]\n",
    "    thought = claude.invoke(f\"Analyze the question deeply: {question}\")\n",
    "    return {\"reasoning\": thought}\n",
    "\n",
    "def answer_node(state):\n",
    "    \"\"\"GPT generates the final formatted answer.\"\"\"\n",
    "    reasoning = state[\"reasoning\"]\n",
    "    answer = gpt.invoke(f\"Based on this reasoning, generate the final answer:\\n{reasoning}\")\n",
    "    return {\"final_answer\": answer}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Reasoning\", reasoning_node)\n",
    "graph.add_node(\"Answer\", answer_node)\n",
    "graph.add_edge(\"Reasoning\", \"Answer\")\n",
    "graph.add_edge(\"Answer\", END)\n",
    "graph.set_entry_point(\"Reasoning\")\n",
    "\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"query\": \"How do RAG architectures combine retrieval and generation?\"})\n",
    "print(result[\"final_answer\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Setting Up Multi-LLM Graphs**\n",
    "\n",
    "Multi-LLM setups enable **load balancing**, **specialized reasoning**, or **cross-verification** between models.\n",
    "\n",
    "**Architecture Diagram — Multi-LLM Decision Graph**\n",
    "\n",
    "```\n",
    "                   ┌──> [GPT Node: General QA]\n",
    "[User Query] ─┬───┤\n",
    "               └──> [Claude Node: Deep Reasoning]\n",
    "                       ↓\n",
    "                 [Final Merge Node]\n",
    "```\n",
    "\n",
    "**Example: Routing Based on Query Type**\n",
    "\n",
    "```python\n",
    "def router_node(state):\n",
    "    \"\"\"Route the query based on its type.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    if \"explain\" in query.lower():\n",
    "        return {\"route\": \"Claude\"}\n",
    "    else:\n",
    "        return {\"route\": \"GPT\"}\n",
    "\n",
    "graph.add_node(\"Router\", router_node)\n",
    "graph.add_conditional_edges(\n",
    "    \"Router\",\n",
    "    {\n",
    "        \"Claude\": \"ClaudeReasoner\",\n",
    "        \"GPT\": \"GPTAnswer\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**Model Switching and Routing Strategies**\n",
    "\n",
    "* **Rule-Based Routing:** Use keyword or intent detection.\n",
    "* **Confidence-Based Routing:** Pick model with highest predicted confidence.\n",
    "* **Ensemble Aggregation:** Combine outputs from multiple LLMs.\n",
    "* **Cost Optimization Routing:** Send simple queries to smaller models (e.g., GPT-4-mini) and complex ones to full GPT-4.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* LangGraph provides **modular integration of external tools and LLMs**.\n",
    "* Each tool or model acts as a **node** that performs a specific task.\n",
    "* You can design **multi-LLM, multi-tool** pipelines with conditional routing.\n",
    "* Great for **RAGs, multi-agent systems, or hybrid reasoning architectures**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction to Multi-Agent Architectures**\n",
    "\n",
    "---\n",
    "\n",
    "**Why Multi-Agent Systems?**\n",
    "\n",
    "Multi-Agent Systems (MAS) represent a paradigm shift from single-agent execution to **collaborative intelligence**, where multiple autonomous agents work together to achieve complex objectives. Each agent specializes in a specific task — such as reasoning, planning, data retrieval, or decision-making — and communicates with others to collectively solve problems that are too broad or intricate for a single model.\n",
    "\n",
    "Key Motivations for Multi-Agent Architectures:\n",
    "\n",
    "| Objective            | Description                                                                                                                   |\n",
    "| -------------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Scalability**      | Distribute tasks across multiple specialized agents for faster and parallel execution.                                        |\n",
    "| **Specialization**   | Each agent can be optimized or fine-tuned for a specific task — reasoning, code generation, summarization, or data retrieval. |\n",
    "| **Error Correction** | Agents can cross-verify or debate each other’s outputs to reduce hallucinations.                                              |\n",
    "| **Explainability**   | Dividing reasoning across multiple steps improves traceability and transparency.                                              |\n",
    "| **Resilience**       | System remains functional even if one agent fails or produces partial information.                                            |\n",
    "\n",
    "**Example Use Cases**\n",
    "\n",
    "* **Research Assistants:** One agent retrieves literature while another summarizes findings.\n",
    "* **Autonomous Workflows:** Agents act as planners, developers, and testers in automated code generation.\n",
    "* **Customer Support:** A retrieval agent gathers context, while a response agent generates empathetic replies.\n",
    "\n",
    "---\n",
    "\n",
    "**Agent Communication in LangGraph**\n",
    "\n",
    "LangGraph provides a **node-based execution model** for agent collaboration, where each agent is represented as a **node** capable of processing messages and maintaining internal or shared state. Agents communicate via **message passing**, either directly or through a **shared memory object**.\n",
    "\n",
    "**Communication Flow Diagram**\n",
    "\n",
    "```\n",
    "[Planner Agent] → [Retriever Agent] → [LLM Agent] → [Evaluator Agent]\n",
    "        ↑                 ↓\n",
    "      (Context)        (Feedback)\n",
    "```\n",
    "\n",
    "* Each arrow indicates the flow of data or decisions.\n",
    "* State (context, goals, memory) can persist through the workflow.\n",
    "* Agents may work **synchronously** (turn-based) or **asynchronously** (in parallel).\n",
    "\n",
    "LangGraph enables message-driven workflows, where each node triggers the next based on the **graph’s conditional logic** — making it ideal for orchestrating large, adaptive agent systems.\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison: LangChain Agents vs. LangGraph Agents**\n",
    "\n",
    "| Feature              | LangChain Agents                  | LangGraph Agents                          |\n",
    "| -------------------- | --------------------------------- | ----------------------------------------- |\n",
    "| **Execution Model**  | Sequential or tool-calling chain  | Graph-based, stateful, dynamic            |\n",
    "| **State Management** | Ephemeral, per chain execution    | Persistent and shared across nodes        |\n",
    "| **Communication**    | Tool invocation or memory context | Direct message passing between nodes      |\n",
    "| **Concurrency**      | Limited; primarily single agent   | Supports multiple concurrent agents       |\n",
    "| **Use Case**         | Simple decision-based automation  | Complex, multi-role collaboration systems |\n",
    "\n",
    "**Analogy:**\n",
    "LangChain is like a **solo performer**, while LangGraph is a **well-coordinated orchestra**.\n",
    "\n",
    "---\n",
    "\n",
    "**Agent Collaboration**\n",
    "\n",
    "---\n",
    "\n",
    "**Defining Roles and Responsibilities**\n",
    "\n",
    "In a multi-agent system, clarity of roles prevents overlap and improves efficiency. Common agent archetypes include:\n",
    "\n",
    "| Agent Type                 | Responsibility                                                |\n",
    "| -------------------------- | ------------------------------------------------------------- |\n",
    "| **Planner Agent**          | Breaks down the task and assigns subtasks.                    |\n",
    "| **Retriever Agent**        | Fetches relevant information from databases or vector stores. |\n",
    "| **Reasoning Agent**        | Synthesizes and analyzes retrieved data.                      |\n",
    "| **Critic/Evaluator Agent** | Evaluates other agents’ outputs for accuracy or bias.         |\n",
    "| **Communicator Agent**     | Formats and delivers final responses to users.                |\n",
    "\n",
    "**Architecture Example**\n",
    "\n",
    "```\n",
    "[Planner Node]\n",
    "     ↓\n",
    "[Retriever Node] → [Reasoner Node]\n",
    "     ↓                   ↓\n",
    "     └──────────────→ [Evaluator Node]\n",
    "```\n",
    "\n",
    "Each agent’s output is passed through LangGraph’s **state object**, maintaining shared memory and enabling cross-agent awareness.\n",
    "\n",
    "---\n",
    "\n",
    "**Message Passing and Context Sharing**\n",
    "\n",
    "Agents in LangGraph communicate through structured **messages** — typically dictionaries or JSON-like objects — containing content, metadata, and role identifiers.\n",
    "\n",
    "**Example: Context-Passing Between Agents**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "planner = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "researcher = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "\n",
    "def planner_node(state):\n",
    "    \"\"\"Planner decomposes task into subtasks.\"\"\"\n",
    "    task = state[\"goal\"]\n",
    "    subtasks = planner.invoke(f\"Break down the goal into smaller steps: {task}\")\n",
    "    return {\"subtasks\": subtasks}\n",
    "\n",
    "def researcher_node(state):\n",
    "    \"\"\"Researcher performs one of the subtasks.\"\"\"\n",
    "    subtasks = state[\"subtasks\"]\n",
    "    research = researcher.invoke(f\"Perform detailed research on:\\n{subtasks}\")\n",
    "    return {\"research_findings\": research}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Planner\", planner_node)\n",
    "graph.add_node(\"Researcher\", researcher_node)\n",
    "graph.add_edge(\"Planner\", \"Researcher\")\n",
    "graph.add_edge(\"Researcher\", END)\n",
    "graph.set_entry_point(\"Planner\")\n",
    "\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"goal\": \"Analyze the impact of AI on climate modeling\"})\n",
    "print(result[\"research_findings\"])\n",
    "```\n",
    "\n",
    "**Key Insight:**\n",
    "Every node inherits state from its predecessors, enabling **multi-agent awareness** and context retention.\n",
    "\n",
    "---\n",
    "\n",
    "**Building a Multi-Agent Debate System**\n",
    "\n",
    "A debate system uses **adversarial reasoning** — two or more agents argue for or against a proposition, while a judge agent evaluates.\n",
    "\n",
    "**Architecture Diagram**\n",
    "\n",
    "```\n",
    "[Pro-Agent] ⇄ [Con-Agent]\n",
    "       ↓           ↓\n",
    "     [Judge Node: Aggregates and Decides]\n",
    "```\n",
    "\n",
    "**Example: Debate Setup**\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "pro_agent = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "con_agent = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "judge_agent = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "\n",
    "def pro_node(state):\n",
    "    claim = state[\"topic\"]\n",
    "    argument = pro_agent.invoke(f\"Argue in favor of: {claim}\")\n",
    "    return {\"pro_argument\": argument}\n",
    "\n",
    "def con_node(state):\n",
    "    claim = state[\"topic\"]\n",
    "    counter = con_agent.invoke(f\"Argue against: {claim}\")\n",
    "    return {\"con_argument\": counter}\n",
    "\n",
    "def judge_node(state):\n",
    "    pro = state[\"pro_argument\"]\n",
    "    con = state[\"con_argument\"]\n",
    "    verdict = judge_agent.invoke(f\"Evaluate both sides:\\nFor: {pro}\\nAgainst: {con}\\nWho is more convincing?\")\n",
    "    return {\"verdict\": verdict}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Pro\", pro_node)\n",
    "graph.add_node(\"Con\", con_node)\n",
    "graph.add_node(\"Judge\", judge_node)\n",
    "\n",
    "graph.add_edge(\"Pro\", \"Con\")\n",
    "graph.add_edge(\"Con\", \"Judge\")\n",
    "graph.add_edge(\"Judge\", END)\n",
    "graph.set_entry_point(\"Pro\")\n",
    "\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"topic\": \"Should AI be given legal personhood?\"})\n",
    "print(result[\"verdict\"])\n",
    "```\n",
    "\n",
    "This system demonstrates **collaborative reasoning** through argumentation — an essential approach in fact-checking, policy generation, and autonomous decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "**Coordinator and Planner Nodes**\n",
    "\n",
    "---\n",
    "\n",
    "**Designing an Agent Orchestrator**\n",
    "\n",
    "The **Coordinator (or Orchestrator) Node** acts as a central controller — dispatching tasks, synchronizing agents, and collecting their outputs. It functions as the “brain” of the system, ensuring the workflow follows a defined or dynamic plan.\n",
    "\n",
    "**Architecture Overview**\n",
    "\n",
    "```\n",
    "[User Query]\n",
    "     ↓\n",
    "[Coordinator Node] \n",
    "     ↓\n",
    " ┌────────────┬────────────┐\n",
    " │ [Retriever]│ [Analyzer] │\n",
    " └────────────┴────────────┘\n",
    "     ↓\n",
    " [Synthesizer Node]\n",
    "     ↓\n",
    "     [Final Output]\n",
    "```\n",
    "\n",
    "**Example: Task-Oriented Multi-Agent Workflow**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "planner = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "retriever = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "analyzer = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "synthesizer = ChatOpenAI(model=\"gpt-4-turbo\")\n",
    "\n",
    "def coordinator_node(state):\n",
    "    \"\"\"Create plan and assign agent roles.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    plan = planner.invoke(f\"Plan tasks for this query: {query}\")\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "def retriever_node(state):\n",
    "    \"\"\"Retrieve contextual information.\"\"\"\n",
    "    plan = state[\"plan\"]\n",
    "    docs = retriever.invoke(f\"Search for documents based on: {plan}\")\n",
    "    return {\"retrieved_data\": docs}\n",
    "\n",
    "def analyzer_node(state):\n",
    "    \"\"\"Analyze the gathered data.\"\"\"\n",
    "    data = state[\"retrieved_data\"]\n",
    "    analysis = analyzer.invoke(f\"Analyze and summarize:\\n{data}\")\n",
    "    return {\"analysis\": analysis}\n",
    "\n",
    "def synthesizer_node(state):\n",
    "    \"\"\"Combine insights and generate final output.\"\"\"\n",
    "    report = synthesizer.invoke(f\"Combine all results and write a coherent response.\")\n",
    "    return {\"final_output\": report}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Coordinator\", coordinator_node)\n",
    "graph.add_node(\"Retriever\", retriever_node)\n",
    "graph.add_node(\"Analyzer\", analyzer_node)\n",
    "graph.add_node(\"Synthesizer\", synthesizer_node)\n",
    "\n",
    "graph.add_edge(\"Coordinator\", \"Retriever\")\n",
    "graph.add_edge(\"Retriever\", \"Analyzer\")\n",
    "graph.add_edge(\"Analyzer\", \"Synthesizer\")\n",
    "graph.add_edge(\"Synthesizer\", END)\n",
    "graph.set_entry_point(\"Coordinator\")\n",
    "\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"query\": \"Generate a sustainability report on renewable energy trends\"})\n",
    "print(result[\"final_output\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* Multi-agent systems enhance **collaboration, specialization, and robustness**.\n",
    "* LangGraph provides **structured agent communication** through state and message passing.\n",
    "* Coordinators and planners enable **dynamic task delegation** and **execution control**.\n",
    "* Applications include **RAG pipelines, autonomous research assistants, and reasoning debates**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conditional and Looping Logic**\n",
    "\n",
    "---\n",
    "\n",
    "**Branching Flows and Decision Nodes**\n",
    "\n",
    "Conditional logic allows LangGraph workflows to **branch dynamically** based on runtime information. Decision nodes evaluate conditions and direct the execution flow to the appropriate path. This is essential for **adaptive reasoning**, multi-agent coordination, and handling diverse query types.\n",
    "\n",
    "**Architecture Diagram — Conditional Branching**\n",
    "\n",
    "```\n",
    "           [Input Node]\n",
    "                 ↓\n",
    "           [Decision Node]\n",
    "          ┌─────────────┐\n",
    "          ↓             ↓\n",
    "  [Path A Node]     [Path B Node]\n",
    "          ↓             ↓\n",
    "        [Merge Node] ←─┘\n",
    "                 ↓\n",
    "           [Final Output]\n",
    "```\n",
    "\n",
    "**Example: Branching Based on Query Type**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def decision_node(state):\n",
    "    \"\"\"Route workflow based on query type.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    if \"summarize\" in query.lower():\n",
    "        return {\"route\": \"Summarizer\"}\n",
    "    else:\n",
    "        return {\"route\": \"AnswerGenerator\"}\n",
    "\n",
    "def summarizer_node(state):\n",
    "    return {\"result\": f\"Summarized: {state['query']}\"}\n",
    "\n",
    "def answer_node(state):\n",
    "    return {\"result\": f\"Answer: {state['query']}\"}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Decision\", decision_node)\n",
    "graph.add_node(\"Summarizer\", summarizer_node)\n",
    "graph.add_node(\"AnswerGenerator\", answer_node)\n",
    "graph.add_edge(\"Decision\", {\"Summarizer\": \"Summarizer\", \"AnswerGenerator\": \"AnswerGenerator\"})\n",
    "graph.add_edge(\"Summarizer\", END)\n",
    "graph.add_edge(\"AnswerGenerator\", END)\n",
    "graph.set_entry_point(\"Decision\")\n",
    "\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"query\": \"Summarize LangGraph features\"})\n",
    "print(result[\"result\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Implementing Loops for Iterative Reasoning**\n",
    "\n",
    "Loops allow nodes to **repeat execution** until a condition is met, enabling iterative refinement of reasoning, recursive querying, or multi-step computation. LangGraph supports loop-like flows by connecting nodes back to previous steps with **conditional checks**.\n",
    "\n",
    "**Architecture Diagram — Loop Node**\n",
    "\n",
    "```\n",
    "          [Input Node]\n",
    "                ↓\n",
    "         [Reasoning Node]\n",
    "                ↓\n",
    "      [Check Completion Node] ────┐\n",
    "             True                  │\n",
    "             ↓                     │\n",
    "         [Final Output]           ─┘\n",
    "             False\n",
    "             ↓\n",
    "       [Reasoning Node]  (Loop)\n",
    "```\n",
    "\n",
    "**Example: Iterative Refinement Loop**\n",
    "\n",
    "```python\n",
    "def reasoning_node(state):\n",
    "    \"\"\"Refine answer iteratively.\"\"\"\n",
    "    step = state.get(\"step\", 1)\n",
    "    answer = f\"Step {step} refinement of query: {state['query']}\"\n",
    "    return {\"answer\": answer, \"step\": step}\n",
    "\n",
    "def check_node(state):\n",
    "    \"\"\"Check if iteration should continue.\"\"\"\n",
    "    return {\"continue\": state[\"step\"] < 3, \"step\": state[\"step\"] + 1}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Reasoning\", reasoning_node)\n",
    "graph.add_node(\"Check\", check_node)\n",
    "\n",
    "graph.add_edge(\"Reasoning\", \"Check\")\n",
    "graph.add_edge(\"Check\", {\"True\": \"Reasoning\", \"False\": END})\n",
    "graph.set_entry_point(\"Reasoning\")\n",
    "\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"query\": \"Explain multi-agent reasoning\"})\n",
    "print(result)  # Iteratively refines answer over 3 steps\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Error Handling in Graph Flows**\n",
    "\n",
    "Error handling ensures that graphs can **gracefully recover** from failures, exceptions, or invalid outputs. LangGraph provides **fallback nodes** and **try-catch logic** to maintain reliability in production workflows.\n",
    "\n",
    "**Architecture Diagram — Error Handling**\n",
    "\n",
    "```\n",
    "           [Main Node]\n",
    "                ↓\n",
    "          [Error Check Node] ──┐\n",
    "             True                │\n",
    "             ↓                   │\n",
    "       [Fallback Node] ←─────────┘\n",
    "             ↓\n",
    "        [Final Output]\n",
    "```\n",
    "\n",
    "**Example: Handling Exceptions**\n",
    "\n",
    "```python\n",
    "def risky_node(state):\n",
    "    \"\"\"Simulate a node that might fail.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    if \"fail\" in query:\n",
    "        raise ValueError(\"Simulated failure\")\n",
    "    return {\"result\": f\"Processed: {query}\"}\n",
    "\n",
    "def fallback_node(state):\n",
    "    \"\"\"Fallback in case of error.\"\"\"\n",
    "    return {\"result\": \"Default response due to error\"}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Main\", risky_node)\n",
    "graph.add_node(\"Fallback\", fallback_node)\n",
    "graph.add_edge(\"Main\", END, on_error=\"Fallback\")\n",
    "graph.add_edge(\"Fallback\", END)\n",
    "graph.set_entry_point(\"Main\")\n",
    "\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"query\": \"fail this step\"})\n",
    "print(result[\"result\"])  # Outputs: Default response due to error\n",
    "```\n",
    "\n",
    "---\n",
    "**Key Takeaways**\n",
    "\n",
    "* **Decision nodes** enable branching based on runtime conditions.\n",
    "* **Loop nodes** support iterative refinement and multi-step reasoning.\n",
    "* **Error handling** ensures reliability and graceful recovery.\n",
    "* Combining these mechanisms allows LangGraph workflows to be **dynamic, adaptive, and fault-tolerant**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Memory and Context Handling**\n",
    "\n",
    "---\n",
    "\n",
    "**Shared and Local Memory**\n",
    "\n",
    "LangGraph provides flexible memory management, allowing nodes to use **local memory** for temporary computations and **shared memory** for cross-node context persistence. This is critical for multi-agent workflows, RAG pipelines, or iterative reasoning where state needs to be retained and reused.\n",
    "\n",
    "* **Local Memory:** Scoped to a single node, useful for temporary calculations or intermediate outputs.\n",
    "* **Shared Memory:** Accessible to multiple nodes, enabling context propagation across the workflow.\n",
    "\n",
    "🧠 **Architecture Diagram — Shared vs. Local Memory**\n",
    "\n",
    "```\n",
    "[Node A] ──┐\n",
    "           │\n",
    "       [Shared Memory] ←── [Node B]\n",
    "           │\n",
    "[Node C] ──┘\n",
    "```\n",
    "\n",
    "**Example: Local and Shared Memory Usage**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def node_local(state):\n",
    "    \"\"\"Local memory usage.\"\"\"\n",
    "    temp_result = state.get(\"input\") * 2\n",
    "    # Local result only visible to this node\n",
    "    return {\"local_result\": temp_result}\n",
    "\n",
    "def node_shared(state):\n",
    "    \"\"\"Shared memory usage.\"\"\"\n",
    "    shared_context = state.get(\"shared_context\", 0)\n",
    "    shared_context += state.get(\"input\", 1)\n",
    "    return {\"shared_context\": shared_context}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"LocalNode\", node_local)\n",
    "graph.add_node(\"SharedNode\", node_shared)\n",
    "graph.add_edge(\"LocalNode\", \"SharedNode\")\n",
    "graph.add_edge(\"SharedNode\", END)\n",
    "graph.set_entry_point(\"LocalNode\")\n",
    "\n",
    "result = graph.compile().invoke({\"input\": 5})\n",
    "print(result)  # {'shared_context': 5}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Long-Term Memory Graphs**\n",
    "\n",
    "Long-term memory allows information to **persist across multiple graph invocations**, enabling continuous learning, user personalization, or cumulative reasoning. LangGraph supports long-term memory by integrating **external storage backends** such as databases, vector stores, or custom persistent stores.\n",
    "\n",
    "**Architecture Diagram — Long-Term Memory Integration**\n",
    "\n",
    "```\n",
    "[User Query] \n",
    "     ↓\n",
    "[Graph Execution]\n",
    "     ↓\n",
    "[Shared Memory Node] ──→ [Long-Term Memory Store]\n",
    "```\n",
    "\n",
    "**Example: Long-Term Memory Persistence**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Mock persistent store\n",
    "long_term_store = {}\n",
    "\n",
    "def memory_node(state):\n",
    "    \"\"\"Read/write to long-term memory.\"\"\"\n",
    "    user_id = state[\"user_id\"]\n",
    "    previous_context = long_term_store.get(user_id, \"\")\n",
    "    new_context = previous_context + \" \" + state[\"input\"]\n",
    "    long_term_store[user_id] = new_context\n",
    "    return {\"memory_context\": new_context}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"MemoryNode\", memory_node)\n",
    "graph.add_edge(\"MemoryNode\", END)\n",
    "graph.set_entry_point(\"MemoryNode\")\n",
    "\n",
    "# Example usage\n",
    "graph.compile().invoke({\"user_id\": \"user123\", \"input\": \"Hello\"})\n",
    "graph.compile().invoke({\"user_id\": \"user123\", \"input\": \"How are you?\"})\n",
    "print(long_term_store[\"user123\"])  # Output: \" Hello How are you?\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Temporal State Propagation**\n",
    "\n",
    "Temporal state propagation ensures that **state evolves over time**, enabling agents to make decisions based on prior events or sequential outputs. This is essential for tasks like multi-step reasoning, conversation tracking, or multi-agent coordination.\n",
    "\n",
    "**Architecture Diagram — Temporal State Flow**\n",
    "\n",
    "```\n",
    "[Step 1 Node] → [Step 2 Node] → [Step 3 Node]\n",
    "       ↑              ↑             ↑\n",
    "  Local/Shared Memory propagates state across steps\n",
    "```\n",
    "\n",
    "**Example: Temporal State Across Steps**\n",
    "\n",
    "```python\n",
    "def step_node(state):\n",
    "    \"\"\"Accumulate results over multiple steps.\"\"\"\n",
    "    step_count = state.get(\"step_count\", 0) + 1\n",
    "    history = state.get(\"history\", [])\n",
    "    history.append(f\"Processed step {step_count}\")\n",
    "    return {\"step_count\": step_count, \"history\": history}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"StepNode\", step_node)\n",
    "graph.add_edge(\"StepNode\", \"StepNode\", condition=lambda s: s[\"step_count\"] < 3)\n",
    "graph.add_edge(\"StepNode\", END)\n",
    "graph.set_entry_point(\"StepNode\")\n",
    "\n",
    "result = graph.compile().invoke({})\n",
    "print(result[\"history\"])\n",
    "# Output: ['Processed step 1', 'Processed step 2', 'Processed step 3']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* **Local memory** is node-specific; **shared memory** enables cross-node context.\n",
    "* **Long-term memory** allows persistence across multiple graph executions.\n",
    "* **Temporal state propagation** supports sequential reasoning and multi-step workflows.\n",
    "* Proper memory management is critical for **multi-agent systems, RAG pipelines, and iterative reasoning tasks**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Callbacks, Logging, and Debugging**\n",
    "\n",
    "---\n",
    "\n",
    "**Tracing Node Execution**\n",
    "\n",
    "LangGraph provides **callback hooks** that allow you to track the execution of each node in real-time. These hooks can be used for monitoring, performance profiling, or triggering custom events during graph execution. Tracing is especially important in **complex multi-agent or iterative workflows**, where understanding the flow of data is critical.\n",
    "\n",
    "**Architecture Diagram — Node Execution Tracing**\n",
    "\n",
    "```\n",
    "[Input Node] → [Node A] → [Node B] → [Output Node]\n",
    "        │          │          │\n",
    "    Callback     Callback    Callback\n",
    "       Logs       Logs       Logs\n",
    "```\n",
    "\n",
    "**Example: Using Callbacks for Node Tracing**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def trace_callback(node_name, state):\n",
    "    print(f\"[TRACE] Node '{node_name}' executed with state: {state}\")\n",
    "\n",
    "def node_a(state):\n",
    "    return {\"a\": state.get(\"input\", 0) + 1}\n",
    "\n",
    "def node_b(state):\n",
    "    return {\"b\": state[\"a\"] * 2}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"NodeA\", node_a, on_execute=trace_callback)\n",
    "graph.add_node(\"NodeB\", node_b, on_execute=trace_callback)\n",
    "graph.add_edge(\"NodeA\", \"NodeB\")\n",
    "graph.add_edge(\"NodeB\", END)\n",
    "graph.set_entry_point(\"NodeA\")\n",
    "\n",
    "result = graph.compile().invoke({\"input\": 5})\n",
    "# Output:\n",
    "# [TRACE] Node 'NodeA' executed with state: {'a': 6}\n",
    "# [TRACE] Node 'NodeB' executed with state: {'b': 12}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Logging States and Transitions**\n",
    "\n",
    "LangGraph supports **detailed logging** of node states, transitions, and outputs. Logs can be written to **console, files, or external monitoring systems** (e.g., MLflow, Grafana) for post-analysis or auditing. This helps in **debugging**, optimizing workflows, and maintaining reproducibility.\n",
    "\n",
    "**Example: Logging Node Transitions**\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def node_with_logging(state):\n",
    "    result = state.get(\"value\", 0) + 10\n",
    "    logging.info(f\"Node executed with input: {state}, output: {result}\")\n",
    "    return {\"value\": result}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"LoggedNode\", node_with_logging)\n",
    "graph.add_edge(\"LoggedNode\", END)\n",
    "graph.set_entry_point(\"LoggedNode\")\n",
    "\n",
    "graph.compile().invoke({\"value\": 5})\n",
    "# Output in logs: INFO:root:Node executed with input: {'value': 5}, output: 15\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Debugging Common Graph Issues**\n",
    "\n",
    "Debugging LangGraph workflows involves understanding **state flow, node dependencies, and execution order**. Some common issues and solutions:\n",
    "\n",
    "| Issue                           | Cause                                       | Solution                                         |\n",
    "| ------------------------------- | ------------------------------------------- | ------------------------------------------------ |\n",
    "| Node outputs missing            | Node didn’t return expected dictionary keys | Ensure node returns correct state structure      |\n",
    "| Infinite loops                  | Loop condition never satisfied              | Add termination condition or max iteration count |\n",
    "| Shared memory conflicts         | Multiple nodes overwrite same key           | Use distinct keys or deep-copy shared objects    |\n",
    "| Exceptions in nodes             | Node raises error during execution          | Use `on_error` hooks or fallback nodes           |\n",
    "| Conditional paths not triggered | Incorrect condition or edge mapping         | Verify conditional edges and routing logic       |\n",
    "\n",
    "**Example: Using Fallback Nodes for Error Handling**\n",
    "\n",
    "```python\n",
    "def risky_node(state):\n",
    "    if state.get(\"input\") < 0:\n",
    "        raise ValueError(\"Input cannot be negative\")\n",
    "    return {\"result\": state[\"input\"] * 2}\n",
    "\n",
    "def fallback_node(state):\n",
    "    return {\"result\": 0}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Risky\", risky_node)\n",
    "graph.add_node(\"Fallback\", fallback_node)\n",
    "graph.add_edge(\"Risky\", END, on_error=\"Fallback\")\n",
    "graph.add_edge(\"Fallback\", END)\n",
    "graph.set_entry_point(\"Risky\")\n",
    "\n",
    "result = graph.compile().invoke({\"input\": -5})\n",
    "print(result[\"result\"])  # Outputs: 0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* **Callbacks** allow real-time tracing of node execution and state changes.\n",
    "* **Logging** provides a record of node outputs, transitions, and graph flow for monitoring and debugging.\n",
    "* **Debugging tools** include fallback nodes, conditional checks, and error-handling hooks.\n",
    "* Proper tracing and logging are essential for **complex multi-agent systems, iterative loops, and long-term workflows** in LangGraph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LangGraph + LangChain Integration**\n",
    "\n",
    "---\n",
    "\n",
    "**Building Hybrid Pipelines**\n",
    "\n",
    "Integrating LangGraph with LangChain allows you to build **hybrid pipelines** that leverage the **strengths of graph-based orchestration** with **LangChain’s chains, agents, and tools**. This enables complex multi-step reasoning workflows, RAG pipelines, and multi-agent collaboration in a structured, maintainable way.\n",
    "\n",
    "**Architecture Diagram — Hybrid Graph + Chain Pipeline**\n",
    "\n",
    "```\n",
    "[User Query]\n",
    "     ↓\n",
    "[LangGraph Coordinator]\n",
    "     ↓\n",
    " ┌───────────────┬───────────────┐\n",
    " │ LangChain RAG │ LangChain LLM │\n",
    " └───────────────┴───────────────┘\n",
    "     ↓\n",
    "[LangGraph Aggregator Node]\n",
    "     ↓\n",
    "   [Final Output]\n",
    "```\n",
    "\n",
    "* LangGraph orchestrates **multi-agent decision-making**, conditional flows, and looping.\n",
    "* LangChain handles **LLM calls, prompt templates, and retrieval-augmented generation**.\n",
    "\n",
    "---\n",
    "\n",
    "**RAG and Graph-Orchestrated Retrieval**\n",
    "\n",
    "LangGraph can **orchestrate RAG pipelines** by connecting retrieval agents, vector stores, and LLMs in a graph. Each retrieval node can query different **vector databases** (Chroma, Pinecone, FAISS), while the reasoning nodes aggregate and process results.\n",
    "\n",
    "**Example: Graph-Orchestrated RAG**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Mock retriever\n",
    "chroma_retriever = Chroma(...).as_retriever()\n",
    "\n",
    "def retrieve_node(state):\n",
    "    query = state[\"query\"]\n",
    "    docs = chroma_retriever.get_relevant_documents(query)\n",
    "    return {\"docs\": docs}\n",
    "\n",
    "def answer_node(state):\n",
    "    qa_chain = RetrievalQA(retriever=chroma_retriever)\n",
    "    answer = qa_chain.run(state[\"query\"])\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Retriever\", retrieve_node)\n",
    "graph.add_node(\"QA\", answer_node)\n",
    "graph.add_edge(\"Retriever\", \"QA\")\n",
    "graph.add_edge(\"QA\", END)\n",
    "graph.set_entry_point(\"Retriever\")\n",
    "\n",
    "result = graph.compile().invoke({\"query\": \"Explain vector databases in RAG\"})\n",
    "print(result[\"answer\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Extending LangChain Components with LangGraph**\n",
    "\n",
    "LangGraph allows LangChain **chains, agents, and tools** to become **nodes in a graph**. This enables:\n",
    "\n",
    "* **Dynamic routing** between chains based on query type.\n",
    "* **Looping** for multi-step reasoning or iterative refinement.\n",
    "* **State sharing** across chains for complex multi-agent reasoning.\n",
    "\n",
    "**Example Flow**\n",
    "\n",
    "```\n",
    "[LangGraph Node: Preprocessing]\n",
    "     ↓\n",
    "[LangChain LLMChain Node]\n",
    "     ↓\n",
    "[LangGraph Node: Post-processing / Aggregator]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Data Flow and APIs**\n",
    "\n",
    "---\n",
    "\n",
    "**Connecting Databases, Vector Stores, and APIs**\n",
    "\n",
    "LangGraph nodes can directly integrate with **databases, APIs, or vector stores**, enabling seamless ingestion of structured and unstructured data.\n",
    "\n",
    "* **Vector Stores:** Chroma, Pinecone, FAISS\n",
    "* **Databases:** SQL, NoSQL\n",
    "* **APIs:** REST, GraphQL, internal/external services\n",
    "\n",
    "**Example: Vector Store Integration**\n",
    "\n",
    "```python\n",
    "def vector_retriever_node(state):\n",
    "    # Using Pinecone as a vector store\n",
    "    import pinecone\n",
    "    index = pinecone.Index(\"my-index\")\n",
    "    results = index.query(vector=state[\"embedding\"], top_k=5)\n",
    "    return {\"retrieved_docs\": results[\"matches\"]}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Streaming Data Through Graphs**\n",
    "\n",
    "LangGraph supports **real-time streaming** by continuously passing **state updates** through nodes. This allows **incremental processing**, live monitoring, or dynamic feedback in workflows such as chatbots or data pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "**Workflow Orchestration**\n",
    "\n",
    "---\n",
    "\n",
    "**Combining LangGraph with Airflow or n8n**\n",
    "\n",
    "LangGraph can be orchestrated using **Airflow DAGs** or **n8n workflows**:\n",
    "\n",
    "* Each graph node can correspond to a task or microservice.\n",
    "* Scheduler triggers, retries, and event-based executions are managed externally.\n",
    "\n",
    "**Architecture Diagram — Event-Driven Orchestration**\n",
    "\n",
    "```\n",
    "[Airflow Scheduler] → [LangGraph Node: Retrieval] → [LangGraph Node: LLM] → [Output]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Event-Driven Triggers and Schedulers**\n",
    "\n",
    "LangGraph supports **event-driven execution**, which allows:\n",
    "\n",
    "* Running workflows on **new data arrival**.\n",
    "* Triggering multi-agent reasoning when **external conditions change**.\n",
    "* Integrating with **webhooks or streaming platforms** for automated triggers.\n",
    "\n",
    "---\n",
    "\n",
    "**Scaling Workflows in Production**\n",
    "\n",
    "* **Horizontal Scaling:** Deploy multiple LangGraph instances across nodes to handle high query volumes.\n",
    "* **State Storage:** Use persistent stores for shared or long-term memory.\n",
    "* **Concurrency Management:** Nodes can execute asynchronously for parallelism.\n",
    "* **Monitoring and Logging:** Integrate with Grafana, MLflow, or custom dashboards for operational insight.\n",
    "\n",
    "**Architecture — Scalable Multi-Agent Graph**\n",
    "\n",
    "```\n",
    "[Load Balancer]\n",
    "     ↓\n",
    "┌───────────────┬───────────────┐\n",
    "│ LangGraph Node│ LangGraph Node│  → Shared State Store\n",
    "└───────────────┴───────────────┘\n",
    "     ↓\n",
    " [Aggregator Node]\n",
    "     ↓\n",
    " [Final Output API]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* **LangGraph + LangChain** enables hybrid RAG pipelines and multi-agent workflows.\n",
    "* **Nodes** can wrap LangChain chains, agents, or tools for **dynamic orchestration**.\n",
    "* Integrates seamlessly with **databases, vector stores, APIs, and streaming data**.\n",
    "* Supports **event-driven triggers, schedulers, and scalable deployment** for production-grade applications.\n",
    "* Ideal for **complex retrieval, reasoning, and multi-step tasks** in LLM-powered systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conversational AI Systems**\n",
    "\n",
    "---\n",
    "\n",
    "**Context-Aware Chatbots**\n",
    "\n",
    "Context-aware chatbots leverage **memory and state propagation** to provide responses that are sensitive to the current conversation context. Unlike stateless bots, they remember previous interactions, user preferences, and prior query results to deliver **cohesive and personalized responses**.\n",
    "\n",
    "**Architecture Diagram — Context-Aware Chatbot**\n",
    "\n",
    "```\n",
    "[User Input]\n",
    "     ↓\n",
    "[Preprocessing Node] → Tokenization, Embeddings\n",
    "     ↓\n",
    "[Context Memory Node] → Retrieve conversation history\n",
    "     ↓\n",
    "[LLM Node] → Generate response based on current input + history\n",
    "     ↓\n",
    "[Postprocessing Node] → Formatting, filtering\n",
    "     ↓\n",
    "[User Output]\n",
    "```\n",
    "\n",
    "**Example: Retrieving Context in LangGraph**\n",
    "\n",
    "```python\n",
    "def context_node(state):\n",
    "    \"\"\"Retrieve conversation history for context-aware responses.\"\"\"\n",
    "    user_id = state[\"user_id\"]\n",
    "    conversation_history = state.get(\"shared_memory\", {}).get(user_id, [])\n",
    "    return {\"context\": conversation_history}\n",
    "\n",
    "def update_memory_node(state):\n",
    "    \"\"\"Update conversation memory after response.\"\"\"\n",
    "    user_id = state[\"user_id\"]\n",
    "    history = state.get(\"shared_memory\", {})\n",
    "    user_history = history.get(user_id, [])\n",
    "    user_history.append(state[\"current_input\"])\n",
    "    history[user_id] = user_history\n",
    "    return {\"shared_memory\": history}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Multi-Intent Dialog Management**\n",
    "\n",
    "Real-world conversations often contain **multiple intents** in a single user input. Multi-intent dialog management involves:\n",
    "\n",
    "* **Intent recognition:** Detect all intents in a query.\n",
    "* **Routing:** Direct each intent to the appropriate processing node or tool.\n",
    "* **Response aggregation:** Combine multiple outputs into a coherent reply.\n",
    "\n",
    "**Architecture Diagram — Multi-Intent Flow**\n",
    "\n",
    "```\n",
    "[User Input]\n",
    "     ↓\n",
    "[Intent Detection Node]\n",
    "     ↓\n",
    " ┌─────────────┬─────────────┐\n",
    " │ Intent A Node│ Intent B Node│\n",
    " └─────────────┴─────────────┘\n",
    "     ↓\n",
    "[Response Aggregator Node]\n",
    "     ↓\n",
    "[User Output]\n",
    "```\n",
    "\n",
    "**Example: Multi-Intent Routing**\n",
    "\n",
    "```python\n",
    "def intent_detection_node(state):\n",
    "    \"\"\"Detect multiple intents in a single message.\"\"\"\n",
    "    query = state[\"query\"].lower()\n",
    "    intents = []\n",
    "    if \"weather\" in query:\n",
    "        intents.append(\"weather\")\n",
    "    if \"news\" in query:\n",
    "        intents.append(\"news\")\n",
    "    return {\"intents\": intents}\n",
    "\n",
    "def weather_node(state):\n",
    "    return {\"response\": \"Weather info: Sunny 25°C\"}\n",
    "\n",
    "def news_node(state):\n",
    "    return {\"response\": \"Latest news: AI adoption is growing!\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Conversation Memory Graphs**\n",
    "\n",
    "Conversation Memory Graphs in LangGraph allow **stateful tracking of dialogues**, including multi-turn reasoning, user preferences, and context propagation. Memory graphs can store:\n",
    "\n",
    "* Previous user queries\n",
    "* LLM-generated responses\n",
    "* External knowledge or retrieval results\n",
    "* Intent and slot information\n",
    "\n",
    "**Architecture Diagram — Conversation Memory Graph**\n",
    "\n",
    "```\n",
    "[Turn 1: User Input] → [Memory Node] → [LLM Node] → [Output]\n",
    "[Turn 2: User Input] → [Memory Node] (retrieves Turn 1 context) → [LLM Node] → [Output]\n",
    "[Turn N: User Input] → Continues updating Memory Graph\n",
    "```\n",
    "\n",
    "**Example: Memory Graph Update**\n",
    "\n",
    "```python\n",
    "def update_conversation_graph(state):\n",
    "    \"\"\"Maintain conversation graph with nodes representing turns.\"\"\"\n",
    "    user_id = state[\"user_id\"]\n",
    "    memory_graph = state.get(\"memory_graph\", {})\n",
    "    turn_id = len(memory_graph.get(user_id, []))\n",
    "    turn_data = {\"input\": state[\"query\"], \"response\": state.get(\"response\")}\n",
    "    memory_graph.setdefault(user_id, []).append(turn_data)\n",
    "    return {\"memory_graph\": memory_graph}\n",
    "```\n",
    "\n",
    "---\n",
    "**Key Takeaways**\n",
    "\n",
    "* **Context-aware chatbots** retain prior conversation information to generate meaningful replies.\n",
    "* **Multi-intent dialog management** allows the system to handle complex queries with multiple requests.\n",
    "* **Conversation memory graphs** provide structured, stateful storage for multi-turn dialogues, enabling long-term context and personalized interactions.\n",
    "* Integrating **LangGraph nodes, memory management, and LLMs** allows building advanced, production-ready conversational AI systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Retrieval-Augmented Generation (RAG) Systems**\n",
    "\n",
    "---\n",
    "\n",
    "**Building RAG Pipelines Using LangGraph**\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines **retrieval of relevant documents or knowledge** with **LLM-based generation** to produce more accurate and context-aware responses. LangGraph enables **orchestration of RAG pipelines** by connecting retrieval nodes, vector stores, and LLM nodes in a structured workflow.\n",
    "\n",
    "**Architecture Diagram — RAG Pipeline in LangGraph**\n",
    "\n",
    "```\n",
    "[User Query]\n",
    "     ↓\n",
    "[Query Preprocessing Node]\n",
    "     ↓\n",
    "[Retriever Node(s)] → Vector DB / Knowledge Base\n",
    "     ↓\n",
    "[Context Aggregation Node]\n",
    "     ↓\n",
    "[LLM Generation Node] → Produce final answer\n",
    "     ↓\n",
    "[Postprocessing Node] → Formatting, filtering\n",
    "     ↓\n",
    "[User Output]\n",
    "```\n",
    "\n",
    "**Example: LangGraph RAG Workflow**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Initialize retriever\n",
    "pinecone_retriever = Pinecone(index_name=\"my-index\").as_retriever()\n",
    "\n",
    "def retrieve_node(state):\n",
    "    \"\"\"Retrieve relevant documents based on the query.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    docs = pinecone_retriever.get_relevant_documents(query)\n",
    "    return {\"retrieved_docs\": docs}\n",
    "\n",
    "def generate_answer_node(state):\n",
    "    \"\"\"Generate answer using retrieved documents.\"\"\"\n",
    "    qa_chain = RetrievalQA(retriever=pinecone_retriever)\n",
    "    answer = qa_chain.run(state[\"query\"])\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Retriever\", retrieve_node)\n",
    "graph.add_node(\"Generator\", generate_answer_node)\n",
    "graph.add_edge(\"Retriever\", \"Generator\")\n",
    "graph.add_edge(\"Generator\", END)\n",
    "graph.set_entry_point(\"Retriever\")\n",
    "\n",
    "result = graph.compile().invoke({\"query\": \"Explain multi-hop reasoning in RAG\"})\n",
    "print(result[\"answer\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Graph-Driven Context Selection**\n",
    "\n",
    "LangGraph allows **dynamic selection of context** using graph-based logic:\n",
    "\n",
    "* Multiple retrievers can query different **vector stores, embeddings, or databases**.\n",
    "* **Decision nodes** determine which retrieved context is most relevant.\n",
    "* Context aggregation nodes merge results from multiple sources to feed into the LLM.\n",
    "\n",
    "**Architecture Diagram — Context Selection**\n",
    "\n",
    "```\n",
    "            [User Query]\n",
    "                  ↓\n",
    "       ┌───────────┬───────────┐\n",
    "       │ Retriever │ Retriever │\n",
    "       │  Node A   │  Node B   │\n",
    "       └─────┬─────┴─────┬─────┘\n",
    "             ↓           ↓\n",
    "      [Context Aggregation Node]\n",
    "             ↓\n",
    "        [LLM Generator Node]\n",
    "```\n",
    "\n",
    "**Example: Conditional Context Selection**\n",
    "\n",
    "```python\n",
    "def context_selector_node(state):\n",
    "    \"\"\"Select the most relevant retrieved documents.\"\"\"\n",
    "    docs_a = state.get(\"RetrieverA_docs\", [])\n",
    "    docs_b = state.get(\"RetrieverB_docs\", [])\n",
    "    # Choose the source with more relevant content\n",
    "    if len(docs_a) > len(docs_b):\n",
    "        selected_docs = docs_a\n",
    "    else:\n",
    "        selected_docs = docs_b\n",
    "    return {\"selected_docs\": selected_docs}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Evaluation and Optimization**\n",
    "\n",
    "Effective RAG systems require **continuous evaluation and optimization**:\n",
    "\n",
    "1. **Metrics for RAG performance:**\n",
    "\n",
    "   * Retrieval accuracy (precision@k, recall@k)\n",
    "   * LLM answer quality (BLEU, ROUGE, METEOR, F1-score)\n",
    "   * Latency and throughput\n",
    "\n",
    "2. **Optimization strategies:**\n",
    "\n",
    "   * **Vector store tuning:** Index type, distance metrics, shard configuration.\n",
    "   * **Embedding quality:** Fine-tune embeddings for domain-specific knowledge.\n",
    "   * **Graph orchestration:** Optimize node placement, parallelism, and conditional paths.\n",
    "   * **Caching and batching:** Reduce redundant retrievals and speed up LLM generation.\n",
    "\n",
    "**Example: Automated Evaluation Node**\n",
    "\n",
    "```python\n",
    "def evaluation_node(state):\n",
    "    \"\"\"Compute precision and answer quality metrics.\"\"\"\n",
    "    retrieved_docs = state[\"selected_docs\"]\n",
    "    generated_answer = state[\"answer\"]\n",
    "    # Mock evaluation\n",
    "    precision = len(retrieved_docs) / max(1, len(retrieved_docs) + 1)\n",
    "    quality_score = 0.9  # Assume some automated LLM evaluation\n",
    "    return {\"precision\": precision, \"quality_score\": quality_score}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* LangGraph provides a **flexible architecture** for building RAG pipelines with multiple retrievers and LLMs.\n",
    "* **Graph-driven context selection** ensures only the most relevant knowledge is passed to the LLM.\n",
    "* **Evaluation and optimization** are crucial for high-quality, efficient RAG systems.\n",
    "* Integrating **LangGraph + LangChain** enables **dynamic, multi-source, and scalable RAG workflows** for production-ready applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Performance Optimization**\n",
    "\n",
    "---\n",
    "\n",
    "**Caching and Memory Efficiency**\n",
    "\n",
    "Effective caching and memory management are critical for ensuring **high-performance LangGraph workflows**, especially in multi-agent or RAG systems. Caching prevents redundant computations, reduces API calls, and improves throughput.\n",
    "\n",
    "* **Node-level caching:** Store outputs of nodes to avoid recalculating identical inputs.\n",
    "* **Shared memory optimization:** Keep frequently accessed data in shared memory rather than recomputing or fetching repeatedly.\n",
    "* **Persistent caching:** Use external stores like Redis or in-memory databases for long-term or cross-session caching.\n",
    "\n",
    "**Architecture Diagram — Node Caching**\n",
    "\n",
    "```\n",
    "[Input] → [Node A] → [Cached Output]\n",
    "                ↓\n",
    "            [Node B] → [Uses Cached Output]\n",
    "```\n",
    "\n",
    "**Example: Node Caching Implementation**\n",
    "\n",
    "```python\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def expensive_computation(input_data):\n",
    "    # Simulate a heavy computation\n",
    "    return sum([i**2 for i in range(input_data)])\n",
    "\n",
    "def node_with_cache(state):\n",
    "    result = expensive_computation(state[\"input\"])\n",
    "    return {\"result\": result}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Parallel Execution of Nodes**\n",
    "\n",
    "LangGraph supports **parallel execution** of independent nodes to reduce workflow latency. This is particularly useful in multi-agent setups, multi-retriever RAG pipelines, and multi-step reasoning flows.\n",
    "\n",
    "**Architecture Diagram — Parallel Node Execution**\n",
    "\n",
    "```\n",
    "        [Graph Entry]\n",
    "             ↓\n",
    "   ┌─────────┬─────────┐\n",
    "   │ Node A  │ Node B  │  → Independent execution\n",
    "   └─────────┴─────────┘\n",
    "             ↓\n",
    "      [Aggregator Node]\n",
    "             ↓\n",
    "         [Output]\n",
    "```\n",
    "\n",
    " **Example: Parallel Node Execution**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "import concurrent.futures\n",
    "\n",
    "def node_a(state):\n",
    "    return {\"a\": state[\"input\"] * 2}\n",
    "\n",
    "def node_b(state):\n",
    "    return {\"b\": state[\"input\"] + 10}\n",
    "\n",
    "graph = StateGraph(parallel=True)  # Enable parallelism\n",
    "graph.add_node(\"NodeA\", node_a)\n",
    "graph.add_node(\"NodeB\", node_b)\n",
    "graph.add_edge(\"NodeA\", END)\n",
    "graph.add_edge(\"NodeB\", END)\n",
    "graph.set_entry_point(\"NodeA\")\n",
    "\n",
    "result = graph.compile().invoke({\"input\": 5})\n",
    "print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Reducing Latency in Multi-Agent Flows**\n",
    "\n",
    "Multi-agent systems often introduce latency due to sequential agent interactions. Optimizations include:\n",
    "\n",
    "* **Asynchronous agent execution:** Run agents concurrently when dependencies allow.\n",
    "* **Partial result streaming:** Return intermediate results to users while processing continues.\n",
    "* **Load balancing:** Distribute agent execution across multiple nodes or servers.\n",
    "\n",
    "---\n",
    "\n",
    "**Deployment**\n",
    "\n",
    "---\n",
    "\n",
    "**Deploying LangGraph Workflows as APIs**\n",
    "\n",
    "LangGraph workflows can be deployed as **REST or FastAPI endpoints**, making them accessible for production systems and client applications. This approach enables real-time interaction with conversational AI systems, RAG pipelines, and multi-agent orchestrations.\n",
    "\n",
    "**Example: Deploying Graph as FastAPI API**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "graph = StateGraph()\n",
    "# Add nodes and edges here...\n",
    "graph.set_entry_point(\"NodeA\")\n",
    "\n",
    "@app.post(\"/execute\")\n",
    "async def execute_graph(input_data: dict):\n",
    "    result = graph.compile().invoke(input_data)\n",
    "    return result\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Docker and Cloud Deployment (AWS, GCP, Azure)**\n",
    "\n",
    "* Containerize LangGraph workflows using **Docker** for portability.\n",
    "* Use **Kubernetes or ECS/EKS/GKE** for orchestration and scaling.\n",
    "* Set up **auto-scaling and load balancing** for high availability in cloud environments.\n",
    "\n",
    "**Architecture Diagram — Cloud Deployment**\n",
    "\n",
    "```\n",
    "[Client Request]\n",
    "     ↓\n",
    "[API Gateway]\n",
    "     ↓\n",
    "[Docker Container Cluster]\n",
    "     ↓\n",
    "[LangGraph Workflow]\n",
    "     ↓\n",
    "[Response]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Version Control and Continuous Integration**\n",
    "\n",
    "* Store workflow definitions, nodes, and scripts in **Git**.\n",
    "* Implement **CI/CD pipelines** to automatically test and deploy graph updates.\n",
    "* Run **unit and integration tests** for nodes, agents, and API endpoints before production.\n",
    "\n",
    "---\n",
    "\n",
    "**Security and Governance**\n",
    "\n",
    "---\n",
    "\n",
    "**Handling Sensitive Data in Graphs**\n",
    "\n",
    "* Encrypt sensitive data at rest and in transit.\n",
    "* Limit memory retention for sensitive user inputs.\n",
    "* Mask or redact sensitive information when logging.\n",
    "\n",
    "---\n",
    "\n",
    "**Secure Tool Execution**\n",
    "\n",
    "* Restrict execution of external tools to trusted environments.\n",
    "* Use sandboxing or containerized execution for untrusted code or API calls.\n",
    "* Implement rate limiting and validation for external API interactions.\n",
    "\n",
    "---\n",
    "\n",
    "**Monitoring and Access Control**\n",
    "\n",
    "* Implement **role-based access control (RBAC)** for workflow execution.\n",
    "* Monitor node execution, errors, and agent activities.\n",
    "* Integrate with **logging and observability tools** like Grafana, Prometheus, or MLflow for real-time monitoring and alerts.\n",
    "\n",
    "**Architecture Diagram — Secure and Monitored Workflow**\n",
    "\n",
    "```\n",
    "[User] → [Auth Layer] → [LangGraph Workflow] → [Monitoring & Logs]\n",
    "                          ↑\n",
    "                   [Secure Tool Execution]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* **Caching, parallelism, and async execution** reduce latency and improve performance.\n",
    "* LangGraph workflows can be **deployed as APIs, containerized, and scaled** in cloud environments.\n",
    "* **Security and governance** are critical: protect sensitive data, secure tool execution, and implement access controls.\n",
    "* Monitoring and observability ensure **reliable, maintainable, and production-ready graph workflows**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
