{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcf6a13",
   "metadata": {},
   "source": [
    "# **Maximal Marginal Relevance**\n",
    "MMR (Maximal Marginal Relevance) is a powerful diversity-aware retrieval technique used in information retrieval and RAG pipelines to balance relevance and novelty when selecting documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87294dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54cb5be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain is an open-source framework designed to simplify the development of applications using large language models (LLMs).\\nLangChain provides abstractions for working with prompts, chains, memory, and agents, making it easier to build complex LLM-based systems.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='The framework supports integration with various vector databases like FAISS and Chroma for semantic retrieval.\\nLangChain enables Retrieval-Augmented Generation (RAG) by allowing developers to fetch relevant context before generating responses.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='Memory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent.\\nAgents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='BM25 and vector-based retrieval can be combined in LangChain to support hybrid retrieval strategies.\\nFAISS is a high-performance library for similarity search that LangChain leverages for efficient retrieval in RAG pipelines.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='Chroma is a lightweight vector store often used in LangChain for embedding-based document storage and retrieval.\\nPrompt templates in LangChain support Jinja-style formatting and variable injection to customize model inputs.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content=\"The 'stuff' chain sends all context at once to the LLM, useful for short documents in RAG.\\nThe 'map-reduce' chain breaks up large documents, processes them separately, and then aggregates the outputs.\\nThe 'refine' chain iteratively updates an answer by incorporating each new chunk of information.\"),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain allows LLMs to act as agents that decide which tool to call and in what order during a task.\\nLangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain agents can interact with external APIs and databases, enhancing the capabilities of LLM-powered applications.\\nRAG pipelines in LangChain involve document loading, splitting, embedding, retrieval, and LLM-based response generation.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='MMR (Maximal Marginal Relevance) retrieval in LangChain improves diversity by balancing relevance and redundancy.\\nTool usage in LangChain allows agents to execute predefined Python functions with contextual input from the user.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain supports reranking retrieved results using LLMs or neural cross-encoders to improve context quality.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load and chunk the document\n",
    "loader = TextLoader(\"langchain_rag_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b95ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: FAISS Vector Store with HuggingFace Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "836eb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 3: Create MMR Retirever\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a54fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prompt and LLM\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context provided.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "llm=init_chat_model(\"groq:gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cff3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: RAG Pipeline\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "rag_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ecccc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer:\n",
      " LangChain supports agents by allowing LLMs to act as decision-makers that choose and use tools like calculators, search APIs, or custom functions based on given instructions. \n",
      "\n",
      "It supports memory in two ways:\n",
      "\n",
      "* **Conversational memory:** Using `ConversationBufferMemory`, agents can retain previous interactions within a conversation, enabling more coherent multi-turn dialogues.\n",
      "* **Summarization memory:**  `ConversationSummaryMemory` allows agents to remember key points from past interactions, providing a condensed view of the conversation history. \n",
      "\n",
      "\n",
      "LangChain also enables agents to interact with external APIs and databases, further expanding their capabilities. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Query\n",
    "query = {\"input\": \"How does LangChain support agents and memory?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(\"✅ Answer:\\n\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75c6aefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How does LangChain support agents and memory?',\n",
       " 'context': [Document(id='27a1531f-1d37-4619-ad03-70b21cdf56b2', metadata={'source': 'langchain_rag_dataset.txt'}, page_content='Memory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent.\\nAgents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.'),\n",
       "  Document(id='fcd7afd5-b72a-4822-8ebe-c75d4e9250e0', metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain agents can interact with external APIs and databases, enhancing the capabilities of LLM-powered applications.\\nRAG pipelines in LangChain involve document loading, splitting, embedding, retrieval, and LLM-based response generation.'),\n",
       "  Document(id='e755e7c1-f863-4f8c-b97b-7b6d7281e29f', metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain allows LLMs to act as agents that decide which tool to call and in what order during a task.\\nLangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.')],\n",
       " 'answer': 'LangChain supports agents by allowing LLMs to act as decision-makers that choose and use tools like calculators, search APIs, or custom functions based on given instructions. \\n\\nIt supports memory in two ways:\\n\\n* **Conversational memory:** Using `ConversationBufferMemory`, agents can retain previous interactions within a conversation, enabling more coherent multi-turn dialogues.\\n* **Summarization memory:**  `ConversationSummaryMemory` allows agents to remember key points from past interactions, providing a condensed view of the conversation history. \\n\\n\\nLangChain also enables agents to interact with external APIs and databases, further expanding their capabilities. \\n\\n'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dc08c",
   "metadata": {},
   "source": [
    "## **Maximal Marginal Relevance (MMR)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6669af",
   "metadata": {},
   "source": [
    "**Maximal Marginal Relevance (MMR)** is a powerful algorithm used in **information retrieval** and **RAG (Retrieval-Augmented Generation)** systems to improve the **diversity and relevance** of search results.\n",
    "It ensures that the selected documents are **highly relevant to the query** while also being **minimally redundant** with each other.\n",
    "\n",
    "In essence, MMR strikes a balance between **query relevance** and **result diversity**, which helps avoid repetition and provides the **most informative subset of documents** for downstream tasks like question answering or summarization.\n",
    "\n",
    "---\n",
    "\n",
    "**1. The Intuition Behind MMR**\n",
    "\n",
    "Imagine you retrieve 100 similar documents for the query:\n",
    "\n",
    "> “What are the effects of climate change on agriculture?”\n",
    "\n",
    "Many of them might contain overlapping information (e.g., about crop yields). MMR ensures you get **a variety of perspectives** — for example:\n",
    "\n",
    "* Crop yield impacts,\n",
    "* Soil degradation,\n",
    "* Pest proliferation,\n",
    "* Water scarcity.\n",
    "\n",
    "By doing so, MMR avoids redundancy and ensures **comprehensive coverage** of the topic.\n",
    "\n",
    "---\n",
    "\n",
    "**2. The MMR Formula**\n",
    "\n",
    "The MMR score for a document *D<sub>i</sub>* is computed as:\n",
    "\n",
    "$$\\text{MMR} = \\arg\\max_{D_i \\in R \\setminus S} \\left[ \\lambda \\cdot \\text{Sim}(D_i, Q) - (1 - \\lambda) \\cdot \\max_{D_j \\in S} \\text{Sim}(D_i, D_j) \\right]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "| Symbol          | Meaning                                                                                               |\n",
    "| --------------- | ----------------------------------------------------------------------------------------------------- |\n",
    "| **Q**           | The query or question                                                                                 |\n",
    "| **R**           | The set of all retrieved documents                                                                    |\n",
    "| **S**           | The set of already selected documents                                                                 |\n",
    "| **Sim(Dᵢ, Q)**  | Similarity between document *Dᵢ* and query *Q* (relevance score)                                      |\n",
    "| **Sim(Dᵢ, Dⱼ)** | Similarity between document *Dᵢ* and another document *Dⱼ* (redundancy)                               |\n",
    "| **λ (lambda)**  | Trade-off parameter (0 ≤ λ ≤ 1):<br>• High λ → prioritize relevance<br>• Low λ → prioritize diversity |\n",
    "\n",
    "---\n",
    "\n",
    "**3. How MMR Works Step-by-Step**\n",
    "\n",
    "1. **Compute relevance scores** between all documents and the query.\n",
    "2. **Select the first document** with the highest relevance score.\n",
    "3. **Iteratively select next documents** by maximizing the MMR equation — i.e., choosing documents that are still relevant but add new information compared to already selected ones.\n",
    "4. Continue until you have the top *K* diverse documents.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Benefits of Using MMR**\n",
    "\n",
    "* ✅ **Reduces Redundancy:** Prevents repeating nearly identical documents.\n",
    "* ✅ **Improves Context Variety:** Useful for summarization or RAG contexts where multiple viewpoints matter.\n",
    "* ✅ **Balances Relevance and Novelty:** Adjustable via λ.\n",
    "* ✅ **Lightweight and Model-Agnostic:** Works with any embedding model or retriever.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Example: Implementing MMR with LangChain**\n",
    "\n",
    "Here’s how MMR is used in a LangChain retriever:\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"Doc 1 text...\", \"Doc 2 text...\", \"Doc 3 text...\"],\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Create an MMR retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # Use Maximal Marginal Relevance\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.7}\n",
    ")\n",
    "\n",
    "# Query the retriever\n",
    "results = retriever.get_relevant_documents(\"climate change and agriculture\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n",
    "```\n",
    "\n",
    "**Parameters Explained:**\n",
    "\n",
    "* `fetch_k`: Number of top candidates retrieved before MMR filtering.\n",
    "* `k`: Final number of documents returned after applying MMR.\n",
    "* `lambda_mult`: Controls the relevance–diversity trade-off.\n",
    "\n",
    "---\n",
    "\n",
    "**6. MMR in RAG Pipelines**\n",
    "\n",
    "In **Retrieval-Augmented Generation (RAG)**, MMR helps select documents that:\n",
    "\n",
    "* Are **semantically relevant** to the user’s query.\n",
    "* Provide **non-redundant context** to the language model.\n",
    "\n",
    "This improves the **completeness and factual quality** of generated responses by covering multiple subtopics or perspectives.\n",
    "\n",
    "Example LangChain RAG setup:\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "response = qa_chain.invoke({\"query\": \"Impacts of climate change on agriculture\"})\n",
    "print(response[\"result\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. Tuning the λ (Lambda) Parameter**\n",
    "\n",
    "| λ Value       | Effect                    | Use Case                         |\n",
    "| ------------- | ------------------------- | -------------------------------- |\n",
    "| **0.9 – 1.0** | Focus purely on relevance | FAQ search, exact query matching |\n",
    "| **0.6 – 0.8** | Balanced trade-off        | General RAG or QA systems        |\n",
    "| **0.3 – 0.5** | Emphasize diversity       | Summarization, topic exploration |\n",
    "| **0.0 – 0.2** | Focus purely on novelty   | Brainstorming, idea generation   |\n",
    "\n",
    "---\n",
    "\n",
    "**8. Comparison: MMR vs. Reranking**\n",
    "\n",
    "| Feature        | MMR                             | Reranking                              |\n",
    "| -------------- | ------------------------------- | -------------------------------------- |\n",
    "| **Purpose**    | Promote diversity among results | Improve accuracy of relevance          |\n",
    "| **Algorithm**  | Balances relevance & redundancy | Re-scores candidates using deep models |\n",
    "| **Efficiency** | Lightweight and fast            | Computationally expensive              |\n",
    "| **Use Case**   | RAG, summarization              | Precision-focused retrieval            |\n",
    "\n",
    "They can even be **combined** — first use MMR to pick diverse candidates, then apply **reranking** (e.g., cross-encoder) to fine-tune the final order.\n",
    "\n",
    "---\n",
    "\n",
    "**9. Real-World Applications**\n",
    "\n",
    "* **Conversational AI:** Prevents the model from repeating similar context passages.\n",
    "* **Document Summarization:** Ensures multiple aspects of a topic are represented.\n",
    "* **Recommendation Systems:** Promotes content diversity for better user engagement.\n",
    "* **Legal and Research Domains:** Retrieves unique precedents or study cases.\n",
    "\n",
    "---\n",
    "\n",
    "**10. Key Takeaways**\n",
    "\n",
    "* MMR = **Relevance - Redundancy**\n",
    "* It’s ideal for systems that need **variety + relevance** (like RAG).\n",
    "* It enhances both **user experience** and **model performance** by presenting the most **useful, non-repetitive** information.\n",
    "* Can be tuned easily and combined with reranking or hybrid retrieval pipelines for **state-of-the-art retrieval quality**.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Formula:**\n",
    "\n",
    "$$\\boxed{\\text{MMR} = \\lambda \\cdot \\text{Relevance} - (1-\\lambda) \\cdot \\text{Redundancy}}$$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
