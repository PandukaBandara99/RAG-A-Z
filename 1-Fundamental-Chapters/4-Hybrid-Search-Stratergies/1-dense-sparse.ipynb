{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a129598",
   "metadata": {},
   "source": [
    "# **Hybrid Retriever- Combining Dense And Sparse Retriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build LLM applications.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"Langchain has many types of retrievers.\")\n",
    "]\n",
    "\n",
    "# Step 2: Dense Retriever (FAISS + HuggingFace)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "dense_vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever = dense_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76569a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sparse Retriever(BM25)\n",
    "sparse_retriever=BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k=3 ##top- k documents to retriever\n",
    "\n",
    "## step 4 : Combine with Ensemble Retriever\n",
    "hybrid_retriever=EnsembleRetriever(\n",
    "    retrievers=[dense_retriever,sparse_retriever],\n",
    "    weight=[0.7,0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d59933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002227A2DFA10>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000002227A5A9A90>, k=3)], weights=[0.5, 0.5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec3b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Document 1:\n",
      "LangChain helps build LLM applications.\n",
      "\n",
      "🔹 Document 2:\n",
      "Langchain can be used to develop agentic ai application.\n",
      "\n",
      "🔹 Document 3:\n",
      "Langchain has many types of retrievers.\n",
      "\n",
      "🔹 Document 4:\n",
      "Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Query and get results\n",
    "query = \"How can I build an application using LLMs?\"\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "# Step 6: Print results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n🔹 Document {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c57cb",
   "metadata": {},
   "source": [
    "# **RAG Pipeline with hybrid retriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf22afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99e17a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002237E1BE710>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002237E1BEC10>, root_client=<openai.OpenAI object at 0x000002237E1BDE50>, root_async_client=<openai.AsyncOpenAI object at 0x000002237E1BE5D0>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Prompt Template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "## step 6-llm\n",
    "llm=init_chat_model(\"openai:gpt-3.5-turbo\",temperature=0.2)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9eb55e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002227A2DFA10>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000002227A5A9A90>, k=3)], weights=[0.5, 0.5]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {input}\\n')\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002237E1BE710>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002237E1BEC10>, root_client=<openai.OpenAI object at 0x000002237E1BDE50>, root_async_client=<openai.AsyncOpenAI object at 0x000002237E1BE5D0>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create stuff Docuemnt Chain\n",
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=prompt)\n",
    "\n",
    "## create Full rAg chain\n",
    "rag_chain=create_retrieval_chain(retriever=hybrid_retriever,combine_docs_chain=document_chain)\n",
    "rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb2441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer:\n",
      " You can build an app using LLMs by utilizing LangChain, which helps in developing LLM applications. LangChain can be used to develop agentic AI applications, and it offers various types of retrievers to enhance the functionality of your app. Additionally, you can also consider using Pinecone, a vector database for semantic search, to further improve the performance of your LLM-based app.\n",
      "\n",
      "📄 Source Documents:\n",
      "\n",
      "Doc 1: LangChain helps build LLM applications.\n",
      "\n",
      "Doc 2: Langchain can be used to develop agentic ai application.\n",
      "\n",
      "Doc 3: Langchain has many types of retrievers.\n",
      "\n",
      "Doc 4: Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Ask a question\n",
    "query = {\"input\": \"How can I build an app using LLMs?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Step 10: Output\n",
    "print(\"✅ Answer:\\n\", response[\"answer\"])\n",
    "\n",
    "print(\"\\n📄 Source Documents:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd0ed25",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cf2743",
   "metadata": {},
   "source": [
    "## **Hybrid Retriever — Combining Dense and Sparse Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63332c4d",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "A **Hybrid Retriever** blends the strengths of **dense (vector-based)** and **sparse (keyword-based)** retrieval systems to produce **more accurate, context-aware, and robust search results**.\n",
    "This approach is ideal when you need both **semantic understanding** (via embeddings) and **exact keyword matching** (via term frequency relevance).\n",
    "\n",
    "---\n",
    "\n",
    "**1. Why Hybrid Retrieval?**\n",
    "\n",
    "Each retrieval method has strengths and weaknesses:\n",
    "\n",
    "| Retrieval Type       | Description                                                         | Strengths                                        | Weaknesses                                  |\n",
    "| -------------------- | ------------------------------------------------------------------- | ------------------------------------------------ | ------------------------------------------- |\n",
    "| **Dense (Vector)**   | Uses embeddings to represent semantic meaning of text               | Captures **context** and **semantic similarity** | May miss **exact keyword matches**          |\n",
    "| **Sparse (Keyword)** | Uses token-based models (like TF-IDF, BM25) for exact term matching | Strong for **exact keyword relevance**           | Fails at understanding **semantic meaning** |\n",
    "\n",
    "💡 **Hybrid retrieval** merges both — leveraging **semantic embeddings** (e.g., OpenAI, Hugging Face models) and **lexical scores** (e.g., BM25, ElasticSearch) — providing the **best of both worlds**.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Core Concept**\n",
    "\n",
    "Let:\n",
    "\n",
    "* ( S_d(q, x) ) = dense similarity score (cosine similarity between embeddings)\n",
    "* ( S_s(q, x) ) = sparse similarity score (BM25, TF-IDF, etc.)\n",
    "\n",
    "Then the **hybrid score** can be defined as:\n",
    "\n",
    "[\n",
    "S_{hybrid}(q, x) = \\alpha \\times S_d(q, x) + (1 - \\alpha) \\times S_s(q, x)\n",
    "]\n",
    "\n",
    "where\n",
    "\n",
    "* ( \\alpha \\in [0,1] ) controls the weighting between dense and sparse results.\n",
    "* Example: ( \\alpha = 0.7 ) → 70% semantic, 30% keyword importance.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Implementing Hybrid Search in Pinecone**\n",
    "\n",
    "Pinecone provides **native support for hybrid retrieval**, allowing you to store **both dense and sparse vectors** in a **single index**.\n",
    "\n",
    "**Step-by-Step Implementation**\n",
    "\n",
    "**(1) Create a Hybrid Index**\n",
    "\n",
    "```python\n",
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=\"YOUR_API_KEY\", environment=\"gcp-starter\")\n",
    "\n",
    "index_name = \"hybrid-search\"\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,     # dense vector dimension\n",
    "        metric=\"dotproduct\"\n",
    "    )\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**(2) Prepare Dense and Sparse Representations**\n",
    "\n",
    "You can use:\n",
    "\n",
    "* **Dense** → Sentence Transformers, OpenAI embeddings, etc.\n",
    "* **Sparse** → BM25 or SPLADE (sparse transformer-based encoder)\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "# Dense embedding model\n",
    "dense_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\"Hybrid retrieval combines dense and sparse models\",\n",
    "          \"Pinecone supports hybrid vector search\",\n",
    "          \"BM25 is a sparse retrieval algorithm\"]\n",
    "\n",
    "# Sparse (BM25)\n",
    "bm25 = BM25Okapi([doc.split() for doc in corpus])\n",
    "\n",
    "# Dense embeddings\n",
    "dense_vectors = dense_model.encode(corpus)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**(3) Insert Combined Representations into Pinecone**\n",
    "\n",
    "Each vector includes **dense + sparse components**:\n",
    "\n",
    "```python\n",
    "# Example: dense + sparse vector upload\n",
    "upserts = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    sparse_vector = bm25.get_scores(doc.split()).tolist()\n",
    "    upserts.append((\n",
    "        str(i),\n",
    "        dense_vectors[i],\n",
    "        {\"sparse_values\": {\"indices\": list(range(len(sparse_vector))),\n",
    "                           \"values\": sparse_vector},\n",
    "         \"text\": doc}\n",
    "    ))\n",
    "\n",
    "index.upsert(vectors=upserts)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**(4) Hybrid Querying**\n",
    "\n",
    "```python\n",
    "query = \"semantic and keyword search\"\n",
    "dense_q = dense_model.encode(query)\n",
    "\n",
    "# Sparse component for the query\n",
    "sparse_scores = bm25.get_scores(query.split()).tolist()\n",
    "\n",
    "# Weighted hybrid query\n",
    "alpha = 0.7\n",
    "hybrid_results = index.query(\n",
    "    vector=dense_q,\n",
    "    sparse_vector={\"indices\": list(range(len(sparse_scores))),\n",
    "                   \"values\": sparse_scores},\n",
    "    top_k=5,\n",
    "    include_metadata=True,\n",
    "    alpha=alpha\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**4. Weighted Scoring Techniques**\n",
    "\n",
    "The **balance parameter (α)** controls the influence of each retriever:\n",
    "\n",
    "| α   | Retrieval Focus | Typical Use Case                                  |\n",
    "| --- | --------------- | ------------------------------------------------- |\n",
    "| 0.0 | Purely Sparse   | Keyword search, factual lookup                    |\n",
    "| 0.5 | Balanced        | Hybrid QA, semantic retrieval with term grounding |\n",
    "| 1.0 | Purely Dense    | Conversational AI, RAG, semantic reasoning        |\n",
    "\n",
    "You can **tune α dynamically** based on query intent — for example:\n",
    "\n",
    "* If query has **rare keywords**, emphasize sparse (α ↓).\n",
    "* If query is **semantic**, emphasize dense (α ↑).\n",
    "\n",
    "---\n",
    "\n",
    "**5. Real-World Applications**\n",
    "\n",
    "| Use Case                                 | Description                                                      |\n",
    "| ---------------------------------------- | ---------------------------------------------------------------- |\n",
    "| **Retrieval-Augmented Generation (RAG)** | Combines semantic understanding with factual grounding for LLMs  |\n",
    "| **Enterprise Search**                    | Merges semantic relevance with company-specific jargon           |\n",
    "| **E-commerce**                           | Matches product descriptions semantically and by keywords        |\n",
    "| **Legal or Medical Search**              | Ensures critical keywords are not missed while capturing context |\n",
    "\n",
    "---\n",
    "\n",
    "**6. Summary**\n",
    "\n",
    "| Aspect       | Dense Retriever           | Sparse Retriever     | Hybrid Retriever  |\n",
    "| ------------ | ------------------------- | -------------------- | ----------------- |\n",
    "| **Basis**    | Semantic similarity       | Keyword matching     | Combined          |\n",
    "| **Models**   | Embeddings (BERT, OpenAI) | TF-IDF, BM25, SPLADE | Both              |\n",
    "| **Speed**    | High                      | High                 | Moderate          |\n",
    "| **Accuracy** | Semantic                  | Lexical              | Best Overall      |\n",
    "| **Use Case** | QA, Semantic Search       | Document Lookup      | RAG, Smart Search |\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "👉 A **Hybrid Retriever** in Pinecone merges **dense embeddings** and **sparse keyword vectors**, enabling **precise + meaningful search** — crucial for **RAG systems**, **enterprise knowledge bases**, and **AI search assistants**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511fd397",
   "metadata": {},
   "source": [
    "## **TF-IDF (Term Frequency–Inverse Document Frequency)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61be3daa",
   "metadata": {},
   "source": [
    "**What Is TF-IDF?**\n",
    "\n",
    "> **TF-IDF** stands for **Term Frequency–Inverse Document Frequency** —\n",
    "a numerical statistic used to **measure the importance of a word** in a document relative to a collection (corpus).\n",
    "\n",
    "It’s the foundation for **sparse vector representations**, where each document is represented as a **vector of term weights**.\n",
    "\n",
    "---\n",
    "\n",
    "**Formula**\n",
    "\n",
    "The **TF-IDF score** of a term *t* in document *d* is calculated as:\n",
    "\n",
    "$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "where:\n",
    "\n",
    "1. **Term Frequency (TF):**\n",
    "   [\n",
    "   \\text{TF}(t, d) = \\frac{\\text{Number of times term t appears in d}}{\\text{Total number of terms in d}}\n",
    "   ]\n",
    "\n",
    "   > Measures how frequently a word occurs within a document.\n",
    "\n",
    "2. **Inverse Document Frequency (IDF):**\n",
    "   [\n",
    "   \\text{IDF}(t) = \\log \\left( \\frac{N}{1 + \\text{DF}(t)} \\right)\n",
    "   ]\n",
    "   where\n",
    "   ( N ) = total number of documents\n",
    "   ( \\text{DF}(t) ) = number of documents containing the term *t*\n",
    "\n",
    "   > Reduces the weight of **common terms** and increases the weight of **rare but important terms**.\n",
    "\n",
    "---\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let’s say we have a corpus of 3 documents:\n",
    "\n",
    "| Doc | Text                                            |\n",
    "| --- | ----------------------------------------------- |\n",
    "| D1  | \"Pinecone provides vector database services\"    |\n",
    "| D2  | \"Vector databases are used for semantic search\" |\n",
    "| D3  | \"TF-IDF is a sparse retrieval technique\"        |\n",
    "\n",
    "> Step 1 — Tokenize\n",
    "\n",
    "```\n",
    "[\"pinecone\", \"provides\", \"vector\", \"database\", \"services\"]\n",
    "[\"vector\", \"databases\", \"used\", \"semantic\", \"search\"]\n",
    "[\"tf-idf\", \"sparse\", \"retrieval\", \"technique\"]\n",
    "```\n",
    "\n",
    "> Step 2 — Compute TF\n",
    "\n",
    "For D1:\n",
    "\n",
    "* TF(\"vector\") = 1/5 = 0.2\n",
    "* TF(\"pinecone\") = 1/5 = 0.2\n",
    "\n",
    "> Step 3 — Compute IDF\n",
    "\n",
    "| Term      | Appears In | IDF            |\n",
    "| --------- | ---------- | -------------- |\n",
    "| vector    | 2          | log(3/2)=0.176 |\n",
    "| pinecone  | 1          | log(3/1)=1.098 |\n",
    "| retrieval | 1          | log(3/1)=1.098 |\n",
    "\n",
    "> Step 4 — Compute TF-IDF\n",
    "\n",
    "For “vector” in D1 → 0.2 × 0.176 = 0.035\n",
    "For “pinecone” in D1 → 0.2 × 1.098 = 0.219\n",
    "\n",
    "→ “pinecone” is **more important** to D1.\n",
    "\n",
    "---\n",
    "\n",
    "**How TF-IDF Works in Vector Search**\n",
    "\n",
    "Each document is represented as a **sparse high-dimensional vector**,\n",
    "where each dimension corresponds to a **word** in the vocabulary, and the value is its **TF-IDF score**.\n",
    "\n",
    "For example:\n",
    "\n",
    "| Term      | D1    | D2    | D3    |\n",
    "| --------- | ----- | ----- | ----- |\n",
    "| pinecone  | 0.219 | 0     | 0     |\n",
    "| vector    | 0.035 | 0.035 | 0     |\n",
    "| retrieval | 0     | 0     | 0.219 |\n",
    "\n",
    "Similarity between two documents (or query & doc) is computed via **cosine similarity** between these sparse vectors.\n",
    "\n",
    "---\n",
    "\n",
    "**Python Implementation Example**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Corpus\n",
    "docs = [\n",
    "    \"Pinecone provides vector database services\",\n",
    "    \"Vector databases are used for semantic search\",\n",
    "    \"TF-IDF is a sparse retrieval technique\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Query\n",
    "query = [\"semantic vector database\"]\n",
    "\n",
    "# Transform query to TF-IDF vector\n",
    "query_vec = vectorizer.transform(query)\n",
    "\n",
    "# Compute similarity\n",
    "cosine_sim = cosine_similarity(query_vec, tfidf_matrix)\n",
    "\n",
    "print(\"Similarity Scores:\", cosine_sim)\n",
    "```\n",
    "\n",
    "🧾 Output:\n",
    "\n",
    "```\n",
    "Similarity Scores: [[0.35 0.67 0.05]]\n",
    "```\n",
    "\n",
    "→ The query matches best with **Document 2** (semantic search, vector databases).\n",
    "\n",
    "---\n",
    "\n",
    "**Integration in Hybrid Search**\n",
    "\n",
    "In **hybrid retrieval**, TF-IDF acts as the **sparse retriever**.\n",
    "\n",
    "For example:\n",
    "\n",
    "* **Sparse vector** → TF-IDF or BM25 output\n",
    "* **Dense vector** → BERT, OpenAI, or Hugging Face embeddings\n",
    "* **Hybrid scoring** → Combine both (using weighted α)\n",
    "\n",
    "Example hybrid score:\n",
    "\n",
    "```python\n",
    "hybrid_score = alpha * dense_similarity + (1 - alpha) * tfidf_similarity\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages & Disadvantages**\n",
    "\n",
    "| Aspect             | Pros                                                    | Cons                                     |\n",
    "| ------------------ | ------------------------------------------------------- | ---------------------------------------- |\n",
    "| **Speed**          | Fast and efficient for small-medium corpora             | Slower on large corpora without indexing |\n",
    "| **Explainability** | Transparent scoring (easy to debug)                     | Lacks semantic understanding             |\n",
    "| **Memory**         | Sparse representation saves space                       | Limited contextual relevance             |\n",
    "| **Best Use Cases** | Keyword-based search, document filtering, hybrid setups | Semantic tasks (use embeddings instead)  |\n",
    "\n",
    "---\n",
    "\n",
    "**Real-World Applications**\n",
    "\n",
    "* **Search engines** (e.g., ElasticSearch uses TF-IDF variants)\n",
    "* **Document similarity & plagiarism detection**\n",
    "* **Keyword-based filtering in hybrid systems**\n",
    "* **Metadata matching in RAG pipelines**\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Concept               | Description                                               |\n",
    "| --------------------- | --------------------------------------------------------- |\n",
    "| **Goal**              | Measure how important a word is to a document in a corpus |\n",
    "| **Formula**           | TF × IDF                                                  |\n",
    "| **Similarity Metric** | Cosine similarity                                         |\n",
    "| **Usage**             | Sparse retrieval, Hybrid RAG, Search systems              |\n",
    "| **Integration**       | Works alongside embeddings in hybrid retrievers           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878f843",
   "metadata": {},
   "source": [
    "## **BM25 — The Core Ranking Algorithm in Modern Search Engines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31514a",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "**BM25 (Best Match 25)** is a **ranking algorithm** used by modern search engines like **Elasticsearch**, **Lucene**, and **Whoosh** to measure how relevant a document is to a user’s search query.\n",
    "It’s an **improved version of TF-IDF (Term Frequency–Inverse Document Frequency)** and is widely adopted because of its **robustness, interpretability, and effectiveness** in real-world retrieval tasks.\n",
    "\n",
    "BM25 belongs to the **Okapi family** of probabilistic retrieval models, developed at **City University London** in the 1990s as part of the **Okapi IR System**.\n",
    "\n",
    "---\n",
    "\n",
    "**Why BM25?**\n",
    "\n",
    "Traditional **TF-IDF** gives more importance to documents with high term frequency, but it doesn’t:\n",
    "\n",
    "* Account for **document length differences** (longer documents unfairly score higher)\n",
    "* **Cap term frequency influence**, which can over-boost repetitive words\n",
    "* Provide easy tuning parameters for different datasets\n",
    "\n",
    "**BM25** solves these issues by:\n",
    "✅ Introducing **term saturation** — diminishing returns for high word counts\n",
    "✅ Adding **document length normalization**\n",
    "✅ Allowing tunable parameters **k₁** and **b** for fine control over ranking\n",
    "\n",
    "---\n",
    "\n",
    "**The BM25 Formula**\n",
    "\n",
    "The BM25 score for a document *d* given a query *q* is calculated as:\n",
    "\n",
    "$$\\text{score}(q, d) = \\sum_{t \\in q} IDF(t) \\times \\frac{TF(t, d) \\times (k_1 + 1)}{TF(t, d) + k_1 \\times (1 - b + b \\times \\frac{|d|}{avg_d})}$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "| Symbol       | Meaning                                                |    |                                   |\n",
    "| :----------- | :----------------------------------------------------- | -- | --------------------------------- |\n",
    "| **t**        | Term in the query                                      |    |                                   |\n",
    "| **q**        | Query consisting of multiple terms                     |    |                                   |\n",
    "| **d**        | Document being scored                                  |    |                                   |\n",
    "| **TF(t, d)** | Term frequency of t in document d                      |    |                                   |\n",
    "| **IDF(t)**   | Inverse document frequency of term t                   |    |                                   |\n",
    "| **           | d                                                      | ** | Length of the document (in words) |\n",
    "| **avg_d**    | Average document length in the corpus                  |    |                                   |\n",
    "| **k₁**       | Controls term frequency saturation (default: 1.2–2.0)  |    |                                   |\n",
    "| **b**        | Controls document length normalization (default: 0.75) |    |                                   |\n",
    "\n",
    "---\n",
    "\n",
    "> **Breaking Down the Components**\n",
    "\n",
    "**1. Term Frequency (TF)**\n",
    "\n",
    "Measures how many times a word appears in a document.\n",
    "\n",
    "In BM25, this is **non-linear** — it saturates after a certain count.\n",
    "So, if a word appears 10 times, it doesn’t make the document 10× more relevant than if it appeared once.\n",
    "\n",
    "[\n",
    "TF(t, d) \\Rightarrow \\frac{TF(t, d) \\times (k_1 + 1)}{TF(t, d) + k_1 \\times (1 - b + b \\times \\frac{|d|}{avg_d})}\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "**2. Inverse Document Frequency (IDF)**\n",
    "\n",
    "[\n",
    "IDF(t) = \\log \\left( \\frac{N - n_t + 0.5}{n_t + 0.5} + 1 \\right)\n",
    "]\n",
    "\n",
    "| Term   | Description                             |\n",
    "| ------ | --------------------------------------- |\n",
    "| **N**  | Total number of documents               |\n",
    "| **nₜ** | Number of documents containing term *t* |\n",
    "\n",
    "IDF ensures **rare terms** across the corpus are **weighted more**, while common terms like “the” or “is” contribute less.\n",
    "\n",
    "\n",
    "\n",
    " **3. Document Length Normalization (b parameter)**\n",
    "\n",
    "| Value of b   | Effect                      |\n",
    "| ------------ | --------------------------- |\n",
    "| **b = 0**    | Ignores document length     |\n",
    "| **b = 1**    | Fully normalizes by length  |\n",
    "| **b = 0.75** | Balanced approach (default) |\n",
    "\n",
    "This adjustment ensures that **longer documents** don’t unfairly gain higher scores simply because they contain more words.\n",
    "\n",
    "---\n",
    "\n",
    "**Parameter Tuning**\n",
    "\n",
    "| Parameter | Range   | Effect                                                                   |\n",
    "| --------- | ------- | ------------------------------------------------------------------------ |\n",
    "| **k₁**    | 1.2–2.0 | Controls term frequency saturation (higher = less saturation)            |\n",
    "| **b**     | 0–1     | Controls document length normalization (higher = stronger normalization) |\n",
    "\n",
    "**Typical Defaults:**\n",
    "\n",
    "* `k₁ = 1.2`\n",
    "* `b = 0.75`\n",
    "\n",
    "You can tune these depending on your dataset:\n",
    "\n",
    "* Short, concise texts (like tweets): lower `b`\n",
    "* Long documents: higher `b`\n",
    "\n",
    "---\n",
    "\n",
    "**Example Calculation**\n",
    "\n",
    "Let’s say we have:\n",
    "\n",
    "* 10,000 documents in total (**N = 10,000**)\n",
    "* Term “LangChain” appears in 100 documents (**nₜ = 100**)\n",
    "* Document *d* contains “LangChain” 3 times (**TF = 3**)\n",
    "* Document length = 500 words (**|d| = 500**)\n",
    "* Average document length = 250 (**avg_d = 250**)\n",
    "* **k₁ = 1.5**, **b = 0.75**\n",
    "\n",
    "**Step 1: Compute IDF**\n",
    "[\n",
    "IDF(LangChain) = \\log \\left( \\frac{10,000 - 100 + 0.5}{100 + 0.5} + 1 \\right) \\approx 3.9\n",
    "]\n",
    "\n",
    "**Step 2: Compute TF Normalization**\n",
    "[\n",
    "\\frac{3 \\times (1.5 + 1)}{3 + 1.5 \\times (1 - 0.75 + 0.75 \\times \\frac{500}{250})} = \\frac{7.5}{3 + 1.5 \\times (1.25)} = \\frac{7.5}{4.875} \\approx 1.54\n",
    "]\n",
    "\n",
    "**Step 3: Final Score**\n",
    "[\n",
    "Score = 3.9 \\times 1.54 \\approx 6.0\n",
    "]\n",
    "\n",
    "So, this document’s relevance score for “LangChain” is **6.0**.\n",
    "Documents with higher scores will rank higher in search results.\n",
    "\n",
    "---\n",
    "\n",
    "**BM25 vs. TF-IDF**\n",
    "\n",
    "| Feature                           | TF-IDF               | BM25                        |\n",
    "| --------------------------------- | -------------------- | --------------------------- |\n",
    "| **Term Saturation**               | Linear               | Non-linear                  |\n",
    "| **Document Length Normalization** | No                   | Yes                         |\n",
    "| **Parameters**                    | None                 | k₁, b                       |\n",
    "| **Accuracy**                      | Moderate             | High                        |\n",
    "| **Used In**                       | Older search engines | Elasticsearch, Lucene, Solr |\n",
    "\n",
    "---\n",
    "\n",
    "**BM25 in Elasticsearch**\n",
    "\n",
    "Elasticsearch uses BM25 as its **default scoring algorithm**.\n",
    "You can confirm or modify it in your index settings:\n",
    "\n",
    "```bash\n",
    "PUT /my_index\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"similarity\": {\n",
    "      \"default\": {\n",
    "        \"type\": \"BM25\",\n",
    "        \"k1\": 1.2,\n",
    "        \"b\": 0.75\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "You can also experiment with custom parameters to optimize retrieval for your specific dataset.\n",
    "\n",
    "---\n",
    "\n",
    "**Applications of BM25**\n",
    "\n",
    "* **Search Engines:** Ranking web pages and documents\n",
    "* **E-commerce:** Product search relevance\n",
    "* **Chatbots:** Keyword retrieval before vector-based matching\n",
    "* **RAG Pipelines:** Combining BM25 (sparse) with embeddings (dense)\n",
    "* **Recommender Systems:** Textual content ranking\n",
    "\n",
    "---\n",
    "\n",
    "**Hybrid Search with BM25 + Vectors**\n",
    "\n",
    "BM25 can be **combined with dense vector embeddings** for **hybrid retrieval** — merging keyword relevance and semantic understanding.\n",
    "\n",
    "```bash\n",
    "POST /hybrid_search/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"should\": [\n",
    "        { \"match\": { \"text\": \"LangChain RAG\" }},\n",
    "        {\n",
    "          \"script_score\": {\n",
    "            \"query\": { \"match_all\": {} },\n",
    "            \"script\": {\n",
    "              \"source\": \"cosineSimilarity(params.vector, 'embedding') + 1.0\",\n",
    "              \"params\": { \"vector\": [0.12, 0.45, ...] }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This **BM25 + embedding synergy** is the foundation of **RAG pipelines** in systems like **Elasticsearch, Pinecone, and Weaviate**.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Aspect                           | Description                                               |\n",
    "| -------------------------------- | --------------------------------------------------------- |\n",
    "| **Full Name**                    | Best Match 25                                             |\n",
    "| **Type**                         | Probabilistic ranking model                               |\n",
    "| **Core Improvement Over TF-IDF** | Term frequency saturation + document length normalization |\n",
    "| **Key Parameters**               | k₁ (term frequency), b (document length)                  |\n",
    "| **Used In**                      | Elasticsearch, Lucene, Solr, Vespa                        |\n",
    "| **Ideal Use Cases**              | Text search, retrieval, hybrid RAG                        |\n",
    "| **Advantages**                   | Accurate, tunable, scalable, interpretable                |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a7d7d",
   "metadata": {},
   "source": [
    "## **Elasticsearch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee8083",
   "metadata": {},
   "source": [
    "**What is Elasticsearch?**\n",
    "\n",
    "> **Elasticsearch** is a **distributed, RESTful search and analytics engine** built on top of **Apache Lucene**.\n",
    "It is designed to **store, search, and analyze massive amounts of structured and unstructured data** in near real-time.\n",
    "\n",
    "Developed by **Elastic NV**, it powers\n",
    "* Search engines\n",
    "* Log analytics (via Elastic Stack / ELK)\n",
    "* Recommendation systems\n",
    "* Vector and hybrid retrieval (semantic + keyword)\n",
    "\n",
    "---\n",
    "\n",
    "**Key Features**\n",
    "\n",
    "| Feature                   | Description                                              |\n",
    "| ------------------------- | -------------------------------------------------------- |\n",
    "| **Full-Text Search**      | Advanced text analysis and relevance ranking             |\n",
    "| **Scalability**           | Handles petabytes of data using sharding and replication |\n",
    "| **Real-Time Indexing**    | Data becomes searchable within seconds of ingestion      |\n",
    "| **RESTful API**           | Accessible via HTTP/JSON APIs                            |\n",
    "| **Aggregation Framework** | Enables complex data analytics                           |\n",
    "| **Vector Search**         | Supports dense vector similarity for AI-powered search   |\n",
    "| **Schema Flexibility**    | JSON-based documents, schema-free or schema-mapped       |\n",
    "\n",
    "---\n",
    "\n",
    "**Core Concepts**\n",
    "\n",
    "Understanding the building blocks of Elasticsearch:\n",
    "\n",
    "| Concept      | Description                                                            | Analogy                  |\n",
    "| ------------ | ---------------------------------------------------------------------- | ------------------------ |\n",
    "| **Cluster**  | A collection of one or more nodes (servers) that holds the entire data | A database system        |\n",
    "| **Node**     | A single running instance of Elasticsearch                             | A single database server |\n",
    "| **Index**    | A logical namespace for related documents                              | A database or table      |\n",
    "| **Document** | A JSON object containing fields (data)                                 | A row in a table         |\n",
    "| **Shard**    | A subset of an index that stores part of the data                      | A partition              |\n",
    "| **Replica**  | A copy of a shard for fault tolerance                                  | A backup                 |\n",
    "\n",
    "---\n",
    "\n",
    "**How Elasticsearch Works**\n",
    "\n",
    "> 1. **Indexing Phase**\n",
    "\n",
    "Data is sent to Elasticsearch as a **JSON document** through its REST API.\n",
    "\n",
    "Example:\n",
    "\n",
    "```bash\n",
    "POST /library/_doc/1\n",
    "{\n",
    "  \"title\": \"Learning LangChain\",\n",
    "  \"author\": \"Panduka Bandara\",\n",
    "  \"tags\": [\"AI\", \"LLM\", \"RAG\"],\n",
    "  \"year\": 2025\n",
    "}\n",
    "```\n",
    "\n",
    "Elasticsearch:\n",
    "\n",
    "* Tokenizes the text\n",
    "* Builds an **inverted index** (maps terms → documents)\n",
    "* Stores metadata for fast lookups\n",
    "\n",
    "\n",
    "\n",
    "> 2. **Searching Phase**\n",
    "\n",
    "Queries are made using the **Query DSL** (Domain Specific Language):\n",
    "\n",
    "```bash\n",
    "GET /library/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"title\": \"LangChain\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Results are ranked by **relevance score** using the **BM25 algorithm** (an improvement over TF-IDF).\n",
    "\n",
    "\n",
    "\n",
    "**Inverted Index: The Heart of Elasticsearch**\n",
    "\n",
    "An **inverted index** is similar to a dictionary:\n",
    "\n",
    "* Words (terms) → list of documents that contain them.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Term        | Documents |\n",
    "| ----------- | --------- |\n",
    "| \"LangChain\" | 1, 3      |\n",
    "| \"RAG\"       | 1, 2      |\n",
    "| \"Python\"    | 2, 4      |\n",
    "\n",
    "When you search for “LangChain RAG,” Elasticsearch quickly finds documents 1, 2, 3 from the inverted index and ranks them.\n",
    "\n",
    "---\n",
    "\n",
    "**Architecture Overview**\n",
    "\n",
    "```\n",
    "+--------------------------------------------------------+\n",
    "|                   Elasticsearch Cluster                |\n",
    "|                                                        |\n",
    "|  +---------------+      +---------------+              |\n",
    "|  |   Node 1      |      |   Node 2      |              |\n",
    "|  | (Primary)     |      | (Replica)     |              |\n",
    "|  +---------------+      +---------------+              |\n",
    "|         |                        |                    |\n",
    "|       [Shards]  <----->       [Shards]                |\n",
    "+--------------------------------------------------------+\n",
    "```\n",
    "\n",
    "Each index is divided into **shards**, distributed across nodes for scalability and fault tolerance.\n",
    "\n",
    "---\n",
    "\n",
    "**Search Algorithms and Scoring**\n",
    "\n",
    "Elasticsearch uses the **BM25** ranking function, which improves on TF-IDF:\n",
    "\n",
    "[\n",
    "\\text{score}(q, d) = \\sum_{t \\in q} IDF(t) \\times \\frac{TF(t, d) \\times (k + 1)}{TF(t, d) + k \\times (1 - b + b \\times \\frac{|d|}{avg_d})}\n",
    "]\n",
    "\n",
    "* **TF** → Term frequency in document\n",
    "* **IDF** → Inverse document frequency\n",
    "* **k, b** → Tuning parameters controlling term saturation and length normalization\n",
    "\n",
    "This results in **relevance scores** that rank the most relevant documents higher.\n",
    "\n",
    "---\n",
    "\n",
    "**Vector Search in Elasticsearch**\n",
    "\n",
    "Since **v8.0**, Elasticsearch supports **dense vector fields** for **semantic and hybrid search**.\n",
    "\n",
    "> Example: Creating an Index with Vector Fields\n",
    "\n",
    "```bash\n",
    "PUT /semantic_index\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"text\": { \"type\": \"text\" },\n",
    "      \"embedding\": { \n",
    "        \"type\": \"dense_vector\",\n",
    "        \"dims\": 1536,\n",
    "        \"index\": true,\n",
    "        \"similarity\": \"cosine\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "> Inserting Data\n",
    "\n",
    "```bash\n",
    "POST /semantic_index/_doc/1\n",
    "{\n",
    "  \"text\": \"LangChain integrates with vector databases like Pinecone.\",\n",
    "  \"embedding\": [0.12, 0.33, ...]  // vector from OpenAI or Hugging Face\n",
    "}\n",
    "```\n",
    "\n",
    "> Querying by Vector Similarity\n",
    "\n",
    "```bash\n",
    "POST /semantic_index/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"script_score\": {\n",
    "      \"query\": { \"match_all\": {} },\n",
    "      \"script\": {\n",
    "        \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "        \"params\": { \"query_vector\": [0.14, 0.31, ...] }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "This enables **semantic retrieval**, where documents are matched by meaning rather than exact keywords.\n",
    "\n",
    "---\n",
    "\n",
    "**Hybrid Search**\n",
    "\n",
    "Elasticsearch can combine **sparse** (BM25/TF-IDF) and **dense** (vector) retrieval:\n",
    "\n",
    "```bash\n",
    "POST /hybrid_search/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"should\": [\n",
    "        { \"match\": { \"text\": \"LangChain embeddings\" }},\n",
    "        {\n",
    "          \"script_score\": {\n",
    "            \"query\": { \"match_all\": {} },\n",
    "            \"script\": {\n",
    "              \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "              \"params\": { \"query_vector\": [0.15, 0.42, ...] }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "→ This approach gives both **keyword precision** and **semantic understanding**, ideal for **RAG pipelines**.\n",
    "\n",
    "---\n",
    "\n",
    "**Deployment Options**\n",
    "\n",
    "| Environment          | Description                                                |\n",
    "| -------------------- | ---------------------------------------------------------- |\n",
    "| **Self-Managed**     | Install Elasticsearch on local or cloud servers            |\n",
    "| **Elastic Cloud**    | Managed service provided by Elastic.co                     |\n",
    "| **AWS OpenSearch**   | Amazon’s managed fork of Elasticsearch                     |\n",
    "| **Kubernetes (ECK)** | Elastic Cloud on Kubernetes for containerized environments |\n",
    "\n",
    "---\n",
    "\n",
    "**Integration with LangChain**\n",
    "\n",
    "LangChain provides direct integration via `ElasticsearchStore`:\n",
    "\n",
    "```python\n",
    "from langchain_community.vectorstores import ElasticsearchStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = ElasticsearchStore(\n",
    "    index_name=\"langchain_docs\",\n",
    "    embedding=embeddings,\n",
    "    es_url=\"http://localhost:9200\"\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "vectorstore.add_texts([\"LangChain integrates with Elasticsearch\"])\n",
    "# Perform search\n",
    "results = vectorstore.similarity_search(\"integration with vector stores\")\n",
    "```\n",
    "\n",
    "This forms the foundation of **RAG (Retrieval-Augmented Generation)** pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "**Security and Access Control**\n",
    "\n",
    "Elasticsearch offers multiple security features:\n",
    "\n",
    "* **API Key Authentication**\n",
    "* **Role-Based Access Control (RBAC)**\n",
    "* **TLS Encryption for communication**\n",
    "* **Index-level and field-level access policies**\n",
    "* **Audit logging and monitoring**\n",
    "\n",
    "---\n",
    "\n",
    "**Real-World Applications**\n",
    "\n",
    "| Domain                  | Use Case                                        |\n",
    "| ----------------------- | ----------------------------------------------- |\n",
    "| **E-commerce**          | Product search and recommendations              |\n",
    "| **Log Analytics (ELK)** | Centralized monitoring with Logstash and Kibana |\n",
    "| **Chatbots**            | Context retrieval for LLMs                      |\n",
    "| **Cybersecurity**       | Threat detection and log correlation            |\n",
    "| **Knowledge Graphs**    | Semantic document retrieval                     |\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Aspect                | Description                                           |\n",
    "| --------------------- | ----------------------------------------------------- |\n",
    "| **Type**              | Distributed search & analytics engine                 |\n",
    "| **Core Strength**     | Full-text + semantic + hybrid retrieval               |\n",
    "| **Underlying Engine** | Apache Lucene                                         |\n",
    "| **Supports**          | Sparse, dense, and hybrid vector search               |\n",
    "| **Best Use Cases**    | Enterprise search, RAG, log analytics, recommendation |\n",
    "| **Integrations**      | LangChain, OpenAI, Hugging Face, Pinecone, Kibana     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a8d1e7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
