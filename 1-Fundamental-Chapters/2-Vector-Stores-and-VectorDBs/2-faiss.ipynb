{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a7036e",
   "metadata": {},
   "source": [
    "# Building a RAG System with LangChain and FAISS \n",
    "Introduction to RAG (Retrieval-Augmented Generation)\n",
    "RAG combines the power of retrieval systems with generative AI models. Instead of relying solely on the model's training data, RAG:\n",
    "\n",
    "1. Retrieves relevant documents from a knowledge base\n",
    "2. Uses these documents as context for the LLM\n",
    "3. Generates responses based on both the retrieved context and the model's knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e937ab9f",
   "metadata": {},
   "source": [
    "### FAISS \n",
    "https://github.com/facebookresearch/faiss\n",
    "\n",
    "FAISS is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "Key advantages:\n",
    "1. Extremely fast similarity search\n",
    "2. Memory efficient\n",
    "3. Supports GPU acceleration\n",
    "4. Can handle millions of vectors\n",
    "\n",
    "How it works:\n",
    "- Indexes vectors for fast nearest neighbor search\n",
    "- Returns most similar vectors based on distance metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0af024c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain core imports\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough, \n",
    " \n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# LangChain specific imports\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4adfdc",
   "metadata": {},
   "source": [
    "### Data Ingestion And Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "511190a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'AI Introduction', 'page': 1, 'topic': 'AI'}, page_content='\\n        Artificial Intelligence (AI) is the simulation of human intelligence in machines.\\n        These systems are designed to think like humans and mimic their actions.\\n        AI can be categorized into narrow AI and general AI.\\n        '), Document(metadata={'source': 'ML Basics', 'page': 1, 'topic': 'ML'}, page_content='\\n        Machine Learning is a subset of AI that enables systems to learn from data.\\n        Instead of being explicitly programmed, ML algorithms find patterns in data.\\n        Common types include supervised, unsupervised, and reinforcement learning.\\n        '), Document(metadata={'source': 'Deep Learning', 'page': 1, 'topic': 'DL'}, page_content='\\n        Deep Learning is a subset of machine learning based on artificial neural networks.\\n        It uses multiple layers to progressively extract higher-level features from raw input.\\n        Deep learning has revolutionized computer vision, NLP, and speech recognition.\\n        '), Document(metadata={'source': 'NLP Overview', 'page': 1, 'topic': 'NLP'}, page_content='\\n        Natural Language Processing (NLP) is a branch of AI that helps computers understand human language.\\n        It combines computational linguistics with machine learning and deep learning models.\\n        Applications include chatbots, translation, sentiment analysis, and text summarization.\\n        ')]\n"
     ]
    }
   ],
   "source": [
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Artificial Intelligence (AI) is the simulation of human intelligence in machines.\n",
    "        These systems are designed to think like humans and mimic their actions.\n",
    "        AI can be categorized into narrow AI and general AI.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"AI Introduction\", \"page\": 1, \"topic\": \"AI\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Machine Learning is a subset of AI that enables systems to learn from data.\n",
    "        Instead of being explicitly programmed, ML algorithms find patterns in data.\n",
    "        Common types include supervised, unsupervised, and reinforcement learning.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"ML Basics\", \"page\": 1, \"topic\": \"ML\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Deep Learning is a subset of machine learning based on artificial neural networks.\n",
    "        It uses multiple layers to progressively extract higher-level features from raw input.\n",
    "        Deep learning has revolutionized computer vision, NLP, and speech recognition.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"Deep Learning\", \"page\": 1, \"topic\": \"DL\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Natural Language Processing (NLP) is a branch of AI that helps computers understand human language.\n",
    "        It combines computational linguistics with machine learning and deep learning models.\n",
    "        Applications include chatbots, translation, sentiment analysis, and text summarization.\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"NLP Overview\", \"page\": 1, \"topic\": \"NLP\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(sample_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "028d3f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Artificial Intelligence (AI) is the simulation of human intelligence in machines.\n",
      "        These systems are designed to think like humans and mimic their actions.\n",
      "        AI can be categorized into narrow AI and general AI.' metadata={'source': 'AI Introduction', 'page': 1, 'topic': 'AI'}\n",
      "page_content='Machine Learning is a subset of AI that enables systems to learn from data.\n",
      "        Instead of being explicitly programmed, ML algorithms find patterns in data.\n",
      "        Common types include supervised, unsupervised, and reinforcement learning.' metadata={'source': 'ML Basics', 'page': 1, 'topic': 'ML'}\n"
     ]
    }
   ],
   "source": [
    "## text splitting\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\" \"]\n",
    ")\n",
    "\n",
    "## split the documents into chunks\n",
    "chunks = text_splitter.split_documents(sample_documents)\n",
    "print(chunks[0])\n",
    "print(chunks[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed228b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4 chunks from 4 documents\n",
      "\n",
      "Example chunk:\n",
      "Content: Artificial Intelligence (AI) is the simulation of human intelligence in machines.\n",
      "        These systems are designed to think like humans and mimic their actions.\n",
      "        AI can be categorized into narrow AI and general AI.\n",
      "Metadata: {'source': 'AI Introduction', 'page': 1, 'topic': 'AI'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Created {len(chunks)} chunks from {len(sample_documents)} documents\")\n",
    "print(\"\\nExample chunk:\")\n",
    "print(f\"Content: {chunks[0].page_content}\")\n",
    "print(f\"Metadata: {chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1051ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the embedding models\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI embeddings with the latest model\n",
    "\n",
    "embeddings=OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=1536\n",
    ")\n",
    "\n",
    "## Example: create a embedding for a single text\n",
    "sample_text=\"What is machine learning\"\n",
    "sample_embedding=embeddings.embed_query(sample_text)\n",
    "sample_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[\"AI\",\"MAchine learning\",\"Deep Learning\",\"Neural Network\"]\n",
    "batch_embeddings=embeddings.embed_documents(texts)\n",
    "print(batch_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ce23d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01823362149298191, 0.01074262149631977, 0.017460593953728676, -0.03524021804332733, 0.037424325942993164, 0.02608659490942955, 0.015890000388026237, 0.008374459110200405, -0.010945080779492855, 0.034872107207775116, -0.006638216320425272, -0.06866443157196045, -0.0323689728975296, 0.008969567716121674, 0.03658994659781456, 0.018712162971496582, 0.016270378604531288, -0.015399189665913582, 0.020614054054021835, 0.012945135124027729, -0.00261049997061491, 0.03781697154045105, 0.03403773158788681, -0.030013080686330795, 0.020650865510106087, 0.007926594465970993, 0.03614821657538414, 0.03766972944140434, 0.01052175648510456, -0.03482302650809288, -0.010932810604572296, -0.024233784526586533, -0.013914486393332481, -0.013865405693650246, 0.011945107951760292, -0.0018451418727636337, -0.027583567425608635, 0.027828972786664963, -0.04508097469806671, -0.005190324503928423, -0.019755134359002113, -0.04277416318655014, 0.03438129648566246, 0.06827178597450256, -0.020577242597937584, -0.03489664942026138, -0.03131372854113579, -0.018785784021019936, 0.0006031604716554284, 0.08333967626094818, -0.055559784173965454, -0.05678681284189224, -0.0374734066426754, 0.011509513482451439, -0.037080757319927216, 0.020810378715395927, 0.03617275878787041, 0.01665075682103634, 0.02763264812529087, 0.029644973576068878, -0.02220918983221054, -0.020479081198573112, 0.02709275670349598, 0.005935743451118469, -0.032000865787267685, -0.06714291870594025, -0.03578010946512222, 0.017644649371504784, -0.051240649074316025, -0.03845502808690071, -0.01696978323161602, -0.026012973859906197, -0.025154054164886475, -0.012846972793340683, -0.012859242968261242, 0.005107500124722719, 0.001053709420375526, 0.03516659513115883, -0.04368216171860695, 0.009282459504902363, 0.013804053887724876, -0.006162743084132671, 0.014527999795973301, -0.026012973859906197, -0.04854118824005127, -0.04962097480893135, -0.016982054337859154, -0.025620324537158012, -0.06262745708227158, -0.029988540336489677, -0.02271226979792118, 0.004576810635626316, -0.01763237826526165, 0.06557232141494751, 0.0029095879290252924, 0.06478703022003174, 0.026160215958952904, -0.06478703022003174, -0.010570838116109371, 0.03379232436418533, -0.0623820535838604, -0.008693486452102661, 0.043264973908662796, -0.012883784249424934, -0.0007304645259864628, -0.053105730563402176, -0.020295027643442154, -0.04378032311797142, -0.025448540225625038, 0.01696978323161602, -0.04407481104135513, -0.03040573000907898, -0.06297102570533752, 0.019914649426937103, 0.008681216277182102, -0.041080866008996964, 0.01006162166595459, 0.014074000529944897, -0.01624583825469017, -0.019693784415721893, -0.02962043322622776, 0.012221189215779305, 0.019755134359002113, -0.07367070019245148, -0.018049567937850952, 0.02706821635365486, -0.0018129324307665229, 0.00823335163295269, -0.037080757319927216, -0.0080922432243824, -0.0006948040681891143, 0.013055567629635334, 0.0007837635348550975, -0.038872215896844864, 0.030430270358920097, -0.0161844864487648, 0.015570973046123981, 0.04066367447376251, -0.03860227018594742, -0.010865324176847935, 0.02974313497543335, -0.00604924326762557, -0.04287232458591461, 0.06670118868350983, -0.010135243646800518, -0.015902269631624222, -0.027362702414393425, 0.02807437814772129, 0.008049297146499157, -0.01155859511345625, -0.0018788852030411363, -0.013411405496299267, -0.05678681284189224, 0.019288865849375725, -0.017521945759654045, -0.028000757098197937, -0.03337513655424118, -0.0011994189117103815, -0.021215297281742096, -0.03835686668753624, -0.001659554080106318, 0.01536237820982933, 0.0476822704076767, -0.00046703717089258134, -0.02608659490942955, 0.049694593995809555, -0.016074053943157196, 0.007239459548145533, -0.015485080890357494, -0.024565082043409348, 0.006840675603598356, 0.008423540741205215, 0.01851583831012249, -0.04255329817533493, -0.004239378497004509, 0.00044709796202369034, -0.013386865146458149, -0.05492173135280609, -0.05413643270730972, -0.026675567030906677, -0.003127385163679719, 0.016920702531933784, -0.020957620814442635, 0.03627092018723488, -0.02182881161570549, -0.014785675331950188, 0.023227620869874954, 0.07043135166168213, -0.016822541132569313, 0.004549202509224415, -0.012503405101597309, 0.030773837119340897, 0.06439437717199326, 0.0331297293305397, 0.025227675214409828, -0.0016043378273025155, 0.049252863973379135, 0.028933297842741013, 0.03126464784145355, -0.01735016144812107, 0.03879859298467636, -0.015337837859988213, -0.01598816178739071, -0.0008842263487167656, 0.01520286500453949, -0.00198318250477314, 0.010208864696323872, 0.009754864498972893, -0.036099135875701904, 0.016601676121354103, 0.010227270424365997, -0.05560886487364769, 0.033326055854558945, -0.015485080890357494, -0.049571890383958817, -0.01706794649362564, 0.011012567207217216, -0.02863881178200245, -0.015951352193951607, 0.06414896994829178, 0.0064848377369344234, 0.00988983828574419, 0.050136324018239975, 0.008380594663321972, -0.0227368101477623, -0.032565295696258545, 0.021853351965546608, -0.058308325707912445, 0.04130173102021217, -0.017104756087064743, 0.022724540904164314, 0.006736378185451031, 0.0006254003383219242, -0.033669620752334595, 0.015730487182736397, 0.0038743377663195133, -0.02293313480913639, -0.0190802700817585, -0.02520313486456871, -0.01255248673260212, 0.004119743127375841, 0.05271308124065399, -0.0007408175733871758, 0.026577405631542206, -0.015681404620409012, 0.022221459075808525, 0.007110621780157089, 0.029055999591946602, 0.019006649032235146, 0.021914701908826828, 0.017607837915420532, -0.013607730157673359, -0.015288757160305977, -0.0023436215706169605, -0.011172081343829632, 0.012392972595989704, -0.029571350663900375, 0.03359600156545639, -0.028467027470469475, -0.0035553108900785446, -0.027043676003813744, 0.01857719011604786, -0.03104378469288349, -0.00601550005376339, 0.00623022997751832, -0.012006459757685661, 0.015006540343165398, -0.034430377185344696, 0.0023390203714370728, -0.021055784076452255, 0.016454432159662247, 0.00579463504254818, -0.01608632504940033, -0.006282378453761339, -0.014454378746449947, -0.02192697301506996, -0.02454053983092308, 0.025792108848690987, -0.004202567506581545, 0.005714878439903259, 0.008503297343850136, -0.033645082265138626, 0.08981838077306747, 0.008423540741205215, -0.024687783792614937, 0.006846810691058636, -0.009423567913472652, -0.03052843175828457, -0.044590163975954056, -0.007718000095337629, 0.01930113509297371, -0.025595784187316895, -0.0035062297247350216, -0.040050163865089417, 0.023448485881090164, 0.04017286375164986, 0.01735016144812107, 0.044614702463150024, -0.00018798437668010592, 0.03855318948626518, 0.04579265043139458, -0.00770572992041707, -0.017460593953728676, -0.008877540938556194, -0.03695805370807648, 0.03995199874043465, -0.056884974241256714, -0.03629545867443085, 0.024025188758969307, -0.04186616092920303, -0.030675675719976425, 0.024368757382035255, 0.025227675214409828, 0.032982487231492996, -0.03175545856356621, -0.02851610817015171, -0.015951352193951607, -0.00823335163295269, 0.019755134359002113, -0.08500843495130539, -0.04949827119708061, 0.005877459421753883, 0.010343837551772594, -0.03703167662024498, -0.036540865898132324, 0.015730487182736397, -0.022626377642154694, -0.0012001858558505774, -0.0017423783428967, -0.05173145979642868, -0.00439889170229435, 0.057326704263687134, -0.018748972564935684, -0.01986556686460972, 0.01111686509102583, -0.0002902685955632478, -0.02618475630879402, 0.032982487231492996, -0.051240649074316025, -0.011779459193348885, -0.05850464850664139, -0.03904400020837784, -0.021522054448723793, 0.014343946240842342, -0.004696446005254984, 0.001791459508240223, -0.02431967668235302, 0.009104540571570396, -0.02321535162627697, 0.0008175067487172782, -0.011239567771553993, -0.053449299186468124, 0.043510377407073975, 0.005380513612180948, -0.030013080686330795, -0.021239837631583214, -0.02072448655962944, -0.01791459508240223, 0.04014832526445389, -0.014466648921370506, -0.05713037773966789, 0.007435783743858337, -0.0017439121147617698, 0.031362809240818024, 0.018319513648748398, 0.047044217586517334, -0.010994162410497665, -0.023853406310081482, 0.031559135764837265, 0.022761352360248566, 0.012435918673872948, -0.05624691769480705, -0.01971832476556301, -0.025068162009119987, -0.05501989275217056, -0.04589081183075905, -0.005398918874561787, -3.750580799533054e-05, -0.01136840507388115, 0.032786160707473755, 0.03759610652923584, -0.006325324531644583, 0.02277362160384655, -0.026773730292916298, 0.006171945948153734, 0.025620324537158012, -0.003635067492723465, -0.013583188876509666, -0.023951567709445953, -0.013411405496299267, -0.023608000949025154, -0.03668810799717903, -0.03624637797474861, 0.017080215737223625, -0.015018810518085957, 0.01728881150484085, -0.00043904560152441263, -0.005739418789744377, 0.0009033986716531217, 0.02974313497543335, -0.02176745980978012, -0.00976100005209446, -0.0017055675853043795, 0.019755134359002113, -0.002612033858895302, 0.017386972904205322, -0.0216692965477705, -0.007257864810526371, -0.008914351463317871, 0.04721600189805031, 0.04540000110864639, -0.0579647570848465, 0.005205661989748478, 0.03882313519716263, -0.014270324259996414, 0.0012684392277151346, -0.022258270531892776, 0.009994135238230228, 0.012613837607204914, -0.002628905465826392, -0.00961989164352417, 0.02831978350877762, 0.013080107979476452, -0.01503108162432909, 0.03435675799846649, 0.03582919016480446, 0.05109340697526932, -0.03006216324865818, -0.0027960878796875477, -0.03511751443147659, 0.012307081371545792, 0.025031352415680885, 0.02331351302564144, -0.0231539998203516, -0.03948573023080826, -0.004374351352453232, -0.017460593953728676, 0.023043567314743996, -0.031117405742406845, -0.0016764256870374084, 0.042602378875017166, 0.03492118790745735, -0.032099027186632156, 0.00032017737976275384, 0.006711837835609913, 0.01536237820982933, -0.08942572772502899, 0.013472756370902061, -0.01763237826526165, -0.03141189366579056, 0.02596389129757881, -0.03413589298725128, -0.0025445471983402967, 0.03440583869814873, 0.025890270248055458, -0.05408735200762749, 0.03339967504143715, -0.01817226968705654, 0.06115502864122391, 0.006349864881485701, 0.011484973132610321, -0.027706271037459373, -0.016945242881774902, 0.04135081171989441, 0.009895972907543182, 0.01813546009361744, 0.010889864526689053, -0.009214973077178001, 0.0035890541039407253, 0.03104378469288349, -0.009214973077178001, -0.022969946265220642, 0.03857773169875145, -0.04186616092920303, -0.02393929660320282, -0.03261438012123108, 0.02104351297020912, -0.02750994637608528, -0.0020706080831587315, 0.026012973859906197, -0.04586626961827278, -0.025497620925307274, -0.01458935160189867, -0.013141459785401821, -0.0227368101477623, 0.057866595685482025, -0.011307054199278355, -0.015840919688344002, 0.02831978350877762, 0.01696978323161602, -0.009650567546486855, -0.024147892370820045, 0.06733924150466919, -0.011705838143825531, -0.029694054275751114, 0.017460593953728676, -0.04012378305196762, -0.03666356950998306, 0.008730296976864338, -0.022859513759613037, -0.013632270507514477, -0.030552973970770836, -0.043289512395858765, -0.02554670348763466, -0.015620053745806217, 0.0068836216814816, 0.004490918945521116, -0.028025297448039055, -0.015264215879142284, 0.018466757610440254, -0.002187175676226616, 0.04419751465320587, -0.032099027186632156, 0.008981837891042233, -0.001416449318639934, 0.027019135653972626, -0.001052942592650652, 0.01750967651605606, 0.008902081288397312, 0.0017362432554364204, 0.005935743451118469, -0.03636908158659935, -0.05845556780695915, -0.013632270507514477, 0.005466405302286148, -0.0572776235640049, -0.028148001059889793, 0.002199445851147175, -0.03106832504272461, 0.025792108848690987, 0.006951108109205961, 0.015386918559670448, 0.05987891927361488, 0.006791594438254833, -0.043338593095541, -0.019227514043450356, -0.014896107837557793, -0.006730243097990751, -0.01930113509297371, 0.03425859659910202, 0.02261410839855671, 0.0016273446381092072, 0.017460593953728676, 0.02728908136487007, -0.025669405236840248, 0.04132626950740814, -0.021018972620368004, -0.015902269631624222, 0.02819708175957203, 0.03052843175828457, 0.039093080908060074, 0.009895972907543182, 0.004377419129014015, -0.024246053770184517, 0.03578010946512222, -0.00044978209189139307, -0.00657686498016119, -0.033669620752334595, -0.014712054282426834, -0.014037189073860645, 0.005647392012178898, -0.019877837970852852, 0.029325945302844048, 0.008871405385434628, -0.018049567937850952, 0.04243059456348419, -0.007540081162005663, -0.058848217129707336, 0.012945135124027729, -0.010865324176847935, -0.019693784415721893, 0.019068000838160515, 0.032442595809698105, -0.010294756852090359, -0.006460297387093306, -0.006214892026036978, 0.024614162743091583, -0.020196864381432533, 0.006932702846825123, 0.0017040338134393096, -0.03308064863085747, -0.03489664942026138, -0.040614593774080276, 0.0022209188900887966, 0.00015730870654806495, 0.009583081118762493, -0.006601405330002308, 0.020233675837516785, -0.005880527198314667, 0.00028969341656193137, 0.050234485417604446, -0.01536237820982933, -0.01606178469955921, -0.02908053994178772, -0.009552405215799809, -0.015043351799249649, 0.010122973471879959, -0.01558324322104454, -0.014037189073860645, -0.011331594549119473, 0.0016012702835723758, -0.0037608379498124123, 0.02962043322622776, -0.02277362160384655, 0.019951459020376205, 0.010159783996641636, -0.021215297281742096, 0.03943664953112602, -0.007767081260681152, -0.003714824328199029, -0.00848489161580801, 0.005064554046839476, 0.015853188931941986, -0.0506516769528389, -0.005294621456414461, -0.0039602299220860004, 0.005128973163664341, 0.022098757326602936, 0.06478703022003174, -0.03295794501900673, -0.029718594625592232, -0.03558378294110298, 0.04314227029681206, -0.0031442567706108093, 0.002612033858895302, -0.022160109132528305, 0.002199445851147175, -0.020479081198573112, 0.0026381080970168114, -0.00883459486067295, -0.024552810937166214, 0.032982487231492996, 0.044271133840084076, -0.05359654128551483, 0.008221081458032131, 0.009951189160346985, 0.001254635164514184, -0.03406227007508278, 0.013779513537883759, -0.020528161898255348, -0.004607486538589001, -0.007509405259042978, 0.006975648459047079, -0.01503108162432909, 0.052222270518541336, 0.009662837721407413, -0.013190540485084057, 0.0039602299220860004, -0.04034464806318283, -0.006570729892700911, -0.02425832487642765, -0.027485406026244164, 0.006380540784448385, -0.004809945821762085, 0.03651632368564606, -0.02227053977549076, 0.02652832493185997, -0.01602497324347496, 0.019129350781440735, -0.00936835166066885, -0.036418162286281586, 0.028884217143058777, -0.00757075659930706, -0.023288972675800323, -0.03629545867443085, 0.02135027013719082, -0.017497405409812927, 0.038651350885629654, 0.04446746036410332, 0.009178162552416325, 0.013521838001906872, 0.013509567826986313, -0.033326055854558945, -0.011687432415783405, -0.012675189413130283, -0.06326551735401154, 0.0019218310480937362, -0.04689697176218033, -0.0016748919151723385, 0.012896054424345493, -0.01602497324347496, 0.024393297731876373, -0.046479783952236176, -0.018871676176786423, -0.009951189160346985, -0.03425859659910202, 0.031338270753622055, 0.0345776230096817, 0.0007642077980563045, -0.008147459477186203, -0.023669350892305374, -0.05305664986371994, -0.0001228944311151281, 0.01914162188768387, -0.0242215134203434, -0.005908135324716568, -0.016172217205166817, 0.050897080451250076, -0.017325621098279953, -0.010116837918758392, -0.00020725253853015602, 0.03335059434175491, -0.008895945735275745, 0.0037608379498124123, -0.016233567148447037, -0.011233432218432426, 0.0013612330658361316, 0.026282919570803642, 0.05207502841949463, -0.012675189413130283, -0.03735070303082466, -0.021620215848088264, -0.0035277027636766434, 0.008067702874541283, -0.02028275653719902, -0.017620109021663666, 0.014650702476501465, 0.0024111082311719656, 0.03207448497414589, -0.043510377407073975, -0.01165675651282072, 0.0049142432399094105, -0.03327697142958641, -0.000840513501316309, -0.010748757049441338, -0.029105082154273987, -0.041841622442007065, -0.00884686503559351, 0.006527783814817667, 0.004067594651132822, 0.022380972281098366, -0.015460540540516376, 0.028663352131843567, -0.017104756087064743, 0.013877675868570805, -0.011307054199278355, -0.03727708011865616, -0.020699946209788322, -0.0032148107420653105, -0.039853837341070175, -0.0062517025507986546, 0.0024264459498226643, 0.011779459193348885, 0.005699540488421917, -0.004819148685783148, 0.01962016150355339, 0.02564486488699913, 0.02233189158141613, 0.0004678040568251163, -0.03435675799846649, -0.024172432720661163, 0.008282432332634926, 0.011613810434937477, -0.01135613489896059, 0.005306892096996307, -0.015325567685067654, -0.0016426824731752276, -0.010552432388067245, -0.021092595532536507, 0.000668729713652283, 0.014871567487716675, 0.05462724342942238, 0.02851610817015171, 0.00613513495773077, 0.02198832482099533, -0.04633254185318947, -0.01052175648510456, 0.003257756819948554, -0.023804323747754097, 0.009264053776860237, 0.008356054313480854, -0.00224699336104095, -0.004757797345519066, 0.014736594632267952, 0.01088372990489006, -0.011257973499596119, 0.04566994681954384, 0.004018513485789299, -0.005208729766309261, 0.03693351522088051, 0.009104540571570396, -0.005015472881495953, 0.010030945762991905, -0.029914919286966324, -0.037547025829553604, -0.006736378185451031, -0.015607783570885658, -0.021841080859303474, -0.014601621776819229, -0.0015253479359671474, -0.01757102645933628, 0.05708129703998566, 0.03126464784145355, -0.012577027082443237, 0.0020629391074180603, 0.01596362143754959, -0.009472648613154888, -0.05958443135023117, 0.001224726322107017, 0.014135351404547691, -0.02135027013719082, 0.030332108959555626, 0.005604445934295654, 0.005739418789744377, 0.009705783799290657, 0.009257919155061245, -0.027117297053337097, -0.007994080893695354, -0.03082291968166828, -0.005417324136942625, 0.04400118812918663, 0.019853297621011734, -0.03050389140844345, 0.014773405157029629, -0.028000757098197937, -0.01227026991546154, -0.002082878490909934, 0.006104459520429373, -0.018184540793299675, -0.024945460259914398, 0.006552324164658785, -0.021693838760256767, 0.004346743226051331, -0.0002751224674284458, 0.028000757098197937, 0.018061837181448936, 0.018209081143140793, -0.007613702677190304, -0.04517913609743118, -0.007129027042537928, 0.045817188918590546, 0.013227351009845734, 0.03524021804332733, 0.013742703013122082, 0.009871432557702065, -0.03494573011994362, 0.016712108626961708, -0.0036381352692842484, 0.0012193580623716116, 0.029718594625592232, -0.031804539263248444, -0.003898878348991275, 0.020552702248096466, 0.011442027054727077, -0.02470005489885807, 0.011583135463297367, -0.0019816486164927483, 0.0044080945663154125, -0.025620324537158012, 0.0019264324801042676, 0.003929554019123316, -0.006840675603598356, -0.014871567487716675, 0.0021825742442160845, 0.01415989175438881, 0.005058418959379196, 0.014957459643483162, 0.005825310945510864, 0.01309237815439701, 0.0009164357907138765, 0.011454297229647636, -0.015656864270567894, 0.0016718243714421988, 0.004702581092715263, 0.03563286364078522, -0.021338000893592834, -0.001115060760639608, -0.011804000474512577, 0.020319567993283272, 0.009141351096332073, 0.06699567288160324, 0.0345776230096817, -0.030798379331827164, 0.021804271265864372, 0.03369416296482086, 0.006257838103920221, 0.015178324654698372, 0.07337621599435806, 0.00883459486067295, -0.027804432436823845, 0.0006020101136527956, -0.029767675325274467, 0.011515649035573006, -0.02179200015962124, -0.030700216069817543, -0.009693513624370098, -0.021620215848088264, 0.04368216171860695, 0.028810594230890274, 0.0014762668870389462, -0.003066033823415637, -0.030356649309396744, -0.034307677298784256, -0.06027156859636307, 0.048148542642593384, 0.012822432443499565, 0.0038804730866104364, 0.01867535151541233, 0.02245459519326687, -0.018098648637533188, -0.015779567882418633, -0.01741151325404644, -0.02620929665863514, -0.01185921672731638, -0.005886662285774946, 0.033669620752334595, -0.015092432498931885, 0.04454107955098152, 0.003270026994869113, -0.02993945963680744, 0.0035890541039407253, -0.016712108626961708, 0.02138708159327507, -0.0027991554234176874, -0.005107500124722719, 0.030921081081032753, -0.021092595532536507, 0.004027716349810362, 0.011754918843507767, -0.018540378659963608, -0.009595351293683052, -0.013644540682435036, -0.02393929660320282, 0.013362324796617031, 0.005343702621757984, -0.016331730410456657, -0.016074053943157196, 0.03460216149687767, 0.020675405859947205, 0.00526701333001256, 0.025473080575466156, -0.04854118824005127, -0.0005824543768540025, 0.0015920675359666348, -0.013828594237565994, -0.0454736202955246, 0.009276323951780796, -0.004515459295362234, -0.008852999657392502, -0.00966897327452898, -0.038307782262563705, -0.0005107499891892076, -0.007196513470262289, 0.01763237826526165, -0.03482302650809288, 0.004285391885787249, 0.057228539139032364, 0.06125319004058838, -0.003592121647670865, 0.004859026987105608, 0.039412107318639755, -0.052222270518541336, 0.015497351065278053, 0.04525275528430939, -0.018552647903561592, 0.008460351265966892, 0.045939892530441284, 0.012153702788054943, 0.015006540343165398, -0.022663189098238945, -0.015570973046123981, -0.016675297170877457, 0.013411405496299267, -0.005417324136942625, -0.011804000474512577, 0.02044226974248886, 0.01370589155703783, -0.035976432263851166, 0.008902081288397312, -0.0023835001047700644, -0.004889702890068293, 0.0035553108900785446, 0.032982487231492996, 0.01624583825469017, 0.007497135084122419, -0.01415989175438881, 0.011147540993988514, 0.01397583819925785, 0.006981784012168646, -0.03614821657538414, 0.01946064829826355, -0.023288972675800323, 0.013521838001906872, -0.019448379054665565, 0.002156500006094575, -0.01640535145998001, 0.05163329839706421, -0.0198410265147686, 0.021865621209144592, 0.014000378549098969, 0.02939956821501255, 0.03558378294110298, -0.03425859659910202, -0.0477558933198452, 0.022245999425649643, 0.004635094664990902, -0.018724432215094566, -0.04530183970928192, 0.024295134469866753, -0.0007657415699213743, -0.006073783617466688, 0.007233324460685253, 0.015730487182736397, -0.014650702476501465, -0.006785459350794554, -0.01083464827388525, 0.027559027075767517, -0.02337486483156681, 0.019828757271170616, 0.030233945697546005, -0.001397276995703578, -0.010540162213146687, -0.013521838001906872, 0.029325945302844048, 0.011122999712824821, 0.010816243477165699, -0.009730324149131775, -0.020135514438152313, -0.0035982567351311445, -0.01665075682103634, 0.02375524304807186, -0.0020061891991645098, 0.0155955133959651, 0.020172324031591415, 0.018442215397953987, -0.012846972793340683, 0.01315372996032238, -0.00796340499073267, -0.013337783515453339, 0.002501601353287697, -0.031902704387903214, 0.0038712702225893736, -0.018270432949066162, 0.017644649371504784, 0.01165675651282072, 0.0278780534863472, -0.046602487564086914, -0.006779324263334274, 0.021031243726611137, -0.012300945818424225, -0.029375027865171432, 0.026381080970168114, -0.00465963501483202, 0.009987999685108662, -0.00018903885211329907, -0.031362809240818024, -0.008079973049461842, -0.0010744155151769519, 0.0030000810511410236, 0.03440583869814873, 0.01481021661311388, -0.006067648530006409, -0.033669620752334595, -0.009221107698976994, 0.004435702692717314, 0.03607459366321564, -0.011172081343829632, 0.004883567802608013, 0.037988755851984024, -0.022098757326602936, 0.006288513541221619, 0.0009877567645162344, -0.026798270642757416, -0.0066811623983085155, -0.003852864960208535, -0.022503675892949104, 0.03951026871800423, -0.010147513821721077, 0.04009924456477165, -0.030332108959555626, 0.003564513521268964, -0.014859297312796116, 0.018319513648748398, 0.011724242940545082, -0.0209085401147604, -0.030577514320611954, -0.03470032289624214, -0.02504362165927887, -0.053891025483608246, 0.01110459491610527, 0.004512391984462738, 0.036418162286281586, -0.005589107982814312, 0.025154054164886475, -0.019485188648104668, 0.015080162324011326, -0.045031893998384476, 0.0016135405749082565, 0.042945947498083115, 0.013558648526668549, 0.028491567820310593, -0.007306945975869894, -0.018810324370861053, -0.018564919009804726, -0.006012432277202606, 0.008472621440887451, -0.016258107498288155, 0.03418497368693352, 0.007276270538568497, -0.008472621440887451, -0.005168851464986801, 0.03163275867700577, 0.011012567207217216, -0.045596323907375336, -0.005383580923080444, -0.00012979644816368818, -0.035436540842056274, -0.01869989186525345, 0.011098459362983704, 0.037522487342357635, 0.005273148883134127, 0.005613648798316717, -0.014908378012478352, 0.0049633244052529335, -0.013558648526668549, -0.02040546014904976, -0.0073744324035942554, 0.016515783965587616, -0.0323689728975296, 0.020810378715395927, -0.0070922160521149635, 0.016196757555007935, -0.014429837465286255, -0.017215188592672348, -0.0015767297009006143, -0.004389689303934574, 0.04998908191919327, 0.030209405347704887, -0.0018681486835703254, 0.01598816178739071, 0.016491243615746498, 0.00014666806964669377, 0.004107472952455282, -0.04078637808561325, -0.006337594706565142, -0.029129622504115105, -0.013828594237565994, 0.024773675948381424, 0.0012001858558505774, 0.014282594434916973, -0.0014908378943800926, -0.013472756370902061, 0.02362027019262314, 0.004622824490070343, 0.00923951342701912, 0.024466918781399727, 0.002639641985297203, 0.07833340764045715, -0.006288513541221619, -0.048688434064388275, -0.004242445807904005, -0.005224067717790604, 0.049915459007024765, -0.015730487182736397, -0.0007009392138570547, -0.050234485417604446, 0.01315372996032238, 0.027117297053337097, 0.013166000135242939, 0.03263891860842705, 0.0042823245748877525, 0.0072517297230660915, 0.00592960836365819, -0.01185921672731638, -0.027264541015028954, -0.004760865122079849, -0.0030522297602146864, 0.008693486452102661, 0.012834702618420124, -0.01933794654905796, -0.03217265009880066, 0.01807410828769207, 0.0033405811991542578, -0.009816216304898262, 0.04022194445133209, 0.01781643182039261, 0.03212356939911842, 0.01646670326590538, -0.027706271037459373, 0.025792108848690987, 0.0169084332883358, -0.029448648914694786, 0.030454810708761215, -0.02068767510354519, -0.0216692965477705, 0.0009049324435181916, 0.021963784471154213, 0.012037135660648346, 0.03018486499786377, 0.021583406254649162, 0.03585372865200043, 0.003990905359387398, 0.002216317690908909, 0.0050246757455170155, -0.007619837764650583, 0.01986556686460972, 0.01438075676560402, -0.012399108149111271, 0.009294729679822922, 0.022160109132528305, -0.005193391814827919, -0.007356027141213417, 0.0023037432692945004, -0.053891025483608246, 0.01233162172138691, 0.030577514320611954, 0.0005119003471918404, -0.012024864554405212, 0.01917843334376812, -0.042160648852586746, 0.056541405618190765, -0.013264162465929985, -0.0278780534863472, -0.04324043169617653, -0.003073702799156308, 0.005714878439903259, 0.07558486610651016, -0.02076129801571369, -0.033007025718688965, -0.0009762533591128886, -0.02406200021505356, 0.024626431986689568, 0.04024648666381836, 0.01135613489896059, -0.028246162459254265, -0.02116621658205986, -0.020245946943759918, -0.013791783712804317, 0.015239675529301167, -0.01939929649233818, 0.012613837607204914, 0.026454702019691467, 0.020012810826301575, -0.01353410817682743, 0.002720932476222515, -0.00025652535259723663, -0.012380702421069145, 0.0005425760173238814, 0.04552270472049713, 0.04353491961956024, 0.026332000270485878, 0.025055892765522003, 0.008681216277182102, -0.03182908147573471, 0.017485136166214943, 0.030700216069817543, 0.003693351289257407, 0.008828459307551384, -0.025178594514727592, -0.053007569164037704, -0.00998186506330967, -0.05457816272974014, 0.016478972509503365, -0.0337432436645031, -0.012264135293662548, 0.004641229752451181, -0.0015360844554379582, 0.010877594351768494, 0.01370589155703783, 0.04586626961827278, -0.010165918618440628, 0.04819762334227562, 0.012527946382761002, -0.02419697307050228, 0.02466324344277382, -0.02365708164870739, -0.0404428094625473, 0.006515513639897108, -0.014957459643483162, 0.023902487009763718, -0.02110486477613449, -0.02598843351006508, -0.007165838032960892, 0.007018594536930323, 0.017386972904205322, 0.012392972595989704, 0.023951567709445953, -0.04434475675225258, 0.005245540756732225, 0.0036994866095483303, -0.0019172297324985266, 0.039068542420864105, 0.056590486317873, -0.0033068377524614334, -0.017276540398597717, -0.009337675757706165, -0.004561473149806261, 0.0278780534863472, -0.008190405555069447, 0.01209848653525114, -0.014687513932585716, -0.013460486195981503, -0.0018528108485043049, 0.007288540713489056, -0.01955881156027317, -0.02643016166985035, 0.0006430388311855495, -0.019644703716039658, 0.03605005517601967, -0.018160000443458557, 0.004481716081500053, 0.003362054005265236, 0.0019387027714401484, -0.003518500132486224, -0.0008343783556483686, 0.0001699624117463827, -0.017104756087064743, 0.019767405465245247, 0.0028727769386023283, -0.01536237820982933, -0.013264162465929985, 0.0550689734518528, -0.02283497340977192, -0.018503567203879356, 0.007846837863326073, 0.0058038379065692425, -0.03639362007379532, -0.024405566975474358, -0.003518500132486224, 0.01420897338539362, 0.025669405236840248, -0.005865189246833324, -0.04940010979771614, -0.025571243837475777, -0.007564621511846781, 0.012313215993344784, 0.015178324654698372, -0.0013735033571720123, -0.005604445934295654, 0.0089879734441638, -0.015276486985385418, -0.017104756087064743, -0.0036166622303426266, 0.022969946265220642, -0.00657686498016119, -0.02135027013719082, -0.01097575668245554, -0.021657027304172516, -0.008294702507555485, 0.0007676588138565421, 0.010294756852090359, -0.01744832471013069, 0.016221297904849052, 0.0367126502096653, -0.007957270368933678, -0.00045016553485766053, 0.021534323692321777, 0.022822702303528786, 0.007828432135283947, -0.017386972904205322, 0.033669620752334595, -0.0032025405671447515, 0.0198042169213295, 0.04763318970799446, 0.0037209594156593084, 0.005724081303924322, 0.008527837693691254, -0.014343946240842342, -0.020295027643442154, -0.01132545992732048, 0.03573102876543999, 0.005540027283132076, 0.02303129807114601, 0.016503512859344482, -0.022700000554323196, 0.004604418762028217, -0.01895756833255291, -0.021055784076452255, -0.0008650540839880705, 0.000758839538320899, 0.02381659485399723, -0.03882313519716263, 0.006687297485768795, 0.02765718847513199, -0.017607837915420532, 0.03423405438661575, 0.050234485417604446, -0.0013397601433098316, 0.00952786486595869, 0.006944973021745682, -0.008190405555069447, 0.0028267635498195887, 0.014650702476501465, -0.02863881178200245, 0.02375524304807186, 0.0747995674610138, -0.004570675548166037, 0.006000162102282047, 0.02353437803685665, 0.0032915000338107347, 0.03578010946512222, -0.008368324488401413, 0.02454053983092308, 0.04434475675225258, -0.00017974029469769448, -0.0220496766269207, -0.0039602299220860004, 0.015693675726652145, 0.04309318959712982, -0.009190432727336884, -0.01596362143754959, 0.02220918983221054, 0.005018540658056736, -0.026552865281701088, 0.012442054226994514, -0.022196918725967407, 0.003521567676216364, -0.0012814763467758894, -0.00028068243409506977, 0.04125265032052994, -0.02706821635365486, 0.004220972768962383, 0.023227620869874954, -0.031804539263248444, 0.010877594351768494, 0.005125905387103558, -0.01927659474313259, -0.006141270510852337, 0.0008543175645172596, -0.0028405676130205393, -0.020147783681750298, -0.05614875629544258, -0.0023022093810141087, 0.004334473051130772, 0.034749407321214676, 0.014037189073860645, -0.023006757721304893, -0.031460974365472794, -0.023828864097595215, -0.006466432474553585, -0.0180127564817667, 0.01857719011604786, -0.020638594403862953, 0.0044080945663154125, 0.04054097458720207, 0.018663080409169197, 0.007546216249465942, 0.015239675529301167, 0.003389662131667137, 0.050357189029455185, 0.009724189527332783, 0.006975648459047079, -0.006619811058044434, -0.00923951342701912, 0.0009233378223143518, -0.009208837524056435, -0.02154659479856491, 0.0005513952928595245, -0.018871676176786423, 0.023264432325959206, 0.0036013242788612843, 0.008632135577499866, 0.03847956657409668, 0.016859350726008415, 0.04343675822019577, -0.03516659513115883, 0.005070689134299755, 0.046038053929805756, 0.0179759468883276, -0.02741178311407566, 0.012325486168265343, 0.019325675442814827, 0.010110702365636826, -0.006668891757726669, -0.017828702926635742, 0.002742405515164137, 0.01646670326590538, 0.020454540848731995, -0.040271028876304626, 0.014638432301580906, 0.010386783629655838, 0.032246269285678864, -0.04235697165131569, 0.0004747060884255916, 0.003604391822591424, 0.009233378805220127, 0.019693784415721893, 0.053007569164037704, 0.0032853649463504553, 0.005886662285774946, 0.008865270763635635, 0.016540324315428734, 0.026798270642757416, -0.014012648724019527, 0.00884686503559351, 0.020049622282385826, -0.0027393377386033535, -0.039068542420864105, -0.0019387027714401484, -0.0155955133959651, -0.016712108626961708, 0.018344053998589516, 0.006490972824394703, -0.030086703598499298, 0.0023390203714370728, -0.012454324401915073, 0.031878162175416946, -0.02854064851999283, 0.003917283844202757, -0.039191242307424545, -0.01102483831346035, 0.021080324426293373, 0.009534000419080257, 0.019202973693609238, 0.0026687837671488523, -0.03425859659910202, -0.019632432609796524, -0.051240649074316025, 0.0004467145190574229, 0.04144897311925888, 0.0005046148435212672, -0.0114052165299654, -0.005426527000963688, -0.032000865787267685, -0.03771881014108658, -0.022405514493584633, -0.010472675785422325, -0.033546920865774155, 0.02438102662563324, 0.05717945843935013, -0.0017209054203704, 0.01096348650753498, -0.001789925736375153, -0.021411621943116188, 0.04385394603013992, 0.03700713440775871, -0.003518500132486224, 0.004776202607899904, -0.023068107664585114, 0.04620983824133873, -0.008742568083107471, 0.025399459525942802, 0.0009777871891856194, 0.0060891215689480305, -0.017325621098279953, 0.0024724595714360476]\n"
     ]
    }
   ],
   "source": [
    "print(batch_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4452d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare Embedding using cosine similarity\n",
    "\n",
    "def compare_embeddings(text1:str,text2:str):\n",
    "    \"\"\"Compare semantic simialrity of 2 texts usign embeddings\"\"\"\n",
    "\n",
    "    emb1=np.array(embeddings.embed_query(text1))\n",
    "    emb2=np.array(embeddings.embed_query(text2))\n",
    "\n",
    "    ## Calculate the simialrity score\n",
    "\n",
    "    similarity=np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19d2e7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Similarity Examples:\n",
      "'AI' vs 'Artificial Intelligence': 0.563\n"
     ]
    }
   ],
   "source": [
    "# Test semantic similarity\n",
    "print(\"\\nSemantic Similarity Examples:\")\n",
    "print(f\"'AI' vs 'Artificial Intelligence': {compare_embeddings('AI', 'Artificial Intelligence'):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4ea6092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'AI' vs 'Pizza': 0.254\n"
     ]
    }
   ],
   "source": [
    "print(f\"'AI' vs 'Pizza': {compare_embeddings('AI', 'Pizza'):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50296f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Machine Learning' vs 'ML': 0.461\n"
     ]
    }
   ],
   "source": [
    "print(f\"'Machine Learning' vs 'ML': {compare_embeddings('Machine Learning', 'ML'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9735ac2",
   "metadata": {},
   "source": [
    "### Create FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e0989a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 4 vectors\n"
     ]
    }
   ],
   "source": [
    "vectorstore=FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "print(f\"Vector store created with {vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32d15527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1b1fa6e0d60>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0d43809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store saved to 'faiss_index' directory\n"
     ]
    }
   ],
   "source": [
    "## Save vector tore for later use\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "print(\"Vector store saved to 'faiss_index' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4dd954a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store contains 4 vectors\n"
     ]
    }
   ],
   "source": [
    "## load vector store\n",
    "loaded_vectorstore=FAISS.load_local(\n",
    "    \"faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded vector store contains {loaded_vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca90941a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='8f8852bf-c2e1-49f5-91dd-8e2418c7d6e4', metadata={'source': 'Deep Learning', 'page': 1, 'topic': 'DL'}, page_content='Deep Learning is a subset of machine learning based on artificial neural networks.\\n        It uses multiple layers to progressively extract higher-level features from raw input.\\n        Deep learning has revolutionized computer vision, NLP, and speech recognition.'), Document(id='c0566de3-4f5b-42a1-be7f-2f6cecf6d6eb', metadata={'source': 'ML Basics', 'page': 1, 'topic': 'ML'}, page_content='Machine Learning is a subset of AI that enables systems to learn from data.\\n        Instead of being explicitly programmed, ML algorithms find patterns in data.\\n        Common types include supervised, unsupervised, and reinforcement learning.'), Document(id='2062f147-d8c2-4fa2-a03b-8647d8b73f75', metadata={'source': 'NLP Overview', 'page': 1, 'topic': 'NLP'}, page_content='Natural Language Processing (NLP) is a branch of AI that helps computers understand human language.\\n        It combines computational linguistics with machine learning and deep learning models.\\n        Applications include chatbots, translation, sentiment analysis, and text summarization.')]\n"
     ]
    }
   ],
   "source": [
    "## Similarity Search \n",
    "query=\"What is deep learning\"\n",
    "\n",
    "results=vectorstore.similarity_search(query,k=3)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b21fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is deep learning\n",
      "\n",
      "Top 3 similar chunks:\n",
      "\n",
      "1. Source: Deep Learning\n",
      "   Content: Deep Learning is a subset of machine learning based on artificial neural networks.\n",
      "        It uses multiple layers to progressively extract higher-level features from raw input.\n",
      "        Deep learning ...\n",
      "\n",
      "2. Source: ML Basics\n",
      "   Content: Machine Learning is a subset of AI that enables systems to learn from data.\n",
      "        Instead of being explicitly programmed, ML algorithms find patterns in data.\n",
      "        Common types include supervised...\n",
      "\n",
      "3. Source: NLP Overview\n",
      "   Content: Natural Language Processing (NLP) is a branch of AI that helps computers understand human language.\n",
      "        It combines computational linguistics with machine learning and deep learning models.\n",
      "      ...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 3 similar chunks:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n{i+1}. Source: {doc.metadata['source']}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0234314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Similarity search with scores:\n",
      "\n",
      "Score: 0.556\n",
      "Source: Deep Learning\n",
      "Content preview: Deep Learning is a subset of machine learning based on artificial neural networks.\n",
      "        It uses m...\n",
      "\n",
      "Score: 1.208\n",
      "Source: ML Basics\n",
      "Content preview: Machine Learning is a subset of AI that enables systems to learn from data.\n",
      "        Instead of being...\n",
      "\n",
      "Score: 1.274\n",
      "Source: NLP Overview\n",
      "Content preview: Natural Language Processing (NLP) is a branch of AI that helps computers understand human language.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "### Similarity Search with score\n",
    "results_with_scores=vectorstore.similarity_search_with_score(query,k=3)\n",
    "\n",
    "print(\"\\n\\nSimilarity search with scores:\")\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"\\nScore: {score:.3f}\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Content preview: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23a80f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AI Introduction', 'page': 1, 'topic': 'AI'}, page_content='Artificial Intelligence (AI) is the simulation of human intelligence in machines.\\n        These systems are designed to think like humans and mimic their actions.\\n        AI can be categorized into narrow AI and general AI.'),\n",
       " Document(metadata={'source': 'ML Basics', 'page': 1, 'topic': 'ML'}, page_content='Machine Learning is a subset of AI that enables systems to learn from data.\\n        Instead of being explicitly programmed, ML algorithms find patterns in data.\\n        Common types include supervised, unsupervised, and reinforcement learning.'),\n",
       " Document(metadata={'source': 'Deep Learning', 'page': 1, 'topic': 'DL'}, page_content='Deep Learning is a subset of machine learning based on artificial neural networks.\\n        It uses multiple layers to progressively extract higher-level features from raw input.\\n        Deep learning has revolutionized computer vision, NLP, and speech recognition.'),\n",
       " Document(metadata={'source': 'NLP Overview', 'page': 1, 'topic': 'NLP'}, page_content='Natural Language Processing (NLP) is a branch of AI that helps computers understand human language.\\n        It combines computational linguistics with machine learning and deep learning models.\\n        Applications include chatbots, translation, sentiment analysis, and text summarization.')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "46e0d681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='c0566de3-4f5b-42a1-be7f-2f6cecf6d6eb', metadata={'source': 'ML Basics', 'page': 1, 'topic': 'ML'}, page_content='Machine Learning is a subset of AI that enables systems to learn from data.\\n        Instead of being explicitly programmed, ML algorithms find patterns in data.\\n        Common types include supervised, unsupervised, and reinforcement learning.')]\n"
     ]
    }
   ],
   "source": [
    "### Search with metadata filtering\n",
    "filter_dict={\"topic\":\"ML\"}\n",
    "filtered_results=vectorstore.similarity_search(\n",
    "    query,\n",
    "    k=3,\n",
    "    filter=filter_dict\n",
    ")\n",
    "print(filtered_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0f153009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3214a1d",
   "metadata": {},
   "source": [
    "## Build RAG Chain With LCEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d6de908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001B1F90CB360>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B1F90C8FC0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LLM GROQ LLM\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:gemma2-9b-it\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c28e9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! 👋\\n\\nHow can I help you today? 😊\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 10, 'total_tokens': 25, 'completion_time': 0.027272727, 'prompt_time': 0.00117063, 'queue_time': 0.253079486, 'total_time': 0.028443357}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--208de7c5-1ae3-45a8-a674-9e8c14783105-0', usage_metadata={'input_tokens': 10, 'output_tokens': 15, 'total_tokens': 25})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "26844d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple RAG Chain with LCEL\n",
    "simple_prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context:\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0f83433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic retriever\n",
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d35a9fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B1FA6E0D60>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# Format documents for the prompt\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents for insertion into prompt\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        formatted.append(f\"Document {i+1} (Source: {source}):\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c6f65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_chain=(\n",
    "    {\"context\":retriever | format_docs,\"question\":RunnablePassthrough() }\n",
    "    | simple_prompt\n",
    "    | llm\n",
    "    |StrOutputParser()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c470a6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001B1FA6E0D60>, search_kwargs={'k': 3})\n",
       "           | RunnableLambda(format_docs),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001B1F90CB360>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B1F90C8FC0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "18098855",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conversational RAg Chain\n",
    "\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the provided context to answer questions.\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c42689e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_rag():\n",
    "    \"\"\"Create a conversational RAG chain with memory\"\"\"\n",
    "    return (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=lambda x: format_docs(retriever.invoke(x[\"input\"]))\n",
    "        )\n",
    "        | conversational_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "conversational_rag = create_conversational_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f255f6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  context: RunnableLambda(lambda x: format_docs(retriever.invoke(x['input'])))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001B1B03A3240>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI assistant. Use the provided context to answer questions.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='Context: {context}\\n\\nQuestion: {input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001B1F90CB360>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001B1F90C8FC0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c88574dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modern RAG chains created successfully!\n",
      "Available chains:\n",
      "- simple_rag_chain: Basic Q&A\n",
      "- conversational_rag: Maintains conversation history\n",
      "- streaming_rag_chain: Supports token streaming\n"
     ]
    }
   ],
   "source": [
    "### streaming RAG chain\n",
    "streaming_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | simple_prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "print(\"Modern RAG chains created successfully!\")\n",
    "print(\"Available chains:\")\n",
    "print(\"- simple_rag_chain: Basic Q&A\")\n",
    "print(\"- conversational_rag: Maintains conversation history\")\n",
    "print(\"- streaming_rag_chain: Supports token streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d8a26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for different chain types\n",
    "def test_rag_chains(question: str):\n",
    "    \"\"\"Test all RAG chain variants\"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Simple RAG\n",
    "    print(\"\\n1. Simple RAG Chain:\")\n",
    "    answer = simple_rag_chain.invoke(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "    print(\"\\n2. Streaming RAG:\")\n",
    "    print(\"Answer: \", end=\"\", flush=True)\n",
    "    for chunk in streaming_rag_chain.stream(question):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b5a9cd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the difference between AI and machine learning\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: AI is the broader concept of machines performing tasks that typically require human intelligence. Machine learning is a specific subset of AI where systems learn from data instead of being explicitly programmed.  \n",
      "\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: AI is the broad concept of machines simulating human intelligence, while machine learning is a subset of AI that allows systems to learn from data without explicit programming. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_rag_chains(\"What is the difference between AI and machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5e375d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n",
      "Question: What is the difference between AI and Machine Learning?\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: AI is the broader concept of machines performing tasks that typically require human intelligence, while Machine Learning is a specific subset of AI that allows systems to learn from data without explicit programming.  \n",
      "\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: Artificial Intelligence (AI) is the broad concept of machines simulating human intelligence, while Machine Learning (ML) is a subset of AI that focuses on enabling systems to learn from data without explicit programming.  \n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: Explain deep learning in simple terms\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: Deep learning is a type of machine learning that uses artificial networks with many layers to learn from data.  Think of it like teaching a computer to recognize patterns, like pictures or words, by showing it lots of examples. Each layer in the network helps it learn more complex features, ultimately allowing it to make accurate predictions. \n",
      "\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: Deep learning is a type of machine learning that uses many layers of artificial neurons to learn complex patterns from data.  It's like teaching a computer to see and understand things the way humans do, by showing it lots of examples.  Deep learning has been especially successful in areas like computer vision (recognizing objects in images), natural language processing (understanding and generating text), and speech recognition. \n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question: How does NLP work?\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: NLP combines computational linguistics with machine learning and deep learning models.  \n",
      "\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: NLP combines computational linguistics with machine learning and deep learning models.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with multiple questions\n",
    "test_questions = [\n",
    "    \"What is the difference between AI and Machine Learning?\",\n",
    "    \"Explain deep learning in simple terms\",\n",
    "    \"How does NLP work?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    test_rag_chains(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "afcfe3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Conversational RAG Example:\n",
      "Q1: What is machine learning?\n",
      "A1: Machine learning is a subset of artificial intelligence that allows systems to learn from data instead of being explicitly programmed. \n",
      "\n",
      "ML algorithms identify patterns in data to make predictions or decisions. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Conversational example\n",
    "print(\"\\n3. Conversational RAG Example:\")\n",
    "chat_history = []\n",
    "\n",
    "# First question\n",
    "q1 = \"What is machine learning?\"\n",
    "a1 = conversational_rag.invoke({\n",
    "    \"input\": q1,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(f\"Q1: {q1}\")\n",
    "print(f\"A1: {a1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b3a72dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update history\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=q1),\n",
    "    AIMessage(content=a1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4b9eda39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q2: How is it different from traditional programming?\n",
      "A2: Here's how machine learning differs from traditional programming, drawing from the provided context:\n",
      "\n",
      "**Traditional Programming:**\n",
      "\n",
      "* **Explicit Instructions:**  Programmers write very specific instructions (code) that the computer follows step-by-step.  \n",
      "* **Rule-Based:**  Programs rely on predefined rules and logic to process information.\n",
      "\n",
      "**Machine Learning:**\n",
      "\n",
      "* **Learning from Data:** Instead of explicit rules, ML algorithms learn patterns and relationships from large datasets.\n",
      "* **Data-Driven:** The \"program\" is not fixed code but a set of algorithms that adapt and improve based on the data they are trained on.\n",
      "\n",
      "**In essence:**\n",
      "\n",
      "Traditional programming is like giving the computer a detailed recipe to follow. Machine learning is more like giving the computer a bunch of ingredients and letting it figure out how to make something delicious based on patterns it observes. \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "q2 = \"How is it different from traditional programming?\"\n",
    "a2 = conversational_rag.invoke({\n",
    "    \"input\": q2,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "print(f\"\\nQ2: {q2}\")\n",
    "print(f\"A2: {a2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c589ffda",
   "metadata": {},
   "source": [
    "# **Notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62509d57",
   "metadata": {},
   "source": [
    "## **Introduction and Fundamentals**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c99120",
   "metadata": {},
   "source": [
    "**Introduction to FAISS**\n",
    "\n",
    "> FAISS (Facebook AI Similarity Search) is a **high-performance library for similarity search and clustering of dense vectors**. It is widely used in machine learning and AI pipelines to **search, retrieve, and index large-scale vector representations**, such as embeddings generated by neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "**What is FAISS?**\n",
    "\n",
    "* FAISS provides **efficient indexing and retrieval** of high-dimensional vectors.\n",
    "* It supports **both exact and approximate nearest neighbor (ANN) search**.\n",
    "* Designed for **large-scale datasets**, it is optimized for **speed and memory usage** and can leverage **GPU acceleration** for massive vector collections.\n",
    "\n",
    "---\n",
    "\n",
    "**History and Evolution**\n",
    "\n",
    "* Developed by **Facebook AI Research (FAIR)** to handle large-scale embedding search problems.\n",
    "* Initially focused on CPU-based indexing and similarity search.\n",
    "* Evolved to include **GPU support, advanced indexing structures**, and distributed capabilities.\n",
    "* Became widely adopted in AI applications such as **RAG (Retrieval-Augmented Generation), recommendation systems, and image retrieval**.\n",
    "\n",
    "---\n",
    "\n",
    "**Applications and Use Cases**\n",
    "\n",
    "* **Semantic Search:** Find documents, images, or audio similar to a query embedding.\n",
    "* **Recommendation Systems:** Suggest products, content, or media based on user embeddings.\n",
    "* **RAG Pipelines:** Retrieve relevant context for language models using vector similarity.\n",
    "* **Clustering and Anomaly Detection:** Identify patterns or outliers in high-dimensional data.\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison with Other Vector Search Libraries**\n",
    "\n",
    "| Library      | Strengths                                                 | Limitations                                                 | Use Case                                |\n",
    "| ------------ | --------------------------------------------------------- | ----------------------------------------------------------- | --------------------------------------- |\n",
    "| **FAISS**    | High-performance, GPU support, exact & approximate search | Complex indexing for very large clusters may require tuning | Large-scale RAG, embedding search       |\n",
    "| **Pinecone** | Managed service, scalable, simple API                     | Cloud-dependent, higher cost for large datasets             | SaaS applications, multi-user scenarios |\n",
    "| **Milvus**   | Open-source, distributed, supports hybrid queries         | Requires separate deployment & management                   | Enterprise-grade vector DB              |\n",
    "| **Weaviate** | Schema-aware, vector + semantic search                    | Slightly slower for large-scale GPU computation             | Knowledge graphs, semantic search       |\n",
    "\n",
    "---\n",
    "\n",
    "**Core Concepts**\n",
    "\n",
    "**Vectors and Embeddings**\n",
    "\n",
    "* Vectors are **numerical representations** of data, often derived from LLMs, CNNs, or other neural networks.\n",
    "* Embeddings encode **semantic information** allowing similarity comparisons.\n",
    "\n",
    "**Distance Metrics**\n",
    "FAISS supports multiple metrics to measure similarity:\n",
    "\n",
    "| Metric                          | Description                                     | Use Case                             |\n",
    "| ------------------------------- | ----------------------------------------------- | ------------------------------------ |\n",
    "| **L2 (Euclidean Distance)**     | Measures straight-line distance between vectors | Most common for dense embeddings     |\n",
    "| **Cosine Similarity**           | Measures angle between vectors, normalized      | Text embeddings, semantic similarity |\n",
    "| **Inner Product (Dot Product)** | Measures projection similarity                  | Ranking, recommendation tasks        |\n",
    "\n",
    "**Indexes and Search Types**\n",
    "\n",
    "* **Flat Index:** Exact search, stores all vectors, high accuracy but slower for large datasets.\n",
    "* **IVF (Inverted File Index):** Approximate search, partitions dataset to reduce search space.\n",
    "* **HNSW (Hierarchical Navigable Small World):** Graph-based ANN, efficient and accurate for high-dimensional data.\n",
    "* **PQ (Product Quantization):** Compresses vectors to save memory while enabling fast search.\n",
    "\n",
    "**Approximate vs. Exact Search**\n",
    "\n",
    "* **Exact Search:** Guarantees correct nearest neighbors but slower for large datasets.\n",
    "* **Approximate Search:** Sacrifices some accuracy for faster search, ideal for large-scale applications where speed is critical.\n",
    "\n",
    "---\n",
    "\n",
    "**Setting Up the Environment**\n",
    "\n",
    "**Installing FAISS (CPU vs. GPU)**\n",
    "\n",
    "* **CPU Installation:**\n",
    "\n",
    "```bash\n",
    "pip install faiss-cpu\n",
    "```\n",
    "\n",
    "* **GPU Installation:**\n",
    "\n",
    "```bash\n",
    "pip install faiss-gpu\n",
    "```\n",
    "\n",
    "**Dependencies and Requirements**\n",
    "\n",
    "* **Python >=3.8**\n",
    "* **NumPy**\n",
    "* Optional: **PyTorch or CuPy** for GPU acceleration\n",
    "\n",
    "**Configuring GPU Support**\n",
    "\n",
    "* Verify GPU availability using:\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "print(faiss.get_num_gpus())\n",
    "```\n",
    "\n",
    "* FAISS automatically uses GPU if `faiss-gpu` is installed.\n",
    "* Large indexes may require **manual GPU assignment**:\n",
    "\n",
    "```python\n",
    "res = faiss.StandardGpuResources()\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e396f9a",
   "metadata": {},
   "source": [
    "## **Creating Indexes in FAISS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1895233",
   "metadata": {},
   "source": [
    "> FAISS provides a variety of **index types** optimized for different use cases, balancing **speed, memory efficiency, and accuracy**. Selecting the appropriate index depends on **dataset size, dimensionality, and performance requirements**.\n",
    "\n",
    "---\n",
    "\n",
    "**Flat (Brute-Force) Index**\n",
    "\n",
    "* **Description:**\n",
    "\n",
    "  * The simplest index in FAISS.\n",
    "  * Stores all vectors in memory and performs **exact nearest neighbor search** by comparing the query vector against all stored vectors.\n",
    "* **Characteristics:**\n",
    "\n",
    "  * High accuracy (guaranteed exact results).\n",
    "  * Linear search complexity (`O(n)`), slower for very large datasets.\n",
    "  * Low preprocessing overhead, easy to implement.\n",
    "* **Use Cases:**\n",
    "\n",
    "  * Small to medium datasets.\n",
    "  * Scenarios where **accuracy is critical** and search speed is less important.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "d = 128  # Dimension\n",
    "nb = 10000  # Number of vectors\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(d)  # L2 distance\n",
    "index.add(xb)\n",
    "query = np.random.random((1, d)).astype('float32')\n",
    "D, I = index.search(query, k=5)  # Top 5 nearest neighbors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**IndexIVFFlat (Inverted File Index)**\n",
    "\n",
    "* **Description:**\n",
    "\n",
    "  * Partitions the vector dataset into **clusters (inverted lists)** for faster search.\n",
    "  * Performs **approximate nearest neighbor (ANN) search** by searching only a subset of clusters.\n",
    "* **Characteristics:**\n",
    "\n",
    "  * Faster than Flat index for large datasets.\n",
    "  * Requires **training** on sample data to generate cluster centroids.\n",
    "  * Trade-off between **accuracy and speed** depending on the number of clusters and probes.\n",
    "* **Use Cases:**\n",
    "\n",
    "  * Large-scale datasets with millions of vectors.\n",
    "  * Applications where **slightly approximate results are acceptable** for speed gains.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "nlist = 100  # Number of clusters\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index_ivf = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
    "index_ivf.train(xb)  # Train on the dataset\n",
    "index_ivf.add(xb)\n",
    "index_ivf.nprobe = 10  # Number of clusters to search\n",
    "D, I = index_ivf.search(query, k=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**IndexHNSW (Hierarchical Navigable Small World)**\n",
    "\n",
    "* **Description:**\n",
    "\n",
    "  * Graph-based approximate nearest neighbor index.\n",
    "  * Connects vectors in a **small-world graph structure** to navigate efficiently.\n",
    "* **Characteristics:**\n",
    "\n",
    "  * High search speed and accuracy for **high-dimensional data**.\n",
    "  * No training required.\n",
    "  * Supports **dynamic insertion of new vectors**.\n",
    "* **Use Cases:**\n",
    "\n",
    "  * Real-time applications needing **fast queries with high recall**.\n",
    "  * Scenarios with **frequent updates** to the dataset.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "M = 32  # Number of neighbors in the graph\n",
    "index_hnsw = faiss.IndexHNSWFlat(d, M)\n",
    "index_hnsw.add(xb)\n",
    "index_hnsw.hnsw.efSearch = 50  # Trade-off between speed and accuracy\n",
    "D, I = index_hnsw.search(query, k=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**IndexPQ (Product Quantization)**\n",
    "\n",
    "* **Description:**\n",
    "\n",
    "  * Compresses vectors using **sub-vector quantization**, reducing memory footprint.\n",
    "  * Performs **approximate search** on compressed representations.\n",
    "* **Characteristics:**\n",
    "\n",
    "  * Reduces storage requirements for large datasets.\n",
    "  * May slightly reduce accuracy compared to exact search.\n",
    "  * Often combined with other indexes like IVF for **scalable ANN search**.\n",
    "* **Use Cases:**\n",
    "\n",
    "  * Extremely large-scale datasets where memory efficiency is critical.\n",
    "  * Cloud or GPU deployments with limited memory.\n",
    "* **Example:**\n",
    "\n",
    "```python\n",
    "m = 8  # Number of sub-vectors\n",
    "nbits = 8  # Bits per sub-vector\n",
    "index_pq = faiss.IndexPQ(d, m, nbits)\n",
    "index_pq.train(xb)\n",
    "index_pq.add(xb)\n",
    "D, I = index_pq.search(query, k=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Choosing the Right Index**\n",
    "\n",
    "| Index Type  | Accuracy    | Speed                   | Memory | Training Required | Best Use Case                             |\n",
    "| ----------- | ----------- | ----------------------- | ------ | ----------------- | ----------------------------------------- |\n",
    "| **Flat**    | Exact       | Slow for large datasets | High   | No                | Small datasets, high precision needs      |\n",
    "| **IVFFlat** | Approximate | Fast                    | Medium | Yes               | Large datasets, balanced speed & accuracy |\n",
    "| **HNSW**    | High        | Very Fast               | Medium | No                | Real-time search, high-dimensional data   |\n",
    "| **PQ**      | Approximate | Fast                    | Low    | Yes               | Massive datasets with memory constraints  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c175f539",
   "metadata": {},
   "source": [
    "## **Adding and Searching Vectors in FAISS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f937b9",
   "metadata": {},
   "source": [
    "\n",
    "> Once an index is created in FAISS, the next critical steps are **adding vectors** to the index and **performing search queries**. These operations enable similarity search, semantic retrieval, and recommendation systems based on embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "**Adding Vectors to Indexes**\n",
    "\n",
    "* **Vectors** are added to FAISS indexes using the `.add()` method.\n",
    "* Vectors must be **NumPy arrays of type `float32`** with shape `(number_of_vectors, dimension)`.\n",
    "* Depending on the index type, vectors may require **training** before adding (e.g., IVF, PQ).\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Prepare vectors (from embeddings).\n",
    "2. Train the index if required.\n",
    "3. Add vectors to the index.\n",
    "\n",
    "**Example: Adding vectors to different indexes**\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "d = 128  # Dimension\n",
    "nb = 10000  # Number of vectors\n",
    "xb = np.random.random((nb, d)).astype('float32')\n",
    "\n",
    "# Flat index\n",
    "flat_index = faiss.IndexFlatL2(d)\n",
    "flat_index.add(xb)\n",
    "\n",
    "# IVF index\n",
    "nlist = 100\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "ivf_index = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
    "ivf_index.train(xb)\n",
    "ivf_index.add(xb)\n",
    "```\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "* For **large datasets**, consider batch adding vectors to reduce memory spikes.\n",
    "* Use **persistent storage** or save the index to disk for future use:\n",
    "\n",
    "```python\n",
    "faiss.write_index(flat_index, \"flat.index\")\n",
    "faiss.write_index(ivf_index, \"ivf.index\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Performing Search Queries**\n",
    "\n",
    "* FAISS supports **K-nearest neighbor (KNN) search** for querying vectors.\n",
    "* The `.search()` method returns two arrays:\n",
    "\n",
    "  1. **Distances (`D`)** – similarity or distance scores between the query and indexed vectors.\n",
    "  2. **Indices (`I`)** – positions of the nearest neighbors in the index.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "D, I = index.search(query_vectors, k)\n",
    "```\n",
    "\n",
    "* `query_vectors`: NumPy array of query embeddings, shape `(n_queries, d)`.\n",
    "* `k`: Number of nearest neighbors to retrieve.\n",
    "\n",
    "**Example: Searching for nearest neighbors**\n",
    "\n",
    "```python\n",
    "query = np.random.random((5, d)).astype('float32')  # 5 query vectors\n",
    "k = 5  # Retrieve top 5 neighbors\n",
    "\n",
    "# Using Flat index\n",
    "D_flat, I_flat = flat_index.search(query, k)\n",
    "\n",
    "# Using IVF index\n",
    "ivf_index.nprobe = 10  # Number of clusters to probe\n",
    "D_ivf, I_ivf = ivf_index.search(query, k)\n",
    "\n",
    "print(\"Distances:\", D_flat)\n",
    "print(\"Indices:\", I_flat)\n",
    "```\n",
    "\n",
    "**Tips for Efficient Search:**\n",
    "\n",
    "* **IVF indexes:** Adjust `nprobe` to trade off **accuracy vs. speed**. Higher `nprobe` → more accurate but slower.\n",
    "* **HNSW indexes:** Adjust `efSearch` parameter for similar trade-offs.\n",
    "* **Batch queries:** Send multiple query vectors together for **better throughput**.\n",
    "\n",
    "---\n",
    "\n",
    "**K-Nearest Neighbor (KNN) Search**\n",
    "\n",
    "* KNN search is the **core operation** for retrieving vectors similar to a query.\n",
    "* Metrics used for similarity depend on the index:\n",
    "\n",
    "  * **L2 distance**: closer vectors have smaller distances.\n",
    "  * **Cosine similarity / inner product**: larger values indicate higher similarity.\n",
    "* FAISS can handle **single query or multiple queries** simultaneously.\n",
    "\n",
    "**Use Cases of KNN in FAISS:**\n",
    "\n",
    "1. **Semantic Search:** Retrieve top relevant documents or sentences.\n",
    "2. **Recommendation Systems:** Find similar items based on embeddings.\n",
    "3. **Clustering & Outlier Detection:** Identify nearest points in high-dimensional space.\n",
    "4. **RAG Pipelines:** Retrieve context passages for language models.\n",
    "\n",
    "---\n",
    "\n",
    "**Saving and Loading Indexes**\n",
    "\n",
    "* After adding vectors, indexes can be **persisted to disk** for reuse:\n",
    "\n",
    "```python\n",
    "faiss.write_index(flat_index, \"flat.index\")\n",
    "loaded_index = faiss.read_index(\"flat.index\")\n",
    "```\n",
    "\n",
    "* For GPU indexes, you may need to transfer back to CPU before saving:\n",
    "\n",
    "```python\n",
    "cpu_index = faiss.index_gpu_to_cpu(gpu_index)\n",
    "faiss.write_index(cpu_index, \"gpu_index.index\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58febafe",
   "metadata": {},
   "source": [
    "## **Serialization and Persistence in FAISS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c6035",
   "metadata": {},
   "source": [
    "> FAISS provides **robust mechanisms for saving, loading, and managing indexes**, enabling long-term storage, sharing, and incremental updates. Proper serialization and persistence are crucial for **production systems, large-scale embeddings, and distributed workflows**.\n",
    "\n",
    "---\n",
    "\n",
    "**Saving Indexes to Disk**\n",
    "\n",
    "* FAISS indexes can be **persisted to disk** using `faiss.write_index()`.\n",
    "* This ensures that **precomputed vectors and trained indexes** do not need to be recomputed every time the application starts.\n",
    "\n",
    "**Example: Saving an index**\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "d = 128\n",
    "xb = np.random.random((10000, d)).astype('float32')\n",
    "\n",
    "# Create and add vectors to a Flat index\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(xb)\n",
    "\n",
    "# Save index to disk\n",
    "faiss.write_index(index, \"flat.index\")\n",
    "```\n",
    "\n",
    "* **Best Practices:**\n",
    "\n",
    "  * Include **metadata or versioning** to track index updates.\n",
    "  * For GPU indexes, transfer to CPU before saving:\n",
    "\n",
    "  ```python\n",
    "  cpu_index = faiss.index_gpu_to_cpu(gpu_index)\n",
    "  faiss.write_index(cpu_index, \"gpu_index.index\")\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "**Loading Indexes**\n",
    "\n",
    "* Previously saved indexes can be **reloaded into memory** for immediate use.\n",
    "* `faiss.read_index()` automatically reconstructs the index structure.\n",
    "\n",
    "**Example: Loading an index**\n",
    "\n",
    "```python\n",
    "# Load the index from disk\n",
    "loaded_index = faiss.read_index(\"flat.index\")\n",
    "\n",
    "# Perform search on the loaded index\n",
    "query = np.random.random((1, d)).astype('float32')\n",
    "D, I = loaded_index.search(query, k=5)\n",
    "print(\"Nearest neighbors indices:\", I)\n",
    "```\n",
    "\n",
    "* **GPU usage:** Loaded indexes can be transferred to GPU for faster queries:\n",
    "\n",
    "```python\n",
    "res = faiss.StandardGpuResources()\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, loaded_index)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Updating and Merging Indexes**\n",
    "\n",
    "**Adding New Vectors**\n",
    "\n",
    "* Vectors can be **added incrementally** without retraining (for most index types like Flat, HNSW).\n",
    "* For IVF or PQ indexes, if the **new data significantly differs**, retraining might improve accuracy.\n",
    "\n",
    "**Example: Adding new vectors**\n",
    "\n",
    "```python\n",
    "new_vectors = np.random.random((5000, d)).astype('float32')\n",
    "loaded_index.add(new_vectors)  # Incremental addition\n",
    "```\n",
    "\n",
    "**Merging Indexes**\n",
    "\n",
    "* FAISS allows **merging multiple indexes** into one, useful for distributed datasets or batch indexing.\n",
    "* Use `faiss.merge_into()` or `faiss.merge_indexes()` depending on the index type.\n",
    "\n",
    "**Example: Merging indexes**\n",
    "\n",
    "```python\n",
    "index1 = faiss.IndexFlatL2(d)\n",
    "index2 = faiss.IndexFlatL2(d)\n",
    "\n",
    "index1.add(np.random.random((5000, d)).astype('float32'))\n",
    "index2.add(np.random.random((5000, d)).astype('float32'))\n",
    "\n",
    "faiss.merge_into(index2, index1, shift_ids=True)  # Merge index1 into index2\n",
    "```\n",
    "\n",
    "* **Considerations:**\n",
    "\n",
    "  * For IVF or PQ indexes, indexes must have **compatible parameters** (same dimensions, sub-quantizers, etc.).\n",
    "  * After merging, it is advisable to **retrain or reoptimize** for approximate search indexes to maintain accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "**Practical Tips for Persistence**\n",
    "\n",
    "| Task            | Recommendation                                                 |\n",
    "| --------------- | -------------------------------------------------------------- |\n",
    "| Saving indexes  | Use versioned filenames and CPU indexes for compatibility      |\n",
    "| Loading indexes | Transfer to GPU only if needed for fast search                 |\n",
    "| Adding new data | Batch additions to avoid frequent reallocation overhead        |\n",
    "| Merging indexes | Ensure index compatibility; retrain ANN indexes if necessary   |\n",
    "| Backup          | Periodically backup indexes to prevent data loss in production |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c46a3",
   "metadata": {},
   "source": [
    "## **Index Types and Trade-offs in FAISS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d96d4a",
   "metadata": {},
   "source": [
    "> FAISS offers a variety of **index types** designed to optimize **speed, memory usage, and accuracy** for different vector search scenarios. Understanding the **trade-offs** between these indexes is crucial to selecting the right solution for your dataset and use case.\n",
    "\n",
    "---\n",
    "\n",
    "**Flat, IVFFlat, IVF-PQ, HNSW, and Hybrid Indexes**\n",
    "\n",
    "| Index Type                                        | Description                                                       | Accuracy       | Speed                           | Memory Usage | Training Required          | Best Use Case                                                           |\n",
    "| ------------------------------------------------- | ----------------------------------------------------------------- | -------------- | ------------------------------- | ------------ | -------------------------- | ----------------------------------------------------------------------- |\n",
    "| **Flat (Brute-Force)**                            | Exact nearest neighbor search; compares query with all vectors    | 100%           | Slow for large datasets         | High         | No                         | Small datasets, high precision needs                                    |\n",
    "| **IVFFlat (Inverted File)**                       | Partitions dataset into clusters; searches only relevant clusters | High (approx.) | Faster than Flat                | Medium       | Yes (train on sample data) | Large datasets, balanced speed & accuracy                               |\n",
    "| **IVF-PQ (Inverted File + Product Quantization)** | Combines IVF with quantization to reduce memory footprint         | Medium to High | Fast                            | Low          | Yes                        | Massive datasets where memory is limited, approximate search acceptable |\n",
    "| **HNSW (Hierarchical Navigable Small World)**     | Graph-based ANN search; vectors connected in small-world graph    | High           | Very Fast                       | Medium       | No                         | Real-time search, high-dimensional embeddings, dynamic datasets         |\n",
    "| **Hybrid Indexes**                                | Combines multiple strategies (e.g., IVF + HNSW)                   | High           | Optimized for specific workload | Medium       | Depends on combination     | Custom pipelines, large-scale RAG, recommendation systems               |\n",
    "\n",
    "---\n",
    "\n",
    "**Memory vs. Speed Trade-offs**\n",
    "\n",
    "* **Flat Index:**\n",
    "\n",
    "  * Pros: Exact results, simple to implement.\n",
    "  * Cons: Memory-intensive and slow for large datasets.\n",
    "\n",
    "* **IVFFlat / IVF-PQ:**\n",
    "\n",
    "  * Pros: Much faster search on large datasets; IVF-PQ reduces memory usage significantly.\n",
    "  * Cons: Slight loss in accuracy for approximate search; requires training.\n",
    "\n",
    "* **HNSW:**\n",
    "\n",
    "  * Pros: High accuracy and very fast queries; supports dynamic insertions.\n",
    "  * Cons: Slightly more complex structure; memory usage moderate.\n",
    "\n",
    "* **Hybrid Indexes:**\n",
    "\n",
    "  * Pros: Can be optimized for specific use cases combining speed, accuracy, and memory.\n",
    "  * Cons: Configuration complexity; tuning required.\n",
    "\n",
    "**Rule of Thumb:**\n",
    "\n",
    "* Small datasets → **Flat Index** (accuracy prioritized).\n",
    "* Medium to large datasets → **IVFFlat** (good balance).\n",
    "* Very large datasets with memory constraints → **IVF-PQ**.\n",
    "* Real-time queries and high-dimensional embeddings → **HNSW**.\n",
    "* Custom pipelines → **Hybrid** indexes.\n",
    "\n",
    "---\n",
    "\n",
    "**Selecting the Right Index for Your Data\n",
    "**\n",
    "1. **Dataset Size:**\n",
    "\n",
    "   * Small (<100k vectors) → Flat index\n",
    "   * Large (>1M vectors) → IVF or HNSW\n",
    "\n",
    "2. **Memory Constraints:**\n",
    "\n",
    "   * Limited memory → IVF-PQ\n",
    "   * Memory available → Flat or HNSW\n",
    "\n",
    "3. **Query Latency Requirements:**\n",
    "\n",
    "   * Real-time → HNSW or Hybrid\n",
    "   * Batch processing → IVFFlat or IVF-PQ\n",
    "\n",
    "4. **Accuracy Needs:**\n",
    "\n",
    "   * Critical accuracy → Flat or HNSW\n",
    "   * Approximate acceptable → IVFFlat or IVF-PQ\n",
    "\n",
    "5. **Update Frequency:**\n",
    "\n",
    "   * Frequent updates → HNSW (dynamic insertions)\n",
    "   * Static dataset → IVF-PQ or IVFFlat\n",
    "\n",
    "**Example Decision Flow:**\n",
    "\n",
    "| Condition                                        | Recommended Index |\n",
    "| ------------------------------------------------ | ----------------- |\n",
    "| Small dataset, exact search                      | Flat              |\n",
    "| Large dataset, approximate search, medium memory | IVFFlat           |\n",
    "| Large dataset, memory-limited, approximate       | IVF-PQ            |\n",
    "| High-dimensional embeddings, real-time queries   | HNSW              |\n",
    "| Custom needs, mixed requirements                 | Hybrid            |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f9c75",
   "metadata": {},
   "source": [
    "## **Quantization Techniques in FAISS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41303ea",
   "metadata": {},
   "source": [
    "> Quantization is a set of **techniques used to compress vectors**, reducing memory usage and speeding up approximate nearest neighbor (ANN) search without a significant loss in accuracy. FAISS provides several quantization methods to handle **large-scale vector datasets efficiently**.\n",
    "\n",
    "---\n",
    "\n",
    "**Product Quantization (PQ)**\n",
    "\n",
    "* **Concept:**\n",
    "\n",
    "  * Divides each high-dimensional vector into **sub-vectors**.\n",
    "  * Each sub-vector is **quantized separately** using a codebook, allowing compact storage.\n",
    "* **Benefits:**\n",
    "\n",
    "  * Drastically reduces memory footprint.\n",
    "  * Enables fast approximate distance computation using precomputed tables.\n",
    "* **Trade-offs:**\n",
    "\n",
    "  * Slight loss of accuracy compared to exact search.\n",
    "  * Works best when combined with IVF for large datasets.\n",
    "\n",
    "**Example:** Using PQ in FAISS\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "d = 128  # Dimension\n",
    "m = 8    # Number of sub-vectors\n",
    "nbits = 8  # Bits per sub-vector\n",
    "\n",
    "xb = np.random.random((10000, d)).astype('float32')\n",
    "index_pq = faiss.IndexPQ(d, m, nbits)\n",
    "index_pq.train(xb)  # Train PQ codebooks\n",
    "index_pq.add(xb)\n",
    "query = np.random.random((1, d)).astype('float32')\n",
    "D, I = index_pq.search(query, k=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Scalar Quantization**\n",
    "\n",
    "* **Concept:**\n",
    "\n",
    "  * Each vector component is quantized independently to a lower-precision representation (e.g., 8-bit integers).\n",
    "* **Benefits:**\n",
    "\n",
    "  * Extremely fast and memory-efficient.\n",
    "  * Simple to implement.\n",
    "* **Limitations:**\n",
    "\n",
    "  * Accuracy loss may be higher for high-dimensional or highly correlated vectors.\n",
    "\n",
    "---\n",
    "\n",
    "**OPQ (Optimized Product Quantization)**\n",
    "\n",
    "* **Concept:**\n",
    "\n",
    "  * Enhances standard PQ by **rotating vectors before quantization** to reduce quantization error.\n",
    "  * Learned linear transformation maximizes variance along sub-vectors.\n",
    "* **Benefits:**\n",
    "\n",
    "  * Improves accuracy compared to vanilla PQ.\n",
    "  * Especially useful for **high-dimensional embeddings**.\n",
    "* **Trade-offs:**\n",
    "\n",
    "  * Requires extra **training** for the rotation matrix.\n",
    "  * Slightly more computational overhead during indexing.\n",
    "\n",
    "**Example:** Using OPQ with FAISS\n",
    "\n",
    "```python\n",
    "d = 128\n",
    "m = 8\n",
    "nbits = 8\n",
    "\n",
    "index_opq = faiss.IndexPQ(d, m, nbits)\n",
    "index_opq = faiss.IndexPreTransform(faiss.OPQMatrix(d, m), index_opq)\n",
    "index_opq.train(xb)\n",
    "index_opq.add(xb)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Clustering and Partitioning**\n",
    "\n",
    "* FAISS uses **clustering algorithms like k-means** to partition vectors into **coarse groups**.\n",
    "* Helps in **IVF-based indexes** by reducing the number of vectors searched per query.\n",
    "\n",
    "**Coarse Quantizers:**\n",
    "\n",
    "* Assign each vector to the nearest cluster centroid.\n",
    "* During search, only vectors in the **closest clusters** are considered, improving speed.\n",
    "* **Example:** IVF index uses coarse quantization:\n",
    "\n",
    "```python\n",
    "nlist = 100  # Number of clusters\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "ivf_index = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
    "ivf_index.train(xb)\n",
    "ivf_index.add(xb)\n",
    "```\n",
    "\n",
    "* **Probing:** During query, the `nprobe` parameter determines **how many clusters to search**, balancing accuracy vs. speed.\n",
    "\n",
    "---\n",
    "\n",
    "**Handling Large Datasets**\n",
    "\n",
    "1. **Batch Processing:**\n",
    "\n",
    "   * Add vectors in batches to avoid memory spikes.\n",
    "\n",
    "2. **Disk-Based Indexes:**\n",
    "\n",
    "   * Use **IVF-PQ** or **IndexIVFScalarQuantizer** to reduce RAM usage.\n",
    "\n",
    "3. **GPU Acceleration:**\n",
    "\n",
    "   * Transfer indexes to GPU for faster training and search:\n",
    "\n",
    "   ```python\n",
    "   res = faiss.StandardGpuResources()\n",
    "   gpu_index = faiss.index_cpu_to_gpu(res, 0, ivf_index)\n",
    "   ```\n",
    "\n",
    "4. **Hybrid Approaches:**\n",
    "\n",
    "   * Combine coarse quantization (IVF) with PQ or OPQ for **memory-efficient approximate search** on massive datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29889f19",
   "metadata": {},
   "source": [
    "### **GPU Acceleration in FAISS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc931a",
   "metadata": {},
   "source": [
    "> FAISS provides **GPU acceleration** to significantly speed up vector indexing and similarity search, especially for **large-scale, high-dimensional datasets**. Using GPUs allows **parallel computation of distance metrics** and fast approximate nearest neighbor searches.\n",
    "\n",
    "---\n",
    "\n",
    "**GPU vs. CPU Indexes**\n",
    "\n",
    "| Feature                   | CPU Index                             | GPU Index                                                     |\n",
    "| ------------------------- | ------------------------------------- | ------------------------------------------------------------- |\n",
    "| **Speed**                 | Slower for large datasets             | Much faster due to parallelism                                |\n",
    "| **Memory**                | Limited to system RAM                 | Limited to GPU VRAM; multiple GPUs can be used                |\n",
    "| **Scalability**           | Easy to scale with disk-based storage | Better for real-time, in-memory search on massive datasets    |\n",
    "| **Index Types Supported** | All FAISS indexes                     | Most, but some complex indexes may require CPU fallback       |\n",
    "| **Ease of Use**           | Simple, no dependencies               | Requires CUDA, GPU drivers, and proper FAISS GPU installation |\n",
    "\n",
    "**Example:** Creating a CPU vs. GPU index\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "d = 128\n",
    "xb = np.random.random((10000, d)).astype('float32')\n",
    "query = np.random.random((1, d)).astype('float32')\n",
    "\n",
    "# CPU Flat Index\n",
    "cpu_index = faiss.IndexFlatL2(d)\n",
    "cpu_index.add(xb)\n",
    "D_cpu, I_cpu = cpu_index.search(query, k=5)\n",
    "\n",
    "# GPU Flat Index\n",
    "res = faiss.StandardGpuResources()\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)  # Transfer CPU index to GPU\n",
    "D_gpu, I_gpu = gpu_index.search(query, k=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Multi-GPU Support**\n",
    "\n",
    "* FAISS can leverage **multiple GPUs** for indexing and search of extremely large datasets.\n",
    "* Useful when **single GPU memory is insufficient** or parallel computation is needed for high throughput.\n",
    "\n",
    "**Steps for Multi-GPU:**\n",
    "\n",
    "1. Create a **GPU resource object for each GPU**.\n",
    "2. Distribute index shards across GPUs using `faiss.index_cpu_to_all_gpus()`.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "gpu_index_multi = faiss.index_cpu_to_all_gpus(cpu_index)\n",
    "D_multi, I_multi = gpu_index_multi.search(query, k=5)\n",
    "```\n",
    "\n",
    "* FAISS automatically **splits computations across GPUs**, providing **linear speedup** in many scenarios.\n",
    "* Multi-GPU works best with **large batch queries or huge indexes**.\n",
    "\n",
    "---\n",
    "\n",
    "**Performance Tuning**\n",
    "\n",
    "To maximize performance on GPU:\n",
    "\n",
    "1. **Batch Queries:**\n",
    "\n",
    "   * Perform multiple queries simultaneously to fully utilize GPU cores.\n",
    "\n",
    "2. **Use Approximate Search:**\n",
    "\n",
    "   * Combine IVF or HNSW with PQ/OPQ to reduce computation.\n",
    "\n",
    "3. **Tune IVF Parameters:**\n",
    "\n",
    "   * `nlist` (number of clusters) and `nprobe` (number of clusters to search) control the **trade-off between speed and accuracy**.\n",
    "\n",
    "4. **Memory Management:**\n",
    "\n",
    "   * Monitor GPU memory usage to prevent **out-of-memory errors**.\n",
    "   * Use smaller **sub-vector size or PQ codes** to reduce memory footprint.\n",
    "\n",
    "5. **Hybrid Indexing:**\n",
    "\n",
    "   * Combine coarse quantizers (IVF) with PQ/OPQ on GPU for **efficient large-scale ANN search**.\n",
    "\n",
    "6. **Use Pretrained Indexes:**\n",
    "\n",
    "   * Load precomputed indexes to GPU for **immediate high-performance search** without retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce816c",
   "metadata": {},
   "source": [
    "## **Embedding Generation and FAISS Integration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f02fd5",
   "metadata": {},
   "source": [
    "> FAISS works most effectively when **vectors (embeddings) represent meaningful features** of your data. Proper embedding generation and preprocessing are crucial for **accurate similarity search** and **retrieval-augmented generation (RAG) pipelines**.\n",
    "\n",
    "---\n",
    "\n",
    "**Integrating FAISS with NLP Embeddings**\n",
    "\n",
    "**NLP embeddings** are dense vector representations of text generated by models such as **BERT, OpenAI embeddings, or Hugging Face Transformers**. These embeddings capture **semantic meaning**, enabling FAISS to perform **semantic search and retrieval**.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Generate embeddings using your model of choice:\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "texts = [\"Hello world\", \"FAISS vector search\", \"OpenAI embeddings\"]\n",
    "embeddings = model.encode(texts).astype('float32')\n",
    "```\n",
    "\n",
    "2. Add embeddings to a FAISS index:\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n",
    "```\n",
    "\n",
    "3. Perform similarity search:\n",
    "\n",
    "```python\n",
    "query = model.encode([\"Vector search example\"]).astype('float32')\n",
    "D, I = index.search(query, k=3)\n",
    "print(\"Nearest neighbors:\", I)\n",
    "```\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "* Normalize embeddings when using **cosine similarity**:\n",
    "\n",
    "```python\n",
    "faiss.normalize_L2(embeddings)\n",
    "faiss.normalize_L2(query)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Image Embeddings**\n",
    "\n",
    "FAISS is also widely used for **image similarity search**. Embeddings can be generated using:\n",
    "\n",
    "* **CNN Features:** Extracted from models like ResNet, EfficientNet.\n",
    "* **CLIP:** Produces joint image-text embeddings, allowing **cross-modal search**.\n",
    "\n",
    "**Example with CLIP:**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "images = [Image.open(\"example1.jpg\"), Image.open(\"example2.jpg\")]\n",
    "inputs = processor(images=images, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embeddings = model.get_image_features(**inputs).numpy().astype('float32')\n",
    "```\n",
    "\n",
    "* Add embeddings to FAISS and perform similarity queries just like text embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "**Preprocessing and Normalization**\n",
    "\n",
    "1. **Standardize Vector Dimensions:** Ensure all embeddings have the same size (`d`).\n",
    "2. **Normalization:**\n",
    "\n",
    "   * Cosine similarity requires L2-normalized embeddings.\n",
    "   * `faiss.normalize_L2(vectors)`\n",
    "3. **Batch Processing:** Efficiently generate embeddings in batches to save memory.\n",
    "\n",
    "---\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "* FAISS is commonly used in **RAG pipelines** to retrieve relevant documents or context for LLMs.\n",
    "* Workflow:\n",
    "\n",
    "  1. Embed a **corpus of documents** into vectors.\n",
    "  2. Store them in FAISS.\n",
    "  3. When a **user query** arrives, embed it and retrieve nearest neighbors.\n",
    "  4. Feed retrieved documents into an LLM for **contextualized generation**.\n",
    "\n",
    "**Example Integration with OpenAI LLM:**\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"YOUR_KEY\")\n",
    "query_embedding = model.encode([\"Explain FAISS indexing\"]).astype('float32')\n",
    "D, I = index.search(query_embedding, k=3)\n",
    "\n",
    "# Retrieve top documents and pass to LLM\n",
    "docs = [texts[i] for i in I[0]]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \" \".join(docs)}]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Using FAISS with Vector Databases**\n",
    "\n",
    "* FAISS can be used as a **standalone vector search engine** or integrated into **vector databases** like **Chroma, Milvus, or Weaviate**.\n",
    "* Vector databases often provide **metadata storage, filtering, and persistence**, while FAISS handles **high-performance similarity search**.\n",
    "\n",
    "---\n",
    "\n",
    "**LangChain and FAISS Integration**\n",
    "\n",
    "* FAISS integrates seamlessly with **LangChain** for building **RAG pipelines and QA systems**.\n",
    "* Steps:\n",
    "\n",
    "  1. Store embeddings in a FAISS index.\n",
    "  2. Use LangChain **retriever interfaces** to query the index.\n",
    "  3. Feed retrieved documents to LLM chains for response generation.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_texts(texts, embedding_model)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", k=3)\n",
    "query = \"How does FAISS handle large datasets?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Building QA Systems**\n",
    "\n",
    "1. **Document Ingestion:** Preprocess and embed knowledge base documents.\n",
    "2. **Indexing:** Add embeddings to FAISS.\n",
    "3. **Query Handling:** Embed user query and retrieve nearest vectors.\n",
    "4. **LLM Response Generation:** Pass retrieved context to an LLM for answer generation.\n",
    "5. **Optional Enhancements:**\n",
    "\n",
    "   * Use **metadata filtering** for domain-specific retrieval.\n",
    "   * Implement **multi-hop reasoning** with multiple retrievals.\n",
    "   * Cache embeddings for frequently asked questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fec7a",
   "metadata": {},
   "source": [
    "## **Hybrid Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e3065",
   "metadata": {},
   "source": [
    "> Hybrid search combines the strengths of **traditional keyword-based search** with **semantic vector-based search**, delivering more **relevant, context-aware results**. This approach is widely used in **modern search engines, QA systems, and recommendation platforms**.\n",
    "\n",
    "---\n",
    "\n",
    "**Combining Keyword and Vector Search**\n",
    "\n",
    "* **Keyword Search:**\n",
    "\n",
    "  * Matches query terms directly against text or metadata.\n",
    "  * Fast and precise for exact matches but limited in understanding context or synonyms.\n",
    "\n",
    "* **Vector Search:**\n",
    "\n",
    "  * Uses embeddings to find **semantically similar content**.\n",
    "  * Captures meaning beyond exact words, robust to paraphrasing.\n",
    "\n",
    "**Hybrid Approach:**\n",
    "\n",
    "* Both search types are executed in parallel.\n",
    "* Results are **merged using a weighted scoring system** to rank documents.\n",
    "* Ensures both **exact matches and semantic relevance** are considered.\n",
    "\n",
    "**Example Workflow:**\n",
    "\n",
    "1. Generate **query embedding** for vector search.\n",
    "2. Run **keyword search** on document corpus.\n",
    "3. Retrieve **top-N results** from both methods.\n",
    "4. Combine results using a **custom scoring formula**.\n",
    "\n",
    "---\n",
    "\n",
    "**Weighted Scoring Mechanisms**\n",
    "\n",
    "* Each document receives **two scores**:\n",
    "\n",
    "  1. **Vector similarity score** (cosine similarity or L2 distance).\n",
    "  2. **Keyword relevance score** (BM25, TF-IDF, or custom metric).\n",
    "\n",
    "* **Combined score:**\n",
    "\n",
    "```python\n",
    "final_score = alpha * vector_score + (1 - alpha) * keyword_score\n",
    "```\n",
    "\n",
    "* `alpha` controls the balance:\n",
    "\n",
    "  * `alpha = 0.7` → more emphasis on semantic relevance\n",
    "  * `alpha = 0.3` → more emphasis on exact keyword matches\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "* Normalize scores to the same scale before combining.\n",
    "* Tune `alpha` based on application type:\n",
    "\n",
    "  * QA Systems → higher vector weight.\n",
    "  * Legal or scientific search → higher keyword weight.\n",
    "* Can extend to **multi-modal hybrid search** (e.g., combining text, image, and metadata).\n",
    "\n",
    "---\n",
    "\n",
    "**Real-World Examples**\n",
    "\n",
    "| Use Case                   | Hybrid Strategy                                                                                       | Benefits                                                                          |\n",
    "| -------------------------- | ----------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------- |\n",
    "| **E-commerce Search**      | Combine product title keyword search with embedding-based semantic search of descriptions and reviews | Improves discovery of relevant products even when exact keywords are missing      |\n",
    "| **Enterprise Document QA** | Keyword search on legal terms + vector search on embeddings of policy documents                       | Ensures precise legal term matches while retrieving semantically relevant content |\n",
    "| **Content Recommendation** | Vector search on user interaction history + keyword-based category filters                            | Balances semantic similarity with categorical relevance                           |\n",
    "| **Academic Research**      | Keyword search on metadata + vector search on paper abstracts                                         | Retrieves exact subject papers while identifying semantically related research    |\n",
    "\n",
    "---\n",
    "\n",
    "**Implementing Hybrid Search**\n",
    "\n",
    "**Example using FAISS + Keyword Search**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "docs = [\"Machine learning basics\", \"Deep learning with PyTorch\", \"FAISS vector search\"]\n",
    "\n",
    "# Keyword-based scoring\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "query = \"vector search\"\n",
    "keyword_scores = tfidf_matrix @ vectorizer.transform([query]).T\n",
    "keyword_scores = keyword_scores.toarray().flatten()\n",
    "\n",
    "# Vector-based scoring using embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(docs).astype('float32')\n",
    "query_embedding = model.encode([query]).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "D, I = index.search(query_embedding, k=len(docs))\n",
    "vector_scores = 1 / (1 + D.flatten())  # Convert distance to similarity\n",
    "\n",
    "# Hybrid score\n",
    "alpha = 0.6\n",
    "final_scores = alpha * vector_scores + (1 - alpha) * keyword_scores\n",
    "ranked_docs = [docs[i] for i in np.argsort(-final_scores)]\n",
    "print(ranked_docs)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8c228",
   "metadata": {},
   "source": [
    "## **Handling Large Datasets in FAISS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc830a6",
   "metadata": {},
   "source": [
    "> FAISS is designed to **scale to millions or even billions of vectors**, but working with very large datasets requires **special strategies for memory management, distributed computing, and incremental updates**. Proper handling ensures **efficient, low-latency searches** without exhausting system resources.\n",
    "\n",
    "---\n",
    "\n",
    "**Sharding Indexes**\n",
    "\n",
    "* **Concept:** Split a large dataset into smaller **shards**, each with its own FAISS index.\n",
    "* **Benefits:**\n",
    "\n",
    "  * Reduces memory usage per index.\n",
    "  * Enables **parallel search across shards**.\n",
    "* **Implementation:**\n",
    "\n",
    "  * Divide embeddings into N chunks.\n",
    "  * Create separate indexes for each chunk.\n",
    "  * Perform queries across all shards and merge results.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "shard_size = 100000\n",
    "shards = [embeddings[i:i+shard_size] for i in range(0, len(embeddings), shard_size)]\n",
    "indexes = []\n",
    "for shard in shards:\n",
    "    idx = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    idx.add(shard)\n",
    "    indexes.append(idx)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Memory Management**\n",
    "\n",
    "* **Strategies:**\n",
    "\n",
    "  * Use **IVF-PQ or HNSW** to reduce RAM usage.\n",
    "  * Normalize vectors in-place to avoid duplicates.\n",
    "  * Store indexes on **disk or persistent storage** when not in active use.\n",
    "* **GPU Considerations:**\n",
    "\n",
    "  * Transfer only active shards to GPU for processing.\n",
    "  * Use `faiss.index_cpu_to_gpu()` selectively for high-demand queries.\n",
    "\n",
    "---\n",
    "\n",
    "**Incremental Updates**\n",
    "\n",
    "* FAISS supports **adding new vectors without retraining** for many index types (e.g., Flat, HNSW).\n",
    "* For **IVF-PQ or IVF-Flat**, retraining may be needed periodically to maintain accuracy.\n",
    "* **Best practices:**\n",
    "\n",
    "  * Batch additions to minimize overhead.\n",
    "  * Keep a **versioned index system** for rollback and consistency.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "new_vectors = np.random.random((5000, d)).astype('float32')\n",
    "indexes[0].add(new_vectors)  # Incremental addition to first shard\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Distributed FAISS**\n",
    "\n",
    "* For extremely large datasets, FAISS can be **distributed across multiple machines**.\n",
    "* Strategies include:\n",
    "\n",
    "  1. **Sharded Indexes across nodes** – Each node handles a portion of the dataset.\n",
    "  2. **Parallel Queries** – Query all nodes simultaneously and merge results.\n",
    "\n",
    "**Multi-Node Setup:**\n",
    "\n",
    "* Each node runs a FAISS instance with a shard.\n",
    "* A **central coordinator** aggregates results.\n",
    "* Optional: Use **gRPC or REST APIs** for communication between nodes.\n",
    "\n",
    "---\n",
    "**Parallelizing Search**\n",
    "\n",
    "* **Within a single node:**\n",
    "\n",
    "  * FAISS supports **multi-threaded search** using `omp_set_num_threads()`.\n",
    "\n",
    "  ```python\n",
    "  faiss.omp_set_num_threads(8)\n",
    "  ```\n",
    "* **Across multiple nodes or GPUs:**\n",
    "\n",
    "  * Each shard can be queried independently.\n",
    "  * Merge top-k results using a **priority queue** or sorted merge.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "* Use **GPU acceleration** for high-dimensional embeddings.\n",
    "* Adjust **nprobe** (for IVF indexes) to balance **speed and accuracy**.\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practices**\n",
    "\n",
    "| Area               | Recommendation                                                           |\n",
    "| ------------------ | ------------------------------------------------------------------------ |\n",
    "| Indexing           | Use approximate indexes (IVF-PQ, HNSW) for massive datasets              |\n",
    "| Sharding           | Split dataset into manageable chunks to reduce memory usage              |\n",
    "| Updates            | Batch incremental additions; retrain IVF indexes periodically            |\n",
    "| Parallelization    | Use multi-threading, multi-GPU, or multi-node setups for high throughput |\n",
    "| Persistence        | Save indexes to disk regularly; maintain versioned backups               |\n",
    "| Monitoring         | Track memory and GPU usage; monitor search latency                       |\n",
    "| Query Optimization | Adjust `nprobe` (IVF) or `efSearch` (HNSW) for optimal trade-offs        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21823f13",
   "metadata": {},
   "source": [
    "## **Performance Optimization in FAISS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390c0648",
   "metadata": {},
   "source": [
    "> Optimizing FAISS is crucial for **low-latency, high-throughput vector search** in production environments. Performance tuning involves **index selection, parameter optimization, hardware utilization, and careful deployment strategies**.\n",
    "\n",
    "---\n",
    "\n",
    "**Speeding Up Searches**\n",
    "\n",
    "1. **Use Approximate Nearest Neighbor (ANN) Indexes:**\n",
    "\n",
    "   * **IVF-PQ, HNSW, or Hybrid indexes** provide significant speedups over brute-force Flat indexes.\n",
    "   * Trade-off between speed and accuracy is adjustable via parameters like `nprobe` or `efSearch`.\n",
    "\n",
    "2. **GPU Acceleration:**\n",
    "\n",
    "   * Transfer indexes to GPU using `faiss.index_cpu_to_gpu()` for **parallelized distance computation**.\n",
    "   * Multi-GPU setups can handle very large indexes efficiently.\n",
    "\n",
    "3. **Batch Queries:**\n",
    "\n",
    "   * Process multiple queries simultaneously to maximize **parallel computation efficiency**.\n",
    "\n",
    "4. **Normalized Embeddings:**\n",
    "\n",
    "   * Normalize vectors to L2 for **cosine similarity**, which can be computed faster than non-normalized distance metrics.\n",
    "\n",
    "5. **Index Sharding:**\n",
    "\n",
    "   * Split large datasets into multiple shards to **reduce per-index memory footprint** and enable parallel search.\n",
    "\n",
    "---\n",
    "\n",
    "**Index Tuning and Parameters**\n",
    "\n",
    "| Index Type   | Key Parameters                                               | Effect                                                                                                                 |\n",
    "| ------------ | ------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------- |\n",
    "| **IVF**      | `nlist` (number of clusters), `nprobe` (clusters to search)  | Higher `nlist` improves partitioning; higher `nprobe` improves accuracy but increases latency                          |\n",
    "| **HNSW**     | `M` (max connections per node), `efConstruction`, `efSearch` | Higher `M` and `efConstruction` improve accuracy but require more memory; `efSearch` balances query speed vs. accuracy |\n",
    "| **PQ / OPQ** | `m` (sub-vectors), `nbits` (quantization bits)               | More sub-vectors and bits improve accuracy but increase memory usage                                                   |\n",
    "| **Flat**     | N/A                                                          | Exact search; no tuning required, but slower for large datasets                                                        |\n",
    "\n",
    "* **Recommendation:** Start with default parameters and **benchmark on your dataset**, then tune based on latency vs. accuracy requirements.\n",
    "\n",
    "---\n",
    "\n",
    "**Benchmarking and Profiling**\n",
    "\n",
    "* **Measure query latency, throughput, and memory usage** on representative datasets.\n",
    "* **Python example for timing searches:**\n",
    "\n",
    "```python\n",
    "import time\n",
    "start = time.time()\n",
    "D, I = index.search(query_embeddings, k=10)\n",
    "end = time.time()\n",
    "print(\"Query latency:\", end - start, \"seconds\")\n",
    "```\n",
    "\n",
    "* **Profile memory usage:** Monitor RAM and GPU VRAM consumption to avoid **out-of-memory errors**.\n",
    "* **Compare different index types** for trade-offs between **speed, accuracy, and memory footprint**.\n",
    "\n",
    "---\n",
    "\n",
    "**Deployment**\n",
    "\n",
    "**Serving FAISS with APIs**\n",
    "\n",
    "* FAISS indexes can be **exposed as REST or gRPC APIs** for real-time vector search.\n",
    "* Frameworks like **FastAPI** or **Flask** allow easy integration:\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "import faiss, numpy as np\n",
    "\n",
    "app = FastAPI()\n",
    "index = faiss.read_index(\"faiss.index\")\n",
    "\n",
    "@app.post(\"/search\")\n",
    "def search(query_vector: list, k: int = 5):\n",
    "    q = np.array(query_vector).astype('float32').reshape(1, -1)\n",
    "    D, I = index.search(q, k)\n",
    "    return {\"indices\": I.tolist(), \"distances\": D.tolist()}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Cloud Deployment (AWS, GCP, Azure)**\n",
    "\n",
    "* **Cloud VMs or GPU instances** can host FAISS for large-scale search:\n",
    "\n",
    "  * **AWS:** EC2 GPU instances, Elastic Inference\n",
    "  * **GCP:** Compute Engine GPU VMs, Vertex AI for LLM integrations\n",
    "  * **Azure:** N-series VMs for GPU workloads\n",
    "* Use **object storage** (S3, GCS, Azure Blob) for storing indexes, loaded into memory on startup.\n",
    "* Consider **auto-scaling** and **load balancing** for high query throughput.\n",
    "\n",
    "---\n",
    "**Integrating with Microservices**\n",
    "\n",
    "* FAISS can be deployed as a **microservice** within a larger architecture:\n",
    "\n",
    "  * Separate service for vector storage and search.\n",
    "  * Other services (RAG pipeline, LLM orchestration) query FAISS via **HTTP/gRPC API**.\n",
    "* Benefits:\n",
    "\n",
    "  * **Decoupled architecture**\n",
    "  * Easy scaling of the FAISS service independently\n",
    "  * Enables **multi-language or multi-platform integration**\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Optimizing FAISS involves:\n",
    "\n",
    "1. Choosing the **right index type** for your data and query load.\n",
    "2. **Tuning index parameters** like `nprobe`, `M`, and `efSearch` for your workload.\n",
    "3. Leveraging **GPU acceleration and multi-threading**.\n",
    "4. Efficiently managing **memory and large datasets** via sharding.\n",
    "5. Deploying indexes as **APIs or microservices** for scalable production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627755b",
   "metadata": {},
   "source": [
    "## **Approximate Nearest Neighbor (ANN) Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65344a",
   "metadata": {},
   "source": [
    "Approximate Nearest Neighbor (ANN) algorithms are designed to **find vectors that are close to a query vector quickly**, trading off some accuracy for **speed and memory efficiency**. They are critical when working with **high-dimensional embeddings** and **large-scale datasets**.\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "* Drastically reduce search time compared to exact nearest neighbor methods.\n",
    "* Enable scaling to **millions or billions of vectors**.\n",
    "* Often sufficient for **semantic search, recommendation systems, and RAG pipelines** where slight approximations are acceptable.\n",
    "\n",
    "**Common ANN Algorithms in FAISS:**\n",
    "\n",
    "* **HNSW (Hierarchical Navigable Small World graphs)**\n",
    "* **IVF-PQ (Inverted File with Product Quantization)**\n",
    "* **IVFFlat**\n",
    "\n",
    "---\n",
    "\n",
    "**HNSW in Depth**\n",
    "\n",
    "HNSW is a **graph-based ANN algorithm** that builds a **hierarchical network of nodes**, where each node represents a vector.\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "1. Nodes are connected to a small number of neighbors (small-world property).\n",
    "2. Search starts at the top layer and progressively descends through layers to refine nearest neighbors.\n",
    "3. Allows **efficient search with logarithmic complexity** for high-dimensional data.\n",
    "\n",
    "**Key Parameters:**\n",
    "\n",
    "| Parameter          | Description                                    | Impact                                                  |\n",
    "| ------------------ | ---------------------------------------------- | ------------------------------------------------------- |\n",
    "| **M**              | Maximum number of connections per node         | Higher M increases accuracy and memory usage            |\n",
    "| **efConstruction** | Size of dynamic list during index construction | Higher value improves index quality but slows training  |\n",
    "| **efSearch**       | Size of candidate list during search           | Higher value improves accuracy but increases query time |\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Very high accuracy for approximate search.\n",
    "* Supports **dynamic insertions** without retraining.\n",
    "* Excellent for **real-time applications** and **high-dimensional embeddings**.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "d = 128\n",
    "index = faiss.IndexHNSWFlat(d, M=32)\n",
    "index.hnsw.efConstruction = 200\n",
    "index.add(embeddings)\n",
    "index.hnsw.efSearch = 100\n",
    "D, I = index.search(query_embeddings, k=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**IVF-PQ Trade-offs**\n",
    "\n",
    "**IVF-PQ (Inverted File + Product Quantization)** combines **coarse clustering** with **vector compression**.\n",
    "\n",
    "* **How it Works:**\n",
    "\n",
    "  1. Dataset is partitioned into clusters (IVF).\n",
    "  2. Within each cluster, vectors are compressed using **PQ**, reducing memory.\n",
    "* **Parameters:**\n",
    "\n",
    "  * `nlist`: Number of clusters (affects speed vs. accuracy).\n",
    "  * `m`: Number of PQ sub-vectors (affects compression and accuracy).\n",
    "  * `nbits`: Bits per sub-vector (affects memory usage and precision).\n",
    "\n",
    "**Trade-offs:**\n",
    "\n",
    "| Factor        | High Accuracy Setting              | High Speed / Low Memory Setting |\n",
    "| ------------- | ---------------------------------- | ------------------------------- |\n",
    "| **nlist**     | Large (more clusters)              | Small (fewer clusters)          |\n",
    "| **m / nbits** | Larger m, more bits per sub-vector | Smaller m, fewer bits           |\n",
    "| **nprobe**    | High (search more clusters)        | Low (search fewer clusters)     |\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Memory-efficient for very large datasets.\n",
    "* Can handle **hundreds of millions of vectors** on commodity hardware.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* Approximate search may slightly reduce accuracy.\n",
    "* Requires **training the PQ and cluster centroids** before adding vectors.\n",
    "\n",
    "---\n",
    "\n",
    "**Evaluating Accuracy vs. Performance**\n",
    "\n",
    "When using ANN algorithms, it is important to **measure the trade-off between speed and accuracy**:\n",
    "\n",
    "1. **Accuracy Metrics:**\n",
    "\n",
    "   * **Recall@k:** Fraction of true nearest neighbors retrieved in top-k results.\n",
    "   * **Precision:** Fraction of retrieved vectors that are truly nearest neighbors.\n",
    "\n",
    "2. **Performance Metrics:**\n",
    "\n",
    "   * **Query latency:** Time per search.\n",
    "   * **Throughput:** Number of queries per second.\n",
    "   * **Memory usage:** RAM or GPU VRAM consumed by the index.\n",
    "\n",
    "**Evaluation Approach:**\n",
    "\n",
    "* Run **benchmark tests** on representative datasets.\n",
    "* Adjust **index parameters** (`nprobe`, `efSearch`, `M`) to find an **optimal balance**.\n",
    "* For real-time applications, prioritize latency and throughput; for batch offline processing, prioritize accuracy.\n",
    "\n",
    "**Example Benchmark Table:**\n",
    "\n",
    "| Index   | Recall@10 | Query Time (ms) | Memory (MB) | Use Case                              |\n",
    "| ------- | --------- | --------------- | ----------- | ------------------------------------- |\n",
    "| Flat    | 1.0       | 120             | 2000        | Small dataset, exact search           |\n",
    "| HNSW    | 0.98      | 5               | 1200        | Real-time semantic search             |\n",
    "| IVF-PQ  | 0.92      | 10              | 500         | Large-scale approximate search        |\n",
    "| IVFFlat | 0.95      | 8               | 800         | Medium-scale, balanced speed/accuracy |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a15c4b",
   "metadata": {},
   "source": [
    "## **Custom Metrics and Distance Functions in FAISS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743263dd",
   "metadata": {},
   "source": [
    "> FAISS allows customization of **distance functions and similarity metrics** to match the requirements of your data and application. Using the appropriate metric is critical for **accurate nearest neighbor retrieval**, whether you are working with text embeddings, image features, or hybrid representations.\n",
    "\n",
    "---\n",
    "\n",
    "**Implementing Common Metrics**\n",
    "\n",
    "**1. L2 (Euclidean Distance)**\n",
    "\n",
    "* **Definition:** Measures straight-line distance between vectors.\n",
    "* **Use Cases:** General-purpose similarity search, image embeddings, numeric feature spaces.\n",
    "* **FAISS Implementation:** Default for `IndexFlatL2` and many other indexes.\n",
    "\n",
    "```python\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "d = 128\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(np.random.random((1000, d)).astype('float32'))\n",
    "query = np.random.random((1, d)).astype('float32')\n",
    "D, I = index.search(query, k=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**2. Inner Product**\n",
    "\n",
    "* **Definition:** Measures similarity by dot product of vectors.\n",
    "* **Use Cases:** When vector norms matter less than direction, e.g., word embeddings, recommendation scores.\n",
    "* **FAISS Implementation:** Use `IndexFlatIP`\n",
    "\n",
    "```python\n",
    "index = faiss.IndexFlatIP(d)\n",
    "faiss.normalize_L2(vectors)  # Optional for cosine similarity equivalence\n",
    "index.add(vectors)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**3. Cosine Similarity**\n",
    "\n",
    "* **Definition:** Measures angle between vectors, independent of magnitude.\n",
    "* **Use Cases:** Semantic similarity for NLP embeddings, text search.\n",
    "* **Implementation in FAISS:** Normalize vectors and use `IndexFlatIP`\n",
    "\n",
    "```python\n",
    "faiss.normalize_L2(vectors)\n",
    "faiss.normalize_L2(query)\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(vectors)\n",
    "D, I = index.search(query, k=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Custom Similarity Functions**\n",
    "\n",
    "* FAISS allows **user-defined distance metrics** using the **Python API or C++ extensions**.\n",
    "* Custom metrics can incorporate **domain-specific weights, hybrid features, or multi-modal embeddings**.\n",
    "\n",
    "**Example:** Weighted combination of text and image embeddings:\n",
    "\n",
    "```python\n",
    "def custom_similarity(query, vectors, alpha=0.7):\n",
    "    # query and vectors: shape (n_vectors, d)\n",
    "    text_sim = np.dot(query[:, :128], vectors[:, :128].T)\n",
    "    image_sim = np.dot(query[:, 128:], vectors[:, 128:].T)\n",
    "    return alpha * text_sim + (1-alpha) * image_sim\n",
    "```\n",
    "\n",
    "* The output can then be used to **rank vectors manually or in a custom FAISS index wrapper**.\n",
    "\n",
    "---\n",
    "\n",
    "**Case Studies**\n",
    "\n",
    "**1. Semantic Search in Tex**t\n",
    "\n",
    "* **Scenario:** Search through thousands of documents for queries like \"AI in healthcare\".\n",
    "* **Approach:**\n",
    "\n",
    "  * Generate embeddings using BERT or OpenAI embeddings.\n",
    "  * Normalize vectors for cosine similarity.\n",
    "  * Store in FAISS `IndexFlatIP` or `HNSW` for fast retrieval.\n",
    "* **Outcome:** Retrieves semantically relevant documents even if keywords do not match exactly.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Image Retrieval Systems**\n",
    "\n",
    "* **Scenario:** Find visually similar images from a large image database.\n",
    "* **Approach:**\n",
    "\n",
    "  * Use CNN or CLIP embeddings.\n",
    "  * Index embeddings in FAISS using `IndexIVFPQ` for large datasets.\n",
    "  * Query by image or text (in case of CLIP cross-modal search).\n",
    "* **Outcome:** Fast, accurate retrieval of visually or semantically similar images.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Recommendation Engines**\n",
    "\n",
    "* **Scenario:** Recommend products or content to users based on embeddings of past interactions.\n",
    "* **Approach:**\n",
    "\n",
    "  * Represent users and items as vectors (e.g., collaborative filtering + content embeddings).\n",
    "  * Index item embeddings in FAISS using `HNSW` or `IVFPQ`.\n",
    "  * Perform nearest neighbor search to recommend top-k items.\n",
    "* **Outcome:** Personalized recommendations that balance semantic similarity and user preferences.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Using **custom metrics, L2, cosine, or inner product distances**, FAISS can handle **varied data types and similarity requirements**. By combining **domain-specific metrics with ANN algorithms**, FAISS powers real-world applications such as:\n",
    "\n",
    "* **Semantic text search**: Fast retrieval of relevant documents.\n",
    "* **Image retrieval**: Find visually similar images in large datasets.\n",
    "* **Recommendation systems**: Personalized suggestions with vector-based similarity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
