{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25489ee",
   "metadata": {},
   "source": [
    "# **Query Enhancement – Query Expansion Techniques**\n",
    "In a RAG pipeline, the quality of the query sent to the retriever determines how good the retrieved context is — and therefore, how accurate the LLM’s final answer will be.\n",
    "\n",
    "That’s where Query Expansion / Enhancement comes in.\n",
    "\n",
    "**What is Query Enhancement?**\n",
    "Query enhancement refers to techniques used to improve or reformulate the user query to retrieve better, more relevant documents from the knowledge base.\n",
    "It is especially useful when:\n",
    "\n",
    "- The original query is short, ambiguous, or under-specified\n",
    "- You want to broaden the scope to catch synonyms, related phrases, or spelling variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff6ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step1 : Load and split the dataset\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ba3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''chunks be like\n",
    "[Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain is an open-source framework designed for developing applications powered by large language models (LLMs). It simplifies the process of building, managing, and scaling complex chains of thought by abstracting prompt management, retrieval, memory, and agent orchestration. Developers can use'),\n",
    " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='and agent orchestration. Developers can use LangChain to create end-to-end pipelines that connect LLMs with tools, APIs, vector databases, and other knowledge sources. (v1)'),\n",
    " \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f001ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 2: Vector Store\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(chunks,embedding_model)\n",
    "\n",
    "## step 3:MMR Retriever\n",
    "retriever=vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8f2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000029BF1766510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000029BF1766F90>, root_client=<openai.OpenAI object at 0x0000029BF058B620>, root_async_client=<openai.AsyncOpenAI object at 0x0000029BF1766CF0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## step 4 : LLM and Prompt\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(\"openai:o4-mini\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116e2cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\\n\\nOriginal query: \"{query}\"\\n\\nExpanded query:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000029BF1766510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000029BF1766F90>, root_client=<openai.OpenAI object at 0x0000029BF058B620>, root_async_client=<openai.AsyncOpenAI object at 0x0000029BF1766CF0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query expansion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\n",
    "\n",
    "Original query: \"{query}\"\n",
    "\n",
    "Expanded query:\n",
    "\"\"\")\n",
    "\n",
    "query_expansion_chain=query_expansion_prompt| llm | StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d629dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expanded query:\\n\\n(\"LangChain memory\" OR \"LangChain memory management\" OR \"LangChain memory module\" OR \"persistent conversation memory\" OR \"embeddings-based memory\" OR \"stateful chatbots\" OR \"session context persistence\" OR \"RAG memory store\" OR \"LLM context window\" OR \"prompt memory\" OR \"memory retriever\")  \\nAND  \\n(\"VectorStoreMemory\" OR \"ConversationBufferMemory\" OR \"ConversationSummaryMemory\" OR \"RedisMemory\" OR \"SQLMemory\" OR \"MemoryChain\" OR \"MemoryRouter\")  \\nAND  \\n(FAISS OR Chroma OR Pinecone OR Weaviate OR Milvus OR \"vector embedding store\" OR \"document chunking\" OR \"knowledge retrieval\")'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_chain.invoke({\"query\":\"Langchain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6e3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG answering prompt\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c57e726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline with query expansion\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\": x[\"input\"]}))\n",
    "    })\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddebe80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s an expanded search query that adds synonyms, technical terms and useful context for better recall of LangChain’s memory features:\n",
      "\n",
      "“LangChain memory support” OR “LangChain memory types” OR “LangChain memory modules” OR “LangChain memory classes”  \n",
      "AND (“ConversationBufferMemory” OR “ConversationSummaryMemory” OR “CombinedMemory” OR “DynamicMemory” OR “ConversationTokenBufferMemory”)  \n",
      "OR (“short-term memory” OR “long-term memory” OR “ephemeral memory” OR “session state” OR “context window”)  \n",
      "OR (“persistent memory” OR “stateful memory store” OR “memory retriever”)  \n",
      "OR (“vector store memory” OR “embedding store” OR “semantic memory” OR “RAG”)  \n",
      "OR (“Chroma” OR “FAISS” OR “Pinecone” OR “Weaviate” OR “Redis” OR “SQLite” OR “PostgreSQL” OR “MongoDB”)  \n",
      "OR (“memory API” OR “memory variable” OR “memory backend” OR “cache” OR “in-memory” OR “file-based”)  \n",
      "AND (“LangChain Python” OR “LLMChain” OR “chatbot context management” OR “agent state management”)\n",
      "✅ Answer:\n",
      " LangChain currently ships with two built-in memory classes:  \n",
      "1. ConversationBufferMemory – keeps the raw chat history in‐memory.  \n",
      "2. ConversationSummaryMemory – maintains a running summary of past turns to stay within token limits.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"What types of memory does LangChain support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd86621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:\n",
      "\n",
      "(\"CrewAI agents\" OR \"Crew AI agents\" OR \"CrewAI bots\" OR \"Crew AI assistants\" OR \"autonomous crew agents\" OR \"AI-driven crew management assistants\" OR \"virtual crew members\" OR \"digital crew agents\")  \n",
      "AND  \n",
      "(\"crew scheduling\" OR \"workforce management\" OR \"staff rostering\" OR \"resource allocation\" OR \"employee roster optimization\")  \n",
      "AND  \n",
      "(\"multi-agent system\" OR \"autonomous agents\" OR \"distributed AI\" OR \"agent-based modeling\")  \n",
      "AND  \n",
      "(\"machine learning\" OR \"reinforcement learning\" OR \"predictive analytics\" OR \"optimization algorithms\" OR \"real-time planning\")  \n",
      "AND  \n",
      "(\"airline\" OR \"maritime\" OR \"hospitality\" OR \"logistics\" OR \"field service\")\n",
      "✅ Answer:\n",
      " CrewAI agents are semi-autonomous, role-specialized AI “workers” that team up in a predefined workflow to tackle complex, multi-step tasks.  Key points:  \n",
      "1. Defined Roles  \n",
      "   • Researcher – gathers data and insights  \n",
      "   • Planner – lays out strategy, timelines, and dependencies  \n",
      "   • Executor – carries out concrete actions (e.g. drafting text, writing code, running analyses)  \n",
      "   • Reviewer/Validator – checks quality and flags issues  \n",
      "\n",
      "2. Structured Collaboration  \n",
      "   • Unlike solitary autonomous agents, CrewAI’s agents pass work products and feedback along a chain of responsibility, ensuring each subtask is handled by the most appropriate specialist.  \n",
      "   • Communication channels and hand-off protocols are built into the system, so progress and context flow smoothly from one agent to the next.  \n",
      "\n",
      "3. Use Cases  \n",
      "   • Market Research – one agent mines data, another synthesizes findings, a third writes the report.  \n",
      "   • Legal Document Analysis – a specialist agent extracts clauses, another summarizes risks, a reviewer validates accuracy.  \n",
      "   • Product Development – ideation, prototyping, testing, documentation handled by distinct agents.  \n",
      "   • Coding Assistance – design, implementation, debugging, and code review split across agents.  \n",
      "\n",
      "By combining specialization (each agent does what it does best) with a clear, collaborative workflow, CrewAI agents can handle end-to-end, multi-stage tasks more efficiently and reliably than lone AI assistants.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"CrewAI agents?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7886c940",
   "metadata": {},
   "source": [
    "## **Querry Expansions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb909f8",
   "metadata": {},
   "source": [
    "> **Query Expansion (QE)** is a technique used in **information retrieval (IR)** and **vector-based search systems** (like FAISS, Pinecone, or Elasticsearch) to improve search accuracy and recall by **broadening the user’s query** with additional, semantically related terms or concepts.\n",
    "\n",
    "It helps the system retrieve **more relevant documents** — even if they don’t contain the *exact words* used in the original query — by introducing synonyms, related entities, or paraphrased expressions.\n",
    "\n",
    "---\n",
    "\n",
    "**1. Why Query Expansion is Needed**\n",
    "\n",
    "When users search for information, they often use **limited or ambiguous terms**. For example:\n",
    "\n",
    "| Original Query  | Problem                      | Example Missed Results                                       |\n",
    "| --------------- | ---------------------------- | ------------------------------------------------------------ |\n",
    "| “car insurance” | Doesn’t include synonyms     | Misses “automobile insurance”, “vehicle coverage”            |\n",
    "| “heart attack”  | Doesn’t include medical term | Misses “myocardial infarction”                               |\n",
    "| “AI jobs”       | Ambiguous                    | Misses “machine learning engineer”, “data science positions” |\n",
    "\n",
    "Query expansion solves this by **adding semantically related words**, improving **recall** (more relevant results) without losing **precision** (accuracy of retrieved documents).\n",
    "\n",
    "---\n",
    "\n",
    "**2. Types of Query Expansion**\n",
    "\n",
    "There are several ways to expand a query depending on the source and intent of expansion:\n",
    "\n",
    "| Type                             | Description                                                           | Example                                                            |\n",
    "| -------------------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| **Synonym Expansion**            | Adds synonyms or alternate terms                                      | “doctor” → “physician”, “medical practitioner”                     |\n",
    "| **Semantic Expansion**           | Uses embeddings or knowledge graphs to add conceptually related terms | “AI” → “machine learning”, “deep learning”, “neural networks”      |\n",
    "| **Stemming / Lemmatization**     | Expands to different forms of the same word                           | “run” → “running”, “ran”                                           |\n",
    "| **Contextual Expansion**         | Uses LLMs or context understanding to expand query meaningfully       | “COVID vaccines” → “Pfizer”, “Moderna”, “vaccine efficacy”         |\n",
    "| **Relevance Feedback Expansion** | Expands based on previous user feedback or clicked results            | If user clicked “Tesla”, add “EV”, “electric vehicle”, “Elon Musk” |\n",
    "| **Statistical Expansion**        | Expands using co-occurrence frequency in corpus                       | “bank” → “river”, “loan”, depending on surrounding words           |\n",
    "\n",
    "---\n",
    "\n",
    "**3. How Query Expansion Works**\n",
    "\n",
    "The general workflow of Query Expansion includes:\n",
    "\n",
    "1. **Receive the original query**\n",
    "   e.g., “AI in healthcare”\n",
    "\n",
    "2. **Analyze or embed the query**\n",
    "   Convert it into an embedding vector or parse keywords.\n",
    "\n",
    "3. **Generate related terms** using one of the following:\n",
    "\n",
    "   * Predefined thesaurus (like WordNet)\n",
    "   * Semantic embedding similarity\n",
    "   * LLMs (e.g., GPT models)\n",
    "   * Statistical models or co-occurrence analysis\n",
    "\n",
    "4. **Expand the query** by adding similar or related terms\n",
    "   → “AI in healthcare” → “machine learning in medicine”, “deep learning for diagnostics”\n",
    "\n",
    "5. **Search using the expanded query set**\n",
    "   Combine results and rank them based on similarity, relevance, or diversity.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Query Expansion in Vector Databases**\n",
    "\n",
    "In **vector search systems** like FAISS, Pinecone, or Chroma, query expansion is often done by:\n",
    "\n",
    "* **Embedding expansion**:\n",
    "  Create multiple embeddings (vectors) for semantically similar queries and perform a joint search.\n",
    "\n",
    "* **Weighted retrieval**:\n",
    "  Give higher weights to original query terms but still include related embeddings.\n",
    "\n",
    "* **Re-ranking or MMR integration**:\n",
    "  Combine expanded query results with MMR (Maximal Marginal Relevance) for diversity and precision.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Query Expansion with LangChain and OpenAI**\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Step 1: Define the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Step 2: Create a prompt for query expansion\n",
    "template = \"\"\"\n",
    "Expand the following query by adding relevant and semantically related search terms:\n",
    "Query: {query}\n",
    "Return a list of related phrases or keywords.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"query\"], template=template)\n",
    "\n",
    "# Step 3: Build the chain\n",
    "expand_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Step 4: Run the expansion\n",
    "expanded_query = expand_chain.run(\"AI in healthcare\")\n",
    "print(expanded_query)\n",
    "```\n",
    "\n",
    "**Output Example:**\n",
    "\n",
    "```\n",
    "[\"artificial intelligence in medicine\", \"machine learning for diagnosis\", \n",
    " \"deep learning in healthcare\", \"medical AI applications\", \n",
    " \"health data analytics\"]\n",
    "```\n",
    "\n",
    "Then these terms can be embedded and searched together in a **vector database** like FAISS or Pinecone.\n",
    "\n",
    "---\n",
    "\n",
    "**5. Query Expansion Techniques by Source**\n",
    "\n",
    "| Technique                 | Source                             | Description                                             |\n",
    "| ------------------------- | ---------------------------------- | ------------------------------------------------------- |\n",
    "| **Thesaurus-Based**       | WordNet, domain glossary           | Adds linguistic synonyms                                |\n",
    "| **Embedding-Based**       | OpenAI, BERT, SentenceTransformers | Adds semantically related terms using vector similarity |\n",
    "| **Knowledge Graph-Based** | Wikidata, ConceptNet               | Adds conceptually linked entities                       |\n",
    "| **LLM-Based**             | GPT, Claude, Gemini                | Dynamically generates context-aware expansions          |\n",
    "| **Relevance Feedback**    | User interactions                  | Uses click history to improve expansion accuracy        |\n",
    "\n",
    "---\n",
    "\n",
    "**6. Example: FAISS + Query Expansion**\n",
    "\n",
    "You can use query expansion in FAISS like this:\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Original query\n",
    "query = \"AI in healthcare\"\n",
    "\n",
    "# Expanded queries (manually or via LLM)\n",
    "expanded_queries = [\n",
    "    \"AI in healthcare\",\n",
    "    \"machine learning in medicine\",\n",
    "    \"deep learning in diagnostics\"\n",
    "]\n",
    "\n",
    "# Search results from multiple queries\n",
    "results = []\n",
    "for q in expanded_queries:\n",
    "    docs = vectorstore.similarity_search(q, k=3)\n",
    "    results.extend(docs)\n",
    "\n",
    "# Optionally deduplicate and rerank results\n",
    "unique_results = list({doc.page_content: doc for doc in results}.values())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**7. Benefits of Query Expansion**\n",
    "\n",
    "- **Improved Recall** — Retrieves more relevant documents.\n",
    "- **Enhanced Understanding** — Captures broader meaning of user intent.\n",
    "- **Bridges Vocabulary Gap** — Connects different terminology (e.g., “COVID” vs. “coronavirus”).\n",
    "- **Adaptive Searching** — Works well with dynamic, domain-specific language.\n",
    "\n",
    "---\n",
    "\n",
    "**8. Drawbacks and Challenges**\n",
    "\n",
    "- **Loss of Precision** — Adding too many terms may include irrelevant results.\n",
    "- **Computational Overhead** — More queries → higher processing time.\n",
    "- **Domain Dependence** — Requires domain-specific expansion sources for best accuracy.\n",
    "- **Ambiguity** — Incorrect expansions can mislead retrieval (e.g., “bank” → “river” instead of “finance”).\n",
    "\n",
    "---\n",
    "\n",
    "**9. Balancing Expansion with Relevance**\n",
    "\n",
    "Often, systems use **weighted query expansion**, where:\n",
    "\n",
    "* Original query terms are given **higher weight**.\n",
    "* Expanded terms are given **lower weight** (controlled by α or β).\n",
    "\n",
    "[\n",
    "\\text{Final Query Vector} = \\alpha \\times \\text{Original Query} + \\beta \\times \\sum(\\text{Expanded Terms})\n",
    "]\n",
    "\n",
    "This ensures results remain faithful to the user’s intent while exploring related areas.\n",
    "\n",
    "---\n",
    "\n",
    "**10. Real-World Applications**\n",
    "\n",
    "* **Search Engines** – Expands user queries for better results (e.g., Google “Did you mean?”).\n",
    "* **RAG Pipelines** – Enhances retrieval coverage for LLMs.\n",
    "* **Recommendation Systems** – Finds similar items even with different keywords.\n",
    "* **Healthcare / Legal Search** – Identifies synonymic or technical terminology.\n",
    "* **Chatbots** – Interprets varied user phrasing for intent matching.\n",
    "\n",
    "---\n",
    "\n",
    "**11. Integrating Query Expansion with MMR and Reranking**\n",
    "\n",
    "A modern **retrieval pipeline** often combines:\n",
    "\n",
    "1. **Query Expansion** → Improves recall\n",
    "2. **MMR (Maximal Marginal Relevance)** → Adds diversity\n",
    "3. **Reranking (Cross-Encoder)** → Improves precision\n",
    "\n",
    "This results in **highly relevant, diverse, and accurate** retrieval — the foundation for **high-quality RAG systems**.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* Query Expansion enhances retrieval quality by adding **semantically related search terms**.\n",
    "* It can be implemented using **LLMs, embeddings, or linguistic databases**.\n",
    "* Works best when **combined with reranking and MMR**.\n",
    "* Crucial for **semantic search, RAG, and large-scale AI retrieval systems**.\n",
    "\n",
    "---\n",
    "\n",
    "**Formula Summary**\n",
    "\n",
    "$$\\text{Expanded Query} = Q + \\sum_{i=1}^{n} \\text{RelatedTerms}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d626d35",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
