{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "# **What is Query Decomposition?**\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "**Why Use Query Decomposition?**\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001FFD9A0B380>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001FFD97D4050>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"groq:gemma2-9b-it\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question: \"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a breakdown of the complex question into smaller sub-questions:\n",
      "\n",
      "1. **What types of memory mechanisms does LangChain offer for its models?**  (This focuses on LangChain's specific memory capabilities)\n",
      "2. **How do LangChain agents utilize memory in their decision-making processes?** (This delves into the practical application of memory within LangChain agents)\n",
      "3. **What memory and retrieval strategies does CrewAI employ for its AI agents?** (This shifts the focus to CrewAI's approach to memory)\n",
      "4. **How do the memory and agent functionalities of LangChain and CrewAI differ in terms of implementation and capabilities?** (This encourages a comparative analysis) \n",
      "\n",
      "\n",
      "These sub-questions target different aspects of the original query, allowing for more focused and precise document retrieval. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be04719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-•1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Answer:\n",
      "\n",
      "Q: Here are some sub-questions to break down the complex question:\n",
      "A: Please provide the complex question! I need to know what question you're trying to break down before I can provide sub-questions. \n",
      "\n",
      "For example, you could ask:\n",
      "\n",
      "* \"How does CrewAI work?\"\n",
      "* \"What are the advantages of using LangChain agents?\"\n",
      "* \"Can you give me an example of a multi-step workflow that could be improved by CrewAI?\"\n",
      "\n",
      "\n",
      "Once you give me the complex question, I can help you break it down into more manageable sub-questions.  \n",
      "\n",
      "\n",
      "\n",
      "Q: **What types of memory mechanisms does LangChain employ?** (This focuses on LangChain's specific memory capabilities)\n",
      "A: LangChain uses memory modules such as **ConversationBufferMemory** and **ConversationSummaryMemory**. \n",
      "\n",
      "\n",
      "These modules allow the LLM to remember past conversation turns and summarize long interactions. \n",
      "\n",
      "\n",
      "Q: **How do LangChain agents utilize memory?** (This explores the application of memory within LangChain's agent framework)\n",
      "A: LangChain agents use **context-aware memory** across the steps they take to achieve a goal. \n",
      "\n",
      "This means they can remember and utilize information from previous steps in their planning and execution process, allowing for more sophisticated and coherent task completion. \n",
      "\n",
      "\n",
      "Q: **What memory and agent functionalities are offered by CrewAI?** (This shifts the focus to CrewAI's features)\n",
      "A: While the provided text describes CrewAI agents' purpose, goals, tools, and collaborative framework, it **doesn't explicitly mention specific memory functionalities or other agent functionalities beyond their roles (researcher, planner, executor) and semi-independent operation**. \n",
      "\n",
      "\n",
      "To answer your question about CrewAI's memory and agent functionalities, you would need additional information from the documentation or other sources. \n",
      "\n",
      "\n",
      "Q: **What are the key differences in how LangChain and CrewAI handle memory and agent interactions?** (This prompts a comparative analysis)\n",
      "A: Based on the provided context, here's a comparative analysis of how LangChain and CrewAI handle memory and agent interactions:\n",
      "\n",
      "**LangChain:**\n",
      "\n",
      "* **Memory:** LangChain agents utilize **context-aware memory** that persists across steps in a sequence of tool invocations. This means the agent can remember previous interactions and use that information to make decisions in subsequent steps. \n",
      "* **Agent Interactions:** LangChain agents operate on a **planner-executor model**. The planner determines the sequence of tools to use based on the goal and the available context. The executor then carries out these instructions, interacting with the tools and updating the memory accordingly.  LangChain emphasizes **dynamic decision-making** and **branching logic** within this framework.\n",
      "\n",
      "**CrewAI:**\n",
      "\n",
      "* **Memory:** While the context mentions CrewAI managing **role-based collaboration**, it doesn't explicitly detail how memory is handled within its system. We can infer that CrewAI likely maintains some form of memory specific to each role, allowing them to track their individual tasks and interactions.\n",
      "* **Agent Interactions:** CrewAI focuses on **role-based collaboration**, suggesting a structure where agents (or entities) take on specific roles with defined responsibilities. Interactions likely revolve around these roles, with CrewAI potentially orchestrating communication and task allocation between them.\n",
      "\n",
      "**Key Differences:**\n",
      "\n",
      "* **Memory Scope:** LangChain's memory is centralized and shared by the agent, while CrewAI's memory might be distributed across individual roles.\n",
      "* **Decision-making:** LangChain emphasizes dynamic planning and decision-making within a single agent, whereas CrewAI's focus on roles suggests a more structured, collaborative decision-making process.\n",
      "\n",
      "\n",
      "**Further Clarification Needed:**\n",
      "\n",
      "To provide a more comprehensive comparison, we'd need more information about CrewAI's internal workings, particularly regarding its memory management and how it handles agent interactions beyond the basic concept of roles.\n",
      "\n",
      "\n",
      "Q: These sub-questions target specific aspects of memory and agents in both LangChain and CrewAI, allowing for more focused and accurate document retrieval\n",
      "A: The provided context highlights the strengths of both LangChain and CrewAI in handling memory and agent functionalities:\n",
      "\n",
      "**LangChain:**\n",
      "\n",
      "* **Hybrid Retrieval:** Leverages both keyword-based and embedding-based retrieval methods, improving recall by capturing both exact matches and semantically similar content. This suggests LangChain's memory system can effectively store and retrieve information based on various criteria.\n",
      "* **Agent Planner-Executor Model:**  LangChain agents use a structured approach to task completion. They plan a sequence of tool invocations, demonstrating an ability to learn from past interactions and adapt their approach based on context. The mention of \"context-aware memory use\" further emphasizes LangChain's capacity to retain and utilize relevant information across multiple steps.\n",
      "\n",
      "**CrewAI:**\n",
      "\n",
      "* **Role-Based Collaboration:**  CrewAI focuses on managing collaboration between different AI \"roles.\" This implies a sophisticated memory system capable of tracking individual roles, their responsibilities, and past interactions.\n",
      "\n",
      "**Combined Strengths:**\n",
      "\n",
      "* **Hybrid Systems:** The context emphasizes the compatibility of LangChain and CrewAI, allowing them to work together. LangChain's retrieval and tool handling capabilities can be combined with CrewAI's role-based collaboration, creating powerful hybrid systems.\n",
      "\n",
      "\n",
      "**In summary:**\n",
      "\n",
      "Both LangChain and CrewAI possess robust memory and agent functionalities.  LangChain excels in information retrieval and planning, while CrewAI specializes in managing collaborative interactions. Their combined strengths create a synergistic effect, enabling the development of sophisticated AI systems. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"✅ Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469672ef",
   "metadata": {},
   "source": [
    "## **Multi-Hop (Multi-Step) Reasoning in Large Language Models (LLMs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf71e00a",
   "metadata": {},
   "source": [
    "**Multi-hop reasoning**, also known as **multi-step reasoning**, refers to an LLM’s ability to **connect multiple pieces of information across different contexts or steps** to arrive at a final answer or decision.\n",
    "It’s a key capability that separates **surface-level retrieval** from **deep, logical comprehension**, enabling the model to reason like a human — combining facts, deducing relationships, and forming conclusions through a sequence of reasoning steps.\n",
    "\n",
    "---\n",
    "\n",
    "## **What is Multi-Hop Reasoning?**\n",
    "\n",
    "Multi-hop reasoning means an LLM performs **sequential inferencing**, where **each reasoning step builds upon the previous one** to solve complex tasks.\n",
    "\n",
    "**Example 1:**\n",
    "\n",
    "> **Question:** “Where was the author of *Pride and Prejudice* born?”\n",
    "\n",
    "**Reasoning path:**\n",
    "\n",
    "1. Identify the author of *Pride and Prejudice* → Jane Austen.\n",
    "2. Retrieve the birthplace of Jane Austen → Steventon, Hampshire, England.\n",
    "3. **Final Answer:** Steventon, Hampshire, England.\n",
    "\n",
    "Here, the model needs to **combine two separate facts** — one about the author and another about her birthplace — instead of relying on a single fact lookup.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Multi-Hop Reasoning Matters**\n",
    "\n",
    "Most real-world tasks require **chained logic** or reasoning beyond one-hop retrieval.\n",
    "For example:\n",
    "\n",
    "* Legal analysis: linking cases and laws.\n",
    "* Scientific reasoning: combining experimental results with theories.\n",
    "* RAG systems: synthesizing multi-document knowledge.\n",
    "* Conversational AI: maintaining long context and logical continuity.\n",
    "\n",
    "---\n",
    "\n",
    "**How Multi-Hop Reasoning Works**\n",
    "\n",
    "There are **two primary mechanisms** by which LLMs perform multi-step reasoning:\n",
    "\n",
    "**A. Implicit Reasoning (Internal Reasoning)**\n",
    "\n",
    "The model performs reasoning *within its internal attention and hidden states*, using its trained parameters to simulate reasoning.\n",
    "\n",
    "**Mechanism:**\n",
    "\n",
    "* The model encodes the question and relevant context.\n",
    "* During decoding, it implicitly models dependencies across concepts.\n",
    "* It “jumps” across related information within its attention mechanism.\n",
    "\n",
    "**Limitation:** This is opaque (“black-box”) and hard to control or verify.\n",
    "\n",
    "---\n",
    "\n",
    "**B. Explicit Reasoning (Externalized or Step-by-Step Reasoning)**\n",
    "\n",
    "The model is guided to **show its reasoning process explicitly**, usually through **prompt engineering**, **chain-of-thought (CoT)**, or **tool-assisted reasoning**.\n",
    "\n",
    "**Techniques Include:**\n",
    "\n",
    "| Method                               | Description                                                                      | Example                         |\n",
    "| ------------------------------------ | -------------------------------------------------------------------------------- | ------------------------------- |\n",
    "| **Chain-of-Thought (CoT) Prompting** | Ask the model to \"think step by step\"                                            | “Let’s think step-by-step: …”   |\n",
    "| **Self-Consistency**                 | Generate multiple reasoning paths and choose the majority result                 | Reduces random reasoning errors |\n",
    "| **Tool-Augmented Reasoning**         | Use external tools (like calculators, retrievers, or Python) to assist reasoning | LangChain Agents calling APIs   |\n",
    "| **Decomposition-based Reasoning**    | Breaks a query into sub-questions and solves each                                | “Who → Where → What” structure  |\n",
    "| **Tree of Thoughts (ToT)**           | Explore multiple reasoning branches before deciding                              | Simulates search tree reasoning |\n",
    "\n",
    "---\n",
    "\n",
    "**Multi-Hop Reasoning in RAG Systems**\n",
    "\n",
    "When integrated into **Retrieval-Augmented Generation (RAG)**, multi-hop reasoning allows LLMs to:\n",
    "\n",
    "1. Retrieve multiple documents across hops (hop-1 → hop-2 → hop-n).\n",
    "2. Extract relevant parts from each document.\n",
    "3. Chain them together logically to form a coherent answer.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> **Question:** “Which company acquired the startup founded by the creator of Instagram?”\n",
    "\n",
    "**Reasoning Path:**\n",
    "\n",
    "1. Who founded Instagram? → Kevin Systrom, Mike Krieger.\n",
    "2. What startup did they found later? → Artifact.\n",
    "3. Which company acquired Artifact? → Yahoo.\n",
    "4. **Answer:** Yahoo acquired Artifact, the startup founded by the creators of Instagram.\n",
    "\n",
    "This process requires **multi-document retrieval** and **sequential logic chaining**.\n",
    "\n",
    "---\n",
    "\n",
    "**Architectural Mechanisms Behind Multi-Hop Reasoning**\n",
    "\n",
    "| Component                               | Role                                                                          |\n",
    "| --------------------------------------- | ----------------------------------------------------------------------------- |\n",
    "| **Attention Mechanisms**                | Allow the model to connect tokens or concepts across long sequences.          |\n",
    "| **Transformers Layers**                 | Learn relationships across distant concepts.                                  |\n",
    "| **Memory Modules (External)**           | Store and retrieve intermediate reasoning states.                             |\n",
    "| **Tool Integration (LangChain, ReAct)** | Externalizes reasoning by invoking search, calculation, or APIs for each hop. |\n",
    "| **Reasoning Loops / LCEL**              | Enable iterative “think → act → observe” loops until convergence.             |\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Multi-Hop Reasoning Chain with LangChain**\n",
    "\n",
    "```python\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Step 1: Find who founded Instagram\n",
    "prompt1 = PromptTemplate(input_variables=[\"question\"], \n",
    "                         template=\"Who founded Instagram?\")\n",
    "chain1 = LLMChain(llm=ChatOpenAI(model=\"gpt-4o-mini\"), prompt=prompt1, output_key=\"founders\")\n",
    "\n",
    "# Step 2: Find which startup they later founded\n",
    "prompt2 = PromptTemplate(input_variables=[\"founders\"], \n",
    "                         template=\"Which startup was founded later by {founders}?\")\n",
    "chain2 = LLMChain(llm=ChatOpenAI(model=\"gpt-4o-mini\"), prompt=prompt2, output_key=\"startup\")\n",
    "\n",
    "# Step 3: Find which company acquired that startup\n",
    "prompt3 = PromptTemplate(input_variables=[\"startup\"], \n",
    "                         template=\"Which company acquired {startup}?\")\n",
    "chain3 = LLMChain(llm=ChatOpenAI(model=\"gpt-4o-mini\"), prompt=prompt3, output_key=\"acquirer\")\n",
    "\n",
    "# Sequential multi-hop reasoning chain\n",
    "multi_hop_chain = SequentialChain(chains=[chain1, chain2, chain3],\n",
    "                                  input_variables=[\"question\"],\n",
    "                                  output_variables=[\"acquirer\"])\n",
    "\n",
    "response = multi_hop_chain.invoke({\"question\": \"Which company acquired the startup founded by the creator of Instagram?\"})\n",
    "print(response)\n",
    "```\n",
    "\n",
    "This chain decomposes reasoning into **3 logical hops**, each producing an intermediate result.\n",
    "\n",
    "---\n",
    "\n",
    "**Strengths of Multi-Hop Reasoning**\n",
    "\n",
    "- **Improved Depth of Understanding:** Enables LLMs to handle complex, compositional questions.\n",
    "- **Better Interpretability:** Step-by-step reasoning can be inspected and validated.\n",
    "- **Generalizable:** Works for text, tables, code, or multi-modal reasoning.\n",
    "- **Supports RAG Pipelines:** Multi-hop retrieval allows deeper document linking.\n",
    "\n",
    "---\n",
    "\n",
    "**Limitations and Challenges**\n",
    "\n",
    "| Limitation                             | Description                                                                   |\n",
    "| -------------------------------------- | ----------------------------------------------------------------------------- |\n",
    "| **Hallucination Propagation**          | Errors early in reasoning can propagate across steps, compounding mistakes.   |\n",
    "| **Context Window Limitations**         | LLMs may lose context in long reasoning chains.                               |\n",
    "| **Lack of True Logical Understanding** | Models simulate reasoning but don’t “understand” logic in a symbolic sense.   |\n",
    "| **Computation Overhead**               | Multi-hop reasoning requires more tokens, leading to higher latency and cost. |\n",
    "| **Non-Deterministic Outputs**          | Same query may yield different reasoning paths across runs.                   |\n",
    "| **Difficulty in Evaluation**           | Hard to measure correctness of intermediate reasoning steps.                  |\n",
    "\n",
    "---\n",
    "\n",
    "**Techniques to Improve Multi-Hop Reasoning**\n",
    "\n",
    "| Technique                       | Description                                                        | Example                                |\n",
    "| ------------------------------- | ------------------------------------------------------------------ | -------------------------------------- |\n",
    "| **Chain-of-Thought Prompting**  | Encourage explicit reasoning steps                                 | “Let’s think step by step.”            |\n",
    "| **Self-Reflection (Reflexion)** | LLM re-evaluates and corrects its previous reasoning               | Used in agentic frameworks             |\n",
    "| **ReAct Framework**             | Combines reasoning and acting (uses tools during reasoning)        | Think → Search → Answer                |\n",
    "| **Memory-Augmented RAG**        | Keeps track of reasoning steps for long queries                    | Helps multi-document reasoning         |\n",
    "| **Tree-of-Thought (ToT)**       | Explores multiple reasoning paths and prunes bad ones              | Used for mathematical or logical tasks |\n",
    "| **Iterative LCEL Pipelines**    | LangChain Expression Language enables dynamic multi-step execution | `RunnableSequence`, `RunnableBranch`   |\n",
    "\n",
    "---\n",
    "\n",
    "**Problems in Current LLM Multi-Hop Reasoning**\n",
    "\n",
    "* **Brittleness**: Minor rephrasing of a question may break reasoning chains.\n",
    "* **Overfitting to Patterns**: Models follow memorized reasoning templates instead of generalizing.\n",
    "* **Opaque Intermediate Steps**: Implicit reasoning steps are hard to trace.\n",
    "* **Error Amplification**: Misstep at one hop corrupts all later reasoning.\n",
    "* **Tool Misuse**: In ReAct-style agents, LLMs sometimes call irrelevant tools or repeat actions.\n",
    "\n",
    "---\n",
    "\n",
    "**Future Directions and Research Trends**\n",
    "\n",
    "1. **Neural-Symbolic Reasoning:**\n",
    "Combining symbolic logic systems with neural LLMs for verifiable reasoning.\n",
    "\n",
    "2. **Memory-Augmented Transformers:**\n",
    "Integrating persistent long-term memory modules for multi-hop consistency.\n",
    "\n",
    "3. **Dynamic RAG Pipelines:**\n",
    "Adaptive retrievers that perform iterative multi-hop document linking.\n",
    "\n",
    "4. **Tree-Based Reasoning Models:**\n",
    "Tree of Thoughts (ToT) and Graph of Thoughts (GoT) architectures for exploring branching reasoning paths.\n",
    "\n",
    "5. **Self-Verification Loops:**\n",
    "LLMs validating their own intermediate results using secondary models.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Aspect              | Description                                                |\n",
    "| ------------------- | ---------------------------------------------------------- |\n",
    "| **Definition**      | Connecting multiple reasoning steps to reach an answer     |\n",
    "| **Goal**            | Combine different pieces of knowledge logically            |\n",
    "| **Mechanisms**      | Attention, CoT, ToT, external tools                        |\n",
    "| **Advantages**      | Deeper understanding, better retrieval, improved RAG       |\n",
    "| **Limitations**     | Hallucinations, context loss, cost                         |\n",
    "| **Best Frameworks** | LangChain, ReAct, LCEL, Tree-of-Thoughts                   |\n",
    "| **Use Cases**       | Question answering, scientific discovery, reasoning agents |\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "* Multi-hop reasoning enables **deep, compositional logic** in LLMs.\n",
    "* It’s central to **advanced RAG**, **agentic reasoning**, and **knowledge synthesis**.\n",
    "* Best implemented via **structured prompting**, **tool integration**, and **iterative reasoning loops**.\n",
    "* Still limited by hallucinations and lack of explainability — ongoing research aims to bridge the gap between **neural reasoning** and **true logical inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309ba2f",
   "metadata": {},
   "source": [
    "## **Query Decomposition in RAG Systems**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744ba63",
   "metadata": {},
   "source": [
    "**What is Query Decomposition?**\n",
    "\n",
    "**Query Decomposition** is the process of **breaking down a complex question into simpler, smaller sub-queries**, so that each sub-query can be answered (or retrieved) independently — and then **combined logically** to form a final, coherent answer.\n",
    "\n",
    "It’s a **core technique in multi-hop reasoning RAGs**, used when a single retrieval operation cannot fetch all relevant information.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Query Decomposition Is Needed in RAG**\n",
    "\n",
    "Traditional RAG pipelines perform:\n",
    "\n",
    "1. A single query to a retriever (e.g., vector database like Pinecone/FAISS)\n",
    "2. Then send retrieved chunks to the LLM for synthesis.\n",
    "\n",
    "This **fails when:**\n",
    "\n",
    "* The question requires *multiple reasoning hops.*\n",
    "* Needed facts are *spread across different documents or sources.*\n",
    "* The retriever retrieves semantically similar but *incomplete* information.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> ❓“Which startup founded by the creators of Instagram was later acquired by Yahoo?”\n",
    "\n",
    "→ Single retrieval may fail because:\n",
    "\n",
    "* “Instagram creators” and “Yahoo acquisition” occur in *different documents.*\n",
    "\n",
    "---\n",
    "\n",
    "**What Query Decomposition Solves**\n",
    "\n",
    "| Problem                    | Solution via Query Decomposition             |\n",
    "| -------------------------- | -------------------------------------------- |\n",
    "| Complex, multi-hop queries | Break into smaller, focused sub-queries      |\n",
    "| Scattered facts            | Retrieve relevant context for each sub-part  |\n",
    "| Retrieval inefficiency     | Perform multiple, precise searches           |\n",
    "| LLM confusion              | Simplify reasoning chain for better accuracy |\n",
    "\n",
    "---\n",
    "\n",
    "**How Query Decomposition Works**\n",
    "\n",
    "**Step-by-Step Process**\n",
    "\n",
    "1. **Input Query:**\n",
    "   A complex or multi-hop natural language question.\n",
    "\n",
    "2. **Decomposition Phase (LLM-based):**\n",
    "   The question is *decomposed* into multiple, smaller questions — often using an LLM (like GPT-4) or a custom prompt.\n",
    "\n",
    "3. **Retrieval Phase:**\n",
    "   Each sub-question is sent to a retriever (e.g., Pinecone, ChromaDB, Elasticsearch) to fetch relevant context.\n",
    "\n",
    "4. **Synthesis Phase:**\n",
    "   Retrieved contexts are **merged or chained** and fed into the generator (LLM) to compose the final answer.\n",
    "\n",
    "---\n",
    "\n",
    "**Example: Step-by-Step**\n",
    "\n",
    "**Input Question:**\n",
    "\n",
    "> “Which company acquired the startup founded by the creator of Instagram?”\n",
    "\n",
    "**Decomposition:**\n",
    "\n",
    "1. Who founded Instagram?\n",
    "2. What startup did that person later found?\n",
    "3. Which company acquired that startup?\n",
    "\n",
    "**Retrieval:**\n",
    "\n",
    "Each sub-question retrieves its own context:\n",
    "\n",
    "* Q1 → [Kevin Systrom, Mike Krieger]\n",
    "* Q2 → [Artifact startup]\n",
    "* Q3 → [Yahoo acquisition]\n",
    "\n",
    "**Synthesis:**\n",
    "\n",
    "LLM combines results:\n",
    "\n",
    "> “Yahoo acquired Artifact, a startup founded by Instagram’s creators Kevin Systrom and Mike Krieger.”\n",
    "\n",
    "---\n",
    "\n",
    "**Decomposition Techniques**\n",
    "\n",
    "| Technique                              | Description                                                  | Example / Implementation                                       |\n",
    "| -------------------------------------- | ------------------------------------------------------------ | -------------------------------------------------------------- |\n",
    "| **Rule-based**                         | Predefined templates split compound questions                | Detect conjunctions (“and”, “after”, “who”)                    |\n",
    "| **LLM-based Decomposition**            | Use an LLM to rewrite a query into sub-queries               | `Prompt: “Decompose this question into smaller sub-questions”` |\n",
    "| **Dependency Parsing**                 | Use NLP parse trees to detect relationships                  | “Who founded X?” → “Who acquired Y?”                           |\n",
    "| **Semantic Graph Decomposition**       | Convert query into graph nodes and edges                     | Uses knowledge graphs                                          |\n",
    "| **Iterative Decomposition (Self-Ask)** | LLM answers one sub-question, generates the next dynamically | Used in “Self-Ask with Search” approach                        |\n",
    "\n",
    "---\n",
    "\n",
    "**Example Implementation in LangChain**\n",
    "\n",
    "Here’s how to perform **LLM-driven Query Decomposition** in a RAG pipeline:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Step 1: LLM to decompose query\n",
    "decomposition_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Decompose the following question into smaller, logically connected sub-questions:\\n\\n{question}\"\n",
    ")\n",
    "\n",
    "decomposition_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    prompt=decomposition_prompt,\n",
    "    output_key=\"sub_questions\"\n",
    ")\n",
    "\n",
    "# Step 2: Retrieve for each sub-question (pseudo example)\n",
    "def retrieve_for_sub_questions(sub_questions, retriever):\n",
    "    docs = []\n",
    "    for q in sub_questions.split(\"\\n\"):\n",
    "        docs.extend(retriever.get_relevant_documents(q))\n",
    "    return docs\n",
    "\n",
    "# Step 3: Synthesize final answer\n",
    "synthesis_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"Using the context below, answer the question:\\nQuestion: {question}\\n\\nContext: {context}\"\n",
    ")\n",
    "\n",
    "synthesis_chain = LLMChain(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    prompt=synthesis_prompt,\n",
    "    output_key=\"final_answer\"\n",
    ")\n",
    "```\n",
    "\n",
    "This **LCEL-style pipeline** can be fully modularized with `RunnableSequence` or `RunnableMap` for parallel retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "**Query Decomposition in LCEL (LangChain Expression Language)**\n",
    "\n",
    "LCEL allows creating structured workflows like:\n",
    "\n",
    "```python\n",
    "from langchain.schema.runnable import RunnableSequence, RunnableMap\n",
    "\n",
    "query_decomposition_rag = RunnableSequence([\n",
    "    decomposition_chain,\n",
    "    RunnableMap({\n",
    "        \"context\": lambda x: retrieve_for_sub_questions(x[\"sub_questions\"], retriever),\n",
    "        \"question\": lambda x: x[\"question\"]\n",
    "    }),\n",
    "    synthesis_chain\n",
    "])\n",
    "```\n",
    "\n",
    "This lets you **decompose, retrieve, and synthesize** in a single expression graph.\n",
    "\n",
    "---\n",
    "\n",
    "**Integration with Hybrid Retrieval**\n",
    "\n",
    "Query decomposition can work alongside **hybrid retrieval** (dense + sparse retrievers):\n",
    "\n",
    "* Sub-query 1 → Vector DB (semantic context)\n",
    "* Sub-query 2 → ElasticSearch/BM25 (keyword facts)\n",
    "* Combine results → Final synthesis\n",
    "\n",
    "This improves **recall** (fetching all relevant data) and **precision** (focusing on relevance).\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "| Benefit                                   | Explanation                                            |\n",
    "| ----------------------------------------- | ------------------------------------------------------ |\n",
    "| **Handles Multi-Hop Reasoning**        | Enables reasoning over multi-document knowledge        |\n",
    "| **Improves Retrieval Accuracy**        | Smaller, targeted sub-queries lead to higher precision |\n",
    "| **Simplifies LLM Processing**          | Each hop is easier for the model to understand         |\n",
    "| **Reduces Hallucination**              | Facts are retrieved explicitly, not guessed            |\n",
    "| **Scalable for Large Knowledge Bases** | Works across multiple retrievers or databases          |\n",
    "\n",
    "---\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "| Limitation                     | Description                                                  |\n",
    "| ------------------------------ | ------------------------------------------------------------ |\n",
    "| **Error Propagation**          | Mistake in one sub-query can affect the entire chain         |\n",
    "| **Increased Latency**          | Multiple retrievals = higher time/cost                       |\n",
    "| **Ambiguous Decomposition**    | LLM may generate incorrect or overlapping sub-queries        |\n",
    "| **Context Merging Complexity** | Combining retrieved chunks can cause redundancy or conflicts |\n",
    "\n",
    "---\n",
    "\n",
    "**Best Practices**\n",
    "\n",
    "- **Prompt-Driven Decomposition:**\n",
    "Guide the LLM explicitly — e.g., “Break this into minimal, logically dependent sub-questions.”\n",
    "\n",
    "- **Limit Sub-Query Count:**\n",
    "Keep decomposition to 3–5 hops to control cost and drift.\n",
    "\n",
    "- **Filter Duplicates:**\n",
    "Avoid retrieving overlapping content.\n",
    "\n",
    "- **Score and Merge Results:**\n",
    "Use reranking (BM25 + cosine similarity) before synthesis.\n",
    "\n",
    "- **Add Memory Tracking:**\n",
    "Store intermediate answers in memory to support iterative reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "**Real-World Use Cases**\n",
    "\n",
    "| Domain                 | Application                                                                 |\n",
    "| ---------------------- | --------------------------------------------------------------------------- |\n",
    "| **Legal AI**           | Breaking long legal questions into smaller precedent lookups                |\n",
    "| **Finance**            | Decomposing investment analysis queries into company and market sub-queries |\n",
    "| **Biomedical RAG**     | Multi-step retrieval across research papers                                 |\n",
    "| **Customer Support**   | Chaining user issues → product → solution database queries                  |\n",
    "| **Knowledge Graph QA** | Mapping sub-queries to nodes/edges in KG                                    |\n",
    "\n",
    "---\n",
    "\n",
    "**Query Decomposition + Multi-Hop Reasoning (Synergy)**\n",
    "\n",
    "These two concepts often **coexist** in advanced RAG systems:\n",
    "\n",
    "| Stage               | Operation                                  |\n",
    "| ------------------- | ------------------------------------------ |\n",
    "| Decomposition       | Break complex question → Sub-questions     |\n",
    "| Multi-hop Reasoning | Connect sub-answers → Logical chain        |\n",
    "| Final Generation    | Compose final, contextually aware response |\n",
    "\n",
    "Together, they enable **deep retrieval + structured reasoning** → the foundation of **next-gen agentic RAGs**.\n",
    "\n",
    "---\n",
    "\n",
    "**In Summary**\n",
    "\n",
    "| Aspect             | Description                                                     |\n",
    "| ------------------ | --------------------------------------------------------------- |\n",
    "| **Definition**     | Splitting a complex question into smaller, simpler sub-queries  |\n",
    "| **Goal**           | Enable accurate retrieval and synthesis for multi-hop reasoning |\n",
    "| **Key Techniques** | LLM-based decomposition, self-ask, LCEL chains                  |\n",
    "| **Benefits**       | Better accuracy, less hallucination, improved retrieval recall  |\n",
    "| **Challenges**     | Latency, error propagation, merging results                     |\n",
    "| **Used In**        | Advanced RAG, agent frameworks, multi-hop QA                    |\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "\n",
    "> *Query decomposition is the “divide and conquer” strategy of RAG systems — breaking down complex user questions into smaller retrieval problems, each solvable with precision, and then recomposed through multi-hop reasoning for a complete, reliable answer.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052735a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
