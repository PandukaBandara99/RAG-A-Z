{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTayvli5n5oi"
      },
      "source": [
        "# Pinecone Vector DB\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VnRuXb5Pn4op"
      },
      "outputs": [],
      "source": [
        "## Create your index and apikey from here https://www.pinecone.io/\n",
        "api_key=\"provide your apikey\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4qBMk_ooF7e",
        "outputId": "1640a029-0a6c-41fd-b41b-2c38785719af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/587.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m583.7/587.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/240.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.1/221.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-pinecone langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xk3bV02uoNJG"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "pc=Pinecone(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6FkmtNbdoNEN"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",dimensions=1024,api_key=\"openai_api_key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pUEG87c5oNBV"
      },
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name = \"rag\"  # change if desired\n",
        "\n",
        "if not pc.has_index(index_name):\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1024,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG6u6hNVoM-Q",
        "outputId": "349a377d-bf69-4e8f-a05e-a37f6ed888b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pinecone.db_data.index.Index at 0x7b6da080f390>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Roi5exMmoM1H"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5g2vYCJo1rR",
        "outputId": "86b507f6-62ff-4786-c114-25efc0352c45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7b6da07ca8d0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0xg_Yi8Fo2p2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7vIFd9vo7RR",
        "outputId": "6dcf8c54-6bc5-4782-8c6f-87d64aa35d2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['319fd410-cf6f-4e54-a1ad-56c3573943e9',\n",
              " 'da12c62d-6b7e-45dc-834a-c0e7c145f606',\n",
              " 'c4e76fea-0f15-4404-a8db-f83462bb7553',\n",
              " '91f15ee8-9617-4c16-8fa3-65573c97093f',\n",
              " 'c2b0f737-3ade-4d32-ae95-0656782cac6f',\n",
              " '05e6dcfc-6e9f-45c6-9147-421ce1d7a7ef',\n",
              " 'd542df32-3a84-4fe6-95c6-5ad00fca866f',\n",
              " '5108e39c-5d2b-4675-875a-adeed65518e2',\n",
              " 'ab3af2be-29ed-451f-87d1-8b18ad388e7c',\n",
              " '75f3a6bf-a79f-4a60-a976-ecc14e1db909']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.add_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SymzuvqVo-_2",
        "outputId": "0cc4a6af-648b-429d-c715-f9f6f57d5b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n"
          ]
        }
      ],
      "source": [
        "### Query Directly\n",
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=2,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-fY5h4JpDN-",
        "outputId": "807d8c23-0e46-49a5-aef8-6f262eda395e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* [SIM=0.572665] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
          ]
        }
      ],
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkcjANiwpFw5",
        "outputId": "4febdd72-e32f-465c-8642-bfcec41db18a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='91f15ee8-9617-4c16-8fa3-65573c97093f', metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.')]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Retriever\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 1, \"score_threshold\": 0.4},\n",
        ")\n",
        "retriever.invoke(\"Stealing from the bank is a crime\", filter={\"source\": \"news\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Introduction and Fundamentals**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "**Introduction to Pinecone**\n",
        "\n",
        "---\n",
        "\n",
        "**What is Pinecone?**\n",
        "Pinecone is a **fully managed vector database** designed for **similarity search, recommendation engines, semantic search, and retrieval-augmented generation (RAG) pipelines**. It enables developers to **store, index, and query high-dimensional vector embeddings** efficiently without managing the underlying infrastructure.\n",
        "\n",
        "**Key Characteristics:**\n",
        "\n",
        "* Cloud-native and **fully managed**\n",
        "* Scalable to **billions of vectors**\n",
        "* Provides **real-time vector search** with low latency\n",
        "* Integrates easily with **machine learning pipelines** and **LLMs**\n",
        "\n",
        "---\n",
        "\n",
        "**History and Evolution**\n",
        "\n",
        "* Founded to address challenges in **scalable vector search** and **semantic retrieval**.\n",
        "* Early vector search systems required **manual infrastructure management**, which Pinecone abstracts.\n",
        "* Over time, Pinecone has evolved to support:\n",
        "\n",
        "  * **Hybrid search** (vector + metadata/keyword filtering)\n",
        "  * **Multi-tenancy and high availability**\n",
        "  * **Distributed, low-latency search for enterprise-scale datasets**\n",
        "\n",
        "---\n",
        "\n",
        "**Key Features and Use Cases**\n",
        "\n",
        "| Feature                 | Description                                                 | Use Case                                                |\n",
        "| ----------------------- | ----------------------------------------------------------- | ------------------------------------------------------- |\n",
        "| **Managed Service**     | Fully hosted, no need to manage servers                     | Quick deployment of vector search applications          |\n",
        "| **High Scalability**    | Handles billions of vectors with distributed infrastructure | Enterprise search and recommendation systems            |\n",
        "| **Low Latency Queries** | Optimized ANN algorithms for real-time retrieval            | Chatbots, RAG pipelines                                 |\n",
        "| **Hybrid Search**       | Combine vector similarity with metadata filters             | E-commerce product search, personalized recommendations |\n",
        "| **Automatic Indexing**  | Supports HNSW, IVFPQ, and other ANN algorithms internally   | Efficient semantic search                               |\n",
        "| **Multi-Tenancy**       | Isolate workloads for different projects                    | SaaS applications, multi-client setups                  |\n",
        "| **Integrations**        | Works with LangChain, LLMs, ML frameworks                   | RAG systems, semantic search apps                       |\n",
        "\n",
        "**Example Applications:**\n",
        "\n",
        "* **Semantic Search**: Retrieve documents, articles, or FAQs based on meaning rather than exact keywords.\n",
        "* **Recommendation Systems**: Suggest content or products based on embedding similarity.\n",
        "* **RAG Pipelines**: Provide LLMs with context by fetching relevant embeddings from a Pinecone index.\n",
        "* **Image or Multi-Modal Search**: Search images using embeddings generated from CNNs or CLIP.\n",
        "\n",
        "---\n",
        "\n",
        "**Comparison with FAISS, Milvus, Weaviate**\n",
        "\n",
        "| Feature             | Pinecone                             | FAISS                            | Milvus                          | Weaviate                   |\n",
        "| ------------------- | ------------------------------------ | -------------------------------- | ------------------------------- | -------------------------- |\n",
        "| **Managed Service** | ✅ Fully managed                      | ❌ Self-hosted required           | ✅ Managed (Milvus Cloud)        | ✅ Managed                  |\n",
        "| **Scaling**         | Auto-scaling for billions of vectors | Manual scaling                   | Sharding & distributed clusters | Distributed & cloud-native |\n",
        "| **Persistence**     | Built-in                             | Needs external storage           | Built-in                        | Built-in                   |\n",
        "| **Query Latency**   | Low (real-time)                      | Very low (in-memory, high speed) | Low                             | Low                        |\n",
        "| **Hybrid Search**   | ✅ Yes (vector + metadata)            | ❌ Only vector                    | ✅ Yes                           | ✅ Yes                      |\n",
        "| **Ease of Use**     | High (API-first)                     | Moderate (requires setup)        | Moderate                        | High                       |\n",
        "| **Integrations**    | LangChain, LLMs, ML pipelines        | LLMs, Python ecosystem           | Python, Java, Go                | LLMs, GraphQL, REST        |\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "* **FAISS:** Extremely fast in-memory searches, suitable for **local, high-speed experiments**, but requires **manual scaling and persistence management**.\n",
        "* **Milvus:** Open-source, supports large-scale distributed vector search; can be self-hosted or cloud-managed.\n",
        "* **Weaviate:** Cloud-native, graph-based search, with strong **semantic and hybrid search support**.\n",
        "* **Pinecone:** Fully managed, scalable, and easy to integrate with **modern ML and LLM pipelines**, ideal for **production-ready vector search applications**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Core Concepts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Vectors and Embeddings**\n",
        "\n",
        "* **Vectors** are numerical representations of data, typically stored as **arrays of floats**, used to capture **semantic meaning**.\n",
        "* **Embeddings** are specialized vectors generated from models like **BERT, OpenAI, CLIP, or custom neural networks**.\n",
        "* Applications include:\n",
        "\n",
        "  * Text search: Represent sentences or documents as vectors.\n",
        "  * Image search: Represent images using CNN or CLIP embeddings.\n",
        "  * Recommendation systems: Represent users/items for similarity matching.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "# Text embeddings using OpenAI\n",
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "embedding = client.embeddings.create(input=\"Hello world\", model=\"text-embedding-3-large\")\n",
        "vector = embedding.data[0].embedding  # 1536-dimensional vector\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Similarity Metrics (Cosine, Euclidean, Dot Product)**\n",
        "\n",
        "| Metric                          | Description                                            | Use Cases                                    |\n",
        "| ------------------------------- | ------------------------------------------------------ | -------------------------------------------- |\n",
        "| **Cosine Similarity**           | Measures angle between two vectors (ignores magnitude) | Text embeddings, semantic similarity         |\n",
        "| **Euclidean Distance (L2)**     | Straight-line distance between vectors                 | Image embeddings, general-purpose similarity |\n",
        "| **Dot Product (Inner Product)** | Measures similarity by vector magnitude and direction  | Recommendation engines, neural embeddings    |\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "* Cosine similarity is commonly used in NLP because it focuses on **direction rather than magnitude**.\n",
        "* Dot product can be used with normalized vectors to achieve **cosine-equivalent similarity**.\n",
        "\n",
        "---\n",
        "\n",
        "**Namespaces and Collections**\n",
        "\n",
        "* **Namespace:** Logical separation of vectors within a Pinecone project. Think of it as **a virtual database** inside your index.\n",
        "* **Collection:** Group of vectors within a namespace. Collections allow **organizing embeddings by type, source, or use case**.\n",
        "* **Benefits:**\n",
        "\n",
        "  * Multi-tenancy support\n",
        "  * Avoid collisions between different applications\n",
        "  * Easier management of metadata and queries\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "import pinecone\n",
        "pinecone.init(api_key=\"YOUR_API_KEY\", environment=\"us-west1-gcp\")\n",
        "index = pinecone.Index(\"example-index\")\n",
        "\n",
        "# Upsert vectors into a specific namespace\n",
        "vectors = [(f\"id-{i}\", vector.tolist()) for i, vector in enumerate(vector_list)]\n",
        "index.upsert(vectors=vectors, namespace=\"semantic-search\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Index Types and Configurations**\n",
        "\n",
        "* Pinecone automatically handles **approximate nearest neighbor (ANN) indexing**, but you can configure for **performance, memory, and accuracy trade-offs**.\n",
        "\n",
        "**Key Index Types:**\n",
        "\n",
        "| Type                      | Description                                             | Best Use Case                                |\n",
        "| ------------------------- | ------------------------------------------------------- | -------------------------------------------- |\n",
        "| **Standard**              | Default ANN index optimized for latency and throughput  | Most general-purpose vector searches         |\n",
        "| **Sparse**                | Optimized for large datasets with lower density vectors | Sparse embeddings like bag-of-words vectors  |\n",
        "| **Hybrid / Multi-Vector** | Supports multiple vector types in a single collection   | Multi-modal search (text + image embeddings) |\n",
        "\n",
        "**Configuration Parameters:**\n",
        "\n",
        "* **Metric:** Choose similarity metric (`cosine`, `euclidean`, `dotproduct`)\n",
        "* **Replicas:** Number of replicas for **redundancy and high availability**\n",
        "* **Shards:** Partition index for **scaling to billions of vectors**\n",
        "* **Metadata Indexing:** Enables filtering by key-value metadata\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "pinecone.create_index(\n",
        "    name=\"example-index\",\n",
        "    dimension=1536,\n",
        "    metric=\"cosine\",\n",
        "    replicas=2,\n",
        "    shards=1\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Getting Started with Pinecone**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Signing Up and API Keys**\n",
        "\n",
        "1. **Sign Up**\n",
        "\n",
        "   * Go to the [Pinecone website](https://www.pinecone.io/) and create an account.\n",
        "   * You can start with a **free tier** to explore basic features.\n",
        "\n",
        "2. **Obtain API Key**\n",
        "\n",
        "   * Navigate to the **API Keys** section in your Pinecone dashboard.\n",
        "   * Generate a new key and **store it securely**, as it will be used to authenticate all requests.\n",
        "   * Example environment variable setup:\n",
        "\n",
        "   ```bash\n",
        "   export PINECONE_API_KEY=\"your_api_key_here\"\n",
        "   export PINECONE_ENVIRONMENT=\"us-west1-gcp\"  # Your chosen region\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "**Installing Pinecone Client (Python SDK)**\n",
        "\n",
        "1. Install via pip:\n",
        "\n",
        "```bash\n",
        "pip install pinecone-client\n",
        "```\n",
        "\n",
        "2. Verify installation and import in Python:\n",
        "\n",
        "```python\n",
        "import pinecone\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=\"your_api_key_here\",\n",
        "    environment=\"us-west1-gcp\"\n",
        ")\n",
        "print(\"Pinecone initialized successfully!\")\n",
        "```\n",
        "\n",
        "3. Optional: Set environment variables to avoid hardcoding your API key:\n",
        "\n",
        "```python\n",
        "import os\n",
        "import pinecone\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"your_api_key_here\"\n",
        "os.environ[\"PINECONE_ENVIRONMENT\"] = \"us-west1-gcp\"\n",
        "\n",
        "pinecone.init(api_key=os.environ[\"PINECONE_API_KEY\"], environment=os.environ[\"PINECONE_ENVIRONMENT\"])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Basic Index Creation**\n",
        "Creating an index is the first step to store and query vectors.\n",
        "\n",
        "1. **Define Index Parameters:**\n",
        "\n",
        "* **Name:** Unique identifier for your index.\n",
        "* **Dimension:** Size of the vectors you plan to store.\n",
        "* **Metric:** Similarity metric (`cosine`, `euclidean`, `dotproduct`).\n",
        "\n",
        "```python\n",
        "# Create a simple Pinecone index\n",
        "pinecone.create_index(\n",
        "    name=\"example-index\",\n",
        "    dimension=1536,         # dimension of your vectors\n",
        "    metric=\"cosine\",        # similarity metric\n",
        "    replicas=1,             # number of replicas for redundancy\n",
        "    shards=1                # number of shards for partitioning\n",
        ")\n",
        "```\n",
        "\n",
        "2. **Connect to the Index:**\n",
        "\n",
        "```python\n",
        "index = pinecone.Index(\"example-index\")\n",
        "print(\"Index connected successfully!\")\n",
        "```\n",
        "\n",
        "3. **Upsert Vectors (Optional Test):**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Sample vector data\n",
        "vectors = [(f\"id-{i}\", np.random.rand(1536).tolist()) for i in range(5)]\n",
        "\n",
        "# Upsert into Pinecone index\n",
        "index.upsert(vectors=vectors)\n",
        "print(\"Vectors added to index!\")\n",
        "```\n",
        "\n",
        "4. **Query the Index:**\n",
        "\n",
        "```python\n",
        "query_vector = np.random.rand(1536).tolist()\n",
        "results = index.query(vector=query_vector, top_k=3)\n",
        "print(results)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Key Takeaways**\n",
        "\n",
        "* Pinecone requires **API key authentication** and region selection.\n",
        "* The Python SDK allows you to **create, connect, upsert, and query indexes** with minimal setup.\n",
        "* Starting with a small index and sample vectors is recommended before scaling to **production datasets**.\n",
        "\n",
        "This workflow provides a foundation for building **semantic search, RAG pipelines, and recommendation systems** using Pinecone.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Creating and Managing Indexes in Pinecone**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Index Types**\n",
        "Pinecone offers multiple index types to optimize for **speed, memory, and functionality**:\n",
        "\n",
        "| Index Type           | Description                                                          | Best Use Case                                     |\n",
        "| -------------------- | -------------------------------------------------------------------- | ------------------------------------------------- |\n",
        "| **Standard**         | Default index, balanced for speed and accuracy                       | Most general-purpose vector search tasks          |\n",
        "| **Memory-Optimized** | Uses less RAM per vector, optimized for large-scale datasets         | Large datasets where memory efficiency is crucial |\n",
        "| **Hybrid**           | Supports multiple vector types and hybrid search (vector + metadata) | Multi-modal search, e.g., text + image embeddings |\n",
        "\n",
        "**Key Considerations:**\n",
        "\n",
        "* Standard indexes provide **fast ANN searches** for small to medium datasets.\n",
        "* Memory-optimized indexes can handle **millions of vectors** in a limited memory footprint.\n",
        "* Hybrid indexes allow **filtering with metadata alongside vector similarity**, ideal for **personalized recommendations or multi-attribute search**.\n",
        "\n",
        "---\n",
        "\n",
        "**Dimension and Metric Selection**\n",
        "\n",
        "1. **Dimension**\n",
        "\n",
        "* The **dimension** of your index must match the size of your embedding vectors.\n",
        "* Common embeddings:\n",
        "\n",
        "  * OpenAI text-embedding-3-large → 1536 dimensions\n",
        "  * CLIP image embeddings → 512 or 1024 dimensions\n",
        "* Mismatched dimensions will result in errors when upserting or querying.\n",
        "\n",
        "2. **Metric**\n",
        "\n",
        "* Determines how similarity between vectors is calculated.\n",
        "* Options:\n",
        "\n",
        "  | Metric             | Description                     | Use Case                                 |\n",
        "  | ------------------ | ------------------------------- | ---------------------------------------- |\n",
        "  | **Cosine**         | Measures angular similarity     | NLP embeddings, semantic search          |\n",
        "  | **Euclidean (L2)** | Measures straight-line distance | Image embeddings, numeric feature spaces |\n",
        "  | **Dot Product**    | Measures vector alignment       | Recommendation engines, ranking scores   |\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "pinecone.create_index(\n",
        "    name=\"semantic-index\",\n",
        "    dimension=1536,\n",
        "    metric=\"cosine\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Sharding and Replication**\n",
        "\n",
        "1. **Sharding**\n",
        "\n",
        "* Splits the index into multiple partitions to **scale horizontally**.\n",
        "* Benefits:\n",
        "\n",
        "  * Supports **billions of vectors** across multiple nodes.\n",
        "  * Enables **parallel search** to improve query throughput.\n",
        "\n",
        "2. **Replication**\n",
        "\n",
        "* Creates copies of your index for **high availability and fault tolerance**.\n",
        "* Benefits:\n",
        "\n",
        "  * Ensures **redundancy** in case of node failures.\n",
        "  * Supports **read-heavy workloads**, improving query speed by distributing requests across replicas.\n",
        "\n",
        "**Configuration Example:**\n",
        "\n",
        "```python\n",
        "pinecone.create_index(\n",
        "    name=\"large-scale-index\",\n",
        "    dimension=1536,\n",
        "    metric=\"cosine\",\n",
        "    replicas=2,    # two replicas for redundancy\n",
        "    shards=4       # index is split into 4 shards for horizontal scaling\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Managing Indexes**\n",
        "\n",
        "* **List Indexes:** `pinecone.list_indexes()`\n",
        "* **Delete Index:** `pinecone.delete_index(\"index_name\")`\n",
        "* **Describe Index:** `pinecone.describe_index(\"index_name\")` returns configuration and metadata\n",
        "\n",
        "**Best Practices:**\n",
        "\n",
        "* Choose **index type and dimension** carefully based on your embeddings.\n",
        "* Use **shards and replicas** to handle **large datasets and high query loads**.\n",
        "* Monitor **query performance and resource utilization** to optimize scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Adding and Querying Vectors in Pinecone**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Inserting Vectors**\n",
        "\n",
        "Pinecone allows you to add vectors to an index **individually** or in **batches**, along with optional metadata for filtering.\n",
        "\n",
        "1. **Single Vector Upsert**\n",
        "\n",
        "```python\n",
        "import pinecone\n",
        "import numpy as np\n",
        "\n",
        "# Connect to your index\n",
        "index = pinecone.Index(\"semantic-index\")\n",
        "\n",
        "# Sample single vector\n",
        "vector = np.random.rand(1536).tolist()\n",
        "metadata = {\"category\": \"AI\"}\n",
        "\n",
        "# Upsert single vector\n",
        "index.upsert(vectors=[(\"vec-1\", vector, metadata)])\n",
        "```\n",
        "\n",
        "2. **Batch Vector Upsert**\n",
        "\n",
        "* Efficient for inserting large datasets.\n",
        "\n",
        "```python\n",
        "vectors = [\n",
        "    (f\"vec-{i}\", np.random.rand(1536).tolist(), {\"category\": \"AI\"}) \n",
        "    for i in range(100)\n",
        "]\n",
        "index.upsert(vectors=vectors)\n",
        "```\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "* Each vector requires a **unique ID**.\n",
        "* Metadata is optional but recommended for **hybrid search**.\n",
        "\n",
        "---\n",
        "\n",
        "**Querying for Similarity (Top-K Search)**\n",
        "\n",
        "* Pinecone supports **nearest neighbor search** using a similarity metric (cosine, L2, dot product).\n",
        "* You can retrieve the **top-K most similar vectors** to a query vector.\n",
        "\n",
        "```python\n",
        "query_vector = np.random.rand(1536).tolist()\n",
        "\n",
        "# Retrieve top 5 similar vectors\n",
        "results = index.query(vector=query_vector, top_k=5)\n",
        "for match in results[\"matches\"]:\n",
        "    print(f\"ID: {match['id']}, Score: {match['score']}\")\n",
        "```\n",
        "\n",
        "* **Top-K Search:** `top_k` specifies the number of nearest neighbors to return.\n",
        "\n",
        "---\n",
        "\n",
        "**Filtering by Metadata**\n",
        "\n",
        "* Pinecone allows **filtering vectors using metadata** during queries.\n",
        "* Useful for **hybrid search**, e.g., retrieve only vectors from a certain category or date range.\n",
        "\n",
        "```python\n",
        "results = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=5,\n",
        "    filter={\"category\": {\"$eq\": \"AI\"}}  # Only consider vectors with category \"AI\"\n",
        ")\n",
        "```\n",
        "\n",
        "* Supported filter operations: `$eq`, `$in`, `$lt`, `$gt`, `$and`, `$or`.\n",
        "\n",
        "---\n",
        "\n",
        "**Upserting and Deleting Vectors**\n",
        "\n",
        "1. **Upserting**\n",
        "\n",
        "* Adds a vector if it doesn’t exist; updates it if the ID already exists.\n",
        "\n",
        "```python\n",
        "# Upsert vector with ID \"vec-1\"\n",
        "index.upsert(vectors=[(\"vec-1\", np.random.rand(1536).tolist())])\n",
        "```\n",
        "\n",
        "2. **Deleting**\n",
        "\n",
        "* Remove vectors by **ID** or using **metadata filters**.\n",
        "\n",
        "```python\n",
        "# Delete by ID\n",
        "index.delete(ids=[\"vec-1\", \"vec-2\"])\n",
        "\n",
        "# Delete by metadata filter\n",
        "index.delete(filter={\"category\": {\"$eq\": \"AI\"}})\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Best Practices:**\n",
        "\n",
        "* Use **batch upserts** for large datasets to reduce API overhead.\n",
        "* Maintain **unique IDs** for each vector.\n",
        "* Leverage **metadata filters** to optimize search relevance.\n",
        "* Regularly monitor vector counts and index performance to ensure **efficient querying**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Namespaces and Metadata in Pinecone**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Organizing Data with Namespaces**\n",
        "\n",
        "* A **namespace** is a logical partition within a Pinecone index that allows you to **segregate vectors** by project, dataset, or use case.\n",
        "* Think of it as a **virtual sub-database** within an index.\n",
        "* Benefits:\n",
        "\n",
        "  * Prevent ID collisions across different applications.\n",
        "  * Support **multi-tenant setups**.\n",
        "  * Simplify management of large, diverse datasets.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "import pinecone\n",
        "index = pinecone.Index(\"semantic-index\")\n",
        "\n",
        "# Upsert vectors into a specific namespace\n",
        "vectors = [\n",
        "    (\"vec-1\", [0.1]*1536, {\"category\": \"AI\"}),\n",
        "    (\"vec-2\", [0.2]*1536, {\"category\": \"ML\"})\n",
        "]\n",
        "index.upsert(vectors=vectors, namespace=\"project-alpha\")\n",
        "```\n",
        "\n",
        "* You can create multiple namespaces within the same index: `\"project-alpha\"`, `\"project-beta\"`, etc.\n",
        "\n",
        "---\n",
        "\n",
        "**Using Metadata for Filtering**\n",
        "\n",
        "* **Metadata** are key-value pairs associated with each vector.\n",
        "* Enables **hybrid search**: combining vector similarity with semantic filters.\n",
        "* Typical metadata examples:\n",
        "\n",
        "  * `\"category\": \"AI\"`\n",
        "  * `\"author\": \"John Doe\"`\n",
        "  * `\"date\": \"2025-10-19\"`\n",
        "\n",
        "**Query with Metadata Filter:**\n",
        "\n",
        "```python\n",
        "query_vector = [0.15]*1536\n",
        "results = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=5,\n",
        "    filter={\"category\": {\"$eq\": \"AI\"}},\n",
        "    namespace=\"project-alpha\"\n",
        ")\n",
        "for match in results[\"matches\"]:\n",
        "    print(f\"ID: {match['id']}, Score: {match['score']}\")\n",
        "```\n",
        "\n",
        "* Only vectors matching the metadata filter are considered in the similarity search.\n",
        "\n",
        "---\n",
        "\n",
        "**Advanced Filtering Queries**\n",
        "\n",
        "* Pinecone supports complex filters using **logical operators**: `$and`, `$or`, `$in`, `$lt`, `$gt`.\n",
        "* Useful for **refining search results** based on multiple attributes.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "1. **Multiple Conditions (AND)**\n",
        "\n",
        "```python\n",
        "filter_query = {\n",
        "    \"$and\": [\n",
        "        {\"category\": {\"$eq\": \"AI\"}},\n",
        "        {\"date\": {\"$gt\": \"2025-01-01\"}}\n",
        "    ]\n",
        "}\n",
        "results = index.query(vector=query_vector, top_k=5, filter=filter_query)\n",
        "```\n",
        "\n",
        "2. **OR Condition**\n",
        "\n",
        "```python\n",
        "filter_query = {\n",
        "    \"$or\": [\n",
        "        {\"category\": {\"$eq\": \"AI\"}},\n",
        "        {\"category\": {\"$eq\": \"ML\"}}\n",
        "    ]\n",
        "}\n",
        "results = index.query(vector=query_vector, top_k=5, filter=filter_query)\n",
        "```\n",
        "\n",
        "3. **Using `$in` Operator**\n",
        "\n",
        "```python\n",
        "filter_query = {\"category\": {\"$in\": [\"AI\", \"ML\", \"NLP\"]}}\n",
        "results = index.query(vector=query_vector, top_k=5, filter=filter_query)\n",
        "```\n",
        "\n",
        "* Combine **metadata filters with namespaces** for **fine-grained control** over which vectors are queried.\n",
        "\n",
        "---\n",
        "\n",
        "**Best Practices:**\n",
        "\n",
        "* Use **namespaces** to separate unrelated datasets or client projects.\n",
        "* Assign **meaningful metadata** for every vector to enable flexible filtering.\n",
        "* Leverage **logical operators** to refine search results without creating multiple indexes.\n",
        "* Avoid storing large metadata fields—keep metadata **lightweight** for performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding Generation and RAG with Pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Integrating Pinecone with OpenAI, Hugging Face, or Custom Embeddings**\n",
        "\n",
        "* Pinecone works with **any vector embeddings**, whether from **pretrained models** or **custom models**.\n",
        "* Typical workflow:\n",
        "\n",
        "  1. Generate embeddings from your text, image, or multi-modal data.\n",
        "  2. Upsert embeddings into Pinecone index.\n",
        "  3. Query the index using similarity search for downstream tasks like **RAG**.\n",
        "\n",
        "**Example: OpenAI Embeddings**\n",
        "\n",
        "```python\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "import pinecone\n",
        "\n",
        "# Initialize embeddings model\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "pinecone.init(api_key=\"YOUR_API_KEY\", environment=\"us-west1-gcp\")\n",
        "index_name = \"semantic-index\"\n",
        "\n",
        "# Example documents\n",
        "documents = [\n",
        "    {\"id\": \"1\", \"text\": \"LangChain is a framework for building LLM applications.\"},\n",
        "    {\"id\": \"2\", \"text\": \"Pinecone provides a managed vector database for embeddings.\"}\n",
        "]\n",
        "\n",
        "# Create a Pinecone vector store\n",
        "vectorstore = Pinecone.from_texts(\n",
        "    texts=[doc[\"text\"] for doc in documents],\n",
        "    embedding=embedding_model,\n",
        "    index_name=index_name\n",
        ")\n",
        "```\n",
        "\n",
        "* Hugging Face embeddings or custom embeddings can be integrated similarly by **generating vectors first** and then upserting them into Pinecone.\n",
        "\n",
        "---\n",
        "\n",
        "**Preprocessing and Normalization**\n",
        "\n",
        "* **Text preprocessing:** remove punctuation, lowercase text, remove stopwords (optional).\n",
        "* **Vector normalization:**\n",
        "\n",
        "  * Normalize embeddings to unit length if using **cosine similarity**, ensuring **distance is proportional to semantic similarity**.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "vector = np.array([0.1, 0.2, 0.3])\n",
        "normalized_vector = vector / np.linalg.norm(vector)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Handling Large Datasets**\n",
        "\n",
        "* **Batch upserts:** Insert vectors in batches to avoid API limits.\n",
        "* **Namespaces:** Organize data for multi-tenant applications or large projects.\n",
        "* **Sharding and Replication:** Use Pinecone’s built-in **sharding** for billions of vectors and **replicas** for high availability.\n",
        "\n",
        "```python\n",
        "batch_size = 100\n",
        "for i in range(0, len(vectors), batch_size):\n",
        "    batch = vectors[i:i+batch_size]\n",
        "    index.upsert(vectors=batch)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Retrieval-Augmented Generation (RAG)**\n",
        "\n",
        "* **Concept:** Combine **vector search** with **LLMs** to provide context-aware answers.\n",
        "* Workflow:\n",
        "\n",
        "  1. User query → embed query into vector space\n",
        "  2. Search Pinecone for **top-K similar vectors**\n",
        "  3. Combine retrieved text with LLM prompt → generate final response\n",
        "\n",
        "---\n",
        "\n",
        "**Using Pinecone with LLMs**\n",
        "\n",
        "* Pinecone is often used in **LangChain pipelines** for RAG.\n",
        "* Example with **LangChain:**\n",
        "\n",
        "```python\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Create RAG pipeline\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    chain_type=\"stuff\"  # or \"map_reduce\", \"refine\"\n",
        ")\n",
        "\n",
        "query = \"What is Pinecone?\"\n",
        "answer = qa.run(query)\n",
        "print(answer)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**LangChain + Pinecone Integration**\n",
        "\n",
        "* Use `Pinecone.from_texts` or `Pinecone.from_documents` to load embeddings.\n",
        "* Use `as_retriever()` to integrate with **RAG chains**.\n",
        "* Supports **filtering by metadata** to improve search relevance.\n",
        "\n",
        "---\n",
        "\n",
        "**Building QA Systems**\n",
        "\n",
        "* Use **vector embeddings** to represent knowledge documents.\n",
        "* Store embeddings in Pinecone with metadata for filtering.\n",
        "* Create **RAG pipeline** in LangChain:\n",
        "\n",
        "  * Query embedding → Vector search → Retrieve context → Feed to LLM → Generate answer\n",
        "* Supports **dynamic knowledge bases**, real-time updates, and **multi-modal retrieval**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Hybrid Search with Pinecone**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Combining Keyword and Vector Search**\n",
        "\n",
        "* **Hybrid search** integrates **traditional keyword-based search** with **vector similarity search**.\n",
        "* Purpose: **leverage the strengths of both methods** for more accurate and context-aware results.\n",
        "\n",
        "  * **Keyword search**: Precise matching of specific terms, great for exact queries.\n",
        "  * **Vector search**: Captures semantic meaning and context, retrieving items with similar embeddings even if keywords differ.\n",
        "\n",
        "**Implementation Concept:**\n",
        "\n",
        "1. Perform a **vector similarity search** in Pinecone to get semantically related items.\n",
        "2. Apply a **keyword filter** or **score boost** to favor results containing specific terms.\n",
        "\n",
        "```python\n",
        "query_vector = embedding_model.embed_query(\"LangChain RAG tutorial\")\n",
        "results = index.query(\n",
        "    vector=query_vector,\n",
        "    top_k=10,\n",
        "    filter={\"category\": {\"$eq\": \"AI\"}},\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "# Optionally, re-rank results based on keyword match scores\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Weighted Scoring Techniques**\n",
        "\n",
        "* Hybrid search often involves **combining scores** from vectors and keywords.\n",
        "* Weighted scoring allows you to **adjust the contribution** of each method:\n",
        "\n",
        "| Component         | Description                      | Weighting Example                                    |\n",
        "| ----------------- | -------------------------------- | ---------------------------------------------------- |\n",
        "| **Vector Score**  | Cosine or dot product similarity | 0.7 (70% influence)                                  |\n",
        "| **Keyword Score** | TF-IDF, BM25, or Boolean match   | 0.3 (30% influence)                                  |\n",
        "| **Final Score**   | Combined score for ranking       | `final_score = 0.7*vector_score + 0.3*keyword_score` |\n",
        "\n",
        "* This approach ensures that **semantically similar results** are retrieved while still respecting **explicit keyword requirements**.\n",
        "\n",
        "---\n",
        "\n",
        "**Real-World Use Cases**\n",
        "\n",
        "1. **E-commerce Search**\n",
        "\n",
        "   * Users search for products with vague descriptions.\n",
        "   * Vector search retrieves **semantically similar products**, while keyword filters enforce categories or brands.\n",
        "\n",
        "2. **Knowledge Base / FAQ Systems**\n",
        "\n",
        "   * Users ask questions in natural language.\n",
        "   * Vector embeddings capture semantic similarity to documents.\n",
        "   * Keywords help narrow results to **specific topics or sections**.\n",
        "\n",
        "3. **Recommendation Engines**\n",
        "\n",
        "   * Combine vector similarity for **user preferences** with keyword metadata like tags, categories, or genres.\n",
        "\n",
        "4. **Multi-Modal Search**\n",
        "\n",
        "   * Example: Text + image search.\n",
        "   * Vector similarity handles **semantic matching**, while keywords filter by **metadata like product type, date, or author**.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "* Hybrid search improves **retrieval accuracy** by combining **semantic understanding** with **precise keyword control**.\n",
        "* Weighted scoring lets you **tune results** based on the application’s needs.\n",
        "* Pinecone’s **vector database + metadata filtering** naturally supports hybrid search pipelines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Performance Optimization in Pinecone**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Index Configuration Tuning**\n",
        "\n",
        "* Proper **index configuration** ensures **fast and accurate vector retrieval**. Key parameters include:\n",
        "\n",
        "  | Parameter            | Description                                        | Optimization Tips                                                                |\n",
        "  | -------------------- | -------------------------------------------------- | -------------------------------------------------------------------------------- |\n",
        "  | **Metric**           | Similarity metric (cosine, Euclidean, dot product) | Choose metric matching embedding type                                            |\n",
        "  | **Shards**           | Number of partitions of your index                 | Increase for large datasets to distribute load                                   |\n",
        "  | **Replicas**         | Number of copies for redundancy                    | More replicas improve read throughput                                            |\n",
        "  | **Vector Dimension** | Dimensionality of embeddings                       | Must match your embedding model; consider **dimensionality reduction** if needed |\n",
        "  | **Index Type**       | Standard, memory-optimized, hybrid                 | Pick based on dataset size and query patterns                                    |\n",
        "\n",
        "* **Tips:**\n",
        "\n",
        "  * For **read-heavy workloads**, increase replicas.\n",
        "  * For **very large datasets**, increase shards to improve parallel query performance.\n",
        "  * Monitor latency and throughput using **Pinecone metrics** and adjust accordingly.\n",
        "\n",
        "---\n",
        "\n",
        "**Managing Throughput and Latency**\n",
        "\n",
        "* **Throughput**: Number of queries processed per second.\n",
        "* **Latency**: Time taken for a single query to return results.\n",
        "* Optimization strategies:\n",
        "\n",
        "  * Use **batch queries** to reduce API overhead.\n",
        "  * Precompute embeddings for static data.\n",
        "  * Choose appropriate **index type** (memory-optimized for large datasets).\n",
        "  * Minimize **metadata filtering complexity** during high-frequency queries.\n",
        "\n",
        "---\n",
        "\n",
        "**Batch Querying and Bulk Operations**\n",
        "\n",
        "* **Batch querying** allows multiple vectors to be queried at once, reducing network overhead.\n",
        "* **Bulk upserts** improve efficiency when inserting large datasets.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "# Batch upsert\n",
        "batch_vectors = [(f\"id-{i}\", np.random.rand(1536).tolist()) for i in range(1000)]\n",
        "index.upsert(vectors=batch_vectors)\n",
        "\n",
        "# Batch query\n",
        "query_vectors = [np.random.rand(1536).tolist() for _ in range(10)]\n",
        "results = index.query_batch(vectors=query_vectors, top_k=5)\n",
        "```\n",
        "\n",
        "* **Advantages:**\n",
        "\n",
        "  * Reduces latency per vector.\n",
        "  * Efficiently utilizes system resources.\n",
        "\n",
        "---\n",
        "\n",
        "**Scaling and High Availability**\n",
        "\n",
        "**Horizontal Scaling**\n",
        "\n",
        "* Sharding splits the index into multiple partitions, allowing **parallel processing**.\n",
        "* Supports **billions of vectors** by distributing load across shards.\n",
        "\n",
        "**Multi-Region Deployments**\n",
        "\n",
        "* Replicate indexes across regions for **low-latency access worldwide**.\n",
        "* Ensures **business continuity** and **geographic fault tolerance**.\n",
        "\n",
        "**Replication and Failover**\n",
        "\n",
        "* **Replicas** provide redundancy, ensuring queries succeed even if a node fails.\n",
        "* Automatic failover: Pinecone redirects queries to available replicas without downtime.\n",
        "\n",
        "**Best Practices:**\n",
        "\n",
        "* Monitor **query latency and throughput** using Pinecone dashboards.\n",
        "* Adjust **shards and replicas** dynamically based on workload.\n",
        "* Use **batch operations** for large-scale inserts and queries.\n",
        "* Combine **metadata filtering** with vector search carefully to avoid bottlenecks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Security and Best Practices in Pinecone**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n",
        "**API Key Management**\n",
        "\n",
        "* Pinecone uses **API keys** to authenticate and authorize access to your indexes.\n",
        "* **Best Practices:**\n",
        "\n",
        "  * **Store keys securely**: Use environment variables or secrets management tools instead of hardcoding.\n",
        "  * **Rotate keys regularly**: Reduces risk in case of exposure.\n",
        "  * **Limit permissions**: Generate keys with minimal required privileges (read-only vs. read/write).\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```bash\n",
        "export PINECONE_API_KEY=\"your_secure_api_key\"\n",
        "export PINECONE_ENVIRONMENT=\"us-west1-gcp\"\n",
        "```\n",
        "\n",
        "```python\n",
        "import pinecone\n",
        "import os\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=os.environ[\"PINECONE_API_KEY\"],\n",
        "    environment=os.environ[\"PINECONE_ENVIRONMENT\"]\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Data Privacy and Encryption**\n",
        "\n",
        "* Pinecone encrypts **data in transit** and **at rest** by default.\n",
        "* **Recommendations for sensitive data:**\n",
        "\n",
        "  * Avoid storing personally identifiable information (PII) in embeddings.\n",
        "  * Preprocess or anonymize sensitive fields before embedding.\n",
        "  * Combine Pinecone with **secure cloud storage** for backups of raw data.\n",
        "\n",
        "**Encryption Details:**\n",
        "\n",
        "* **TLS/HTTPS** for all client-server communications.\n",
        "* Data at rest is encrypted using **industry-standard AES encryption**.\n",
        "\n",
        "---\n",
        "\n",
        "**Access Control and Namespaces**\n",
        "\n",
        "* **Namespaces** provide a logical separation of data, enabling **multi-tenant setups** or **project-specific isolation**.\n",
        "* Access control strategies:\n",
        "\n",
        "  * Restrict API key usage to specific namespaces when possible.\n",
        "  * Use **metadata and filters** to control visibility within an index.\n",
        "  * Implement role-based access on your application layer to complement Pinecone’s native controls.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "# Upsert vectors to a specific namespace\n",
        "index.upsert(\n",
        "    vectors=[(\"vec-1\", [0.1]*1536, {\"project\": \"alpha\"})],\n",
        "    namespace=\"project-alpha\"\n",
        ")\n",
        "\n",
        "# Query only vectors in that namespace\n",
        "results = index.query(vector=query_vector, top_k=5, namespace=\"project-alpha\")\n",
        "```\n",
        "\n",
        "**Best Practices Summary:**\n",
        "\n",
        "* **API Keys:** Store securely, rotate regularly, and limit permissions.\n",
        "* **Data Privacy:** Anonymize sensitive data and rely on Pinecone’s encryption.\n",
        "* **Namespaces:** Use namespaces to isolate data for different teams, projects, or clients.\n",
        "* **Monitoring:** Regularly audit usage, access logs, and index operations to detect anomalies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Vector Index Types Deep Dive**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Understanding Pinecone’s Index Types**\n",
        "\n",
        "Pinecone provides multiple **index types** to optimize for **latency, throughput, memory usage, and functionality**. Each index type is designed for specific use cases:\n",
        "\n",
        "| Index Type           | Description                                                                           | Best Use Case                                                      |\n",
        "| -------------------- | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |\n",
        "| **Standard**         | Default index type optimized for balanced performance and accuracy                    | General-purpose vector search                                      |\n",
        "| **Memory-Optimized** | Reduces RAM usage per vector, allowing larger datasets in memory                      | Large-scale datasets where memory efficiency is critical           |\n",
        "| **Hybrid**           | Supports multiple vector types and combines vector similarity with metadata filtering | Multi-modal search (e.g., text + image), semantic + keyword search |\n",
        "| **Sparse**           | Designed for low-density embeddings (e.g., bag-of-words vectors)                      | Sparse high-dimensional vectors                                    |\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* **Automatic ANN (Approximate Nearest Neighbor) indexing** for fast retrieval.\n",
        "* **Sharding and replication support** for horizontal scaling and high availability.\n",
        "* **Metadata filtering** to combine vector search with attribute-based queries.\n",
        "\n",
        "---\n",
        "\n",
        "**Approximate vs. Exact Search**\n",
        "\n",
        "* **Exact Search:**\n",
        "\n",
        "  * Compares the query vector with every vector in the index.\n",
        "  * Guarantees **perfect accuracy**, but **slow for large datasets**.\n",
        "  * Typically used for **small datasets** or where precision is critical.\n",
        "\n",
        "* **Approximate Nearest Neighbor (ANN) Search:**\n",
        "\n",
        "  * Uses algorithms like **HNSW** (Hierarchical Navigable Small World) to find vectors **close to the query without checking all vectors**.\n",
        "  * **Faster** and more scalable, suitable for **millions to billions of vectors**.\n",
        "  * Slight trade-off in accuracy, usually negligible in practical applications.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "* A standard Pinecone index uses ANN search by default, balancing **speed and retrieval quality**.\n",
        "\n",
        "---\n",
        "\n",
        "**Choosing the Right Index for Your Data**\n",
        "\n",
        "Consider these factors when selecting an index type:\n",
        "\n",
        "| Factor                        | Recommendation                                                                                                           |\n",
        "| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Dataset Size**              | Small (<100k vectors): Standard or exact search. Large (>1M vectors): Memory-optimized or standard with sharding.        |\n",
        "| **Vector Density**            | Dense embeddings (BERT, CLIP): Standard or Hybrid. Sparse embeddings: Sparse index.                                      |\n",
        "| **Query Latency Requirement** | Low-latency: ANN search with standard or hybrid index. High-accuracy required: Exact search or fewer ANN approximations. |\n",
        "| **Multi-Modal Needs**         | Hybrid index supports multiple vector types and metadata filtering.                                                      |\n",
        "| **Resource Constraints**      | Memory-optimized index reduces RAM usage per vector.                                                                     |\n",
        "\n",
        "**Best Practices:**\n",
        "\n",
        "* Start with a **standard index** for initial experiments.\n",
        "* Profile queries for latency and accuracy before scaling.\n",
        "* Use **metadata filtering** to reduce search space and improve relevance.\n",
        "* Consider **shards and replicas** for horizontal scaling and fault tolerance.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
