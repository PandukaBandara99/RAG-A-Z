{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04953c1b",
   "metadata": {},
   "source": [
    "# Module Overview\n",
    "This module covers everything about parsing and ingesting data for RAG systems, from basic text files to complex PDFs and databases. We'll use LangChain v0.3.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- Introduction to Data Ingestion\n",
    "- Text Files (.txt)\n",
    "- PDF Documents\n",
    "- Microsoft Word Documents\n",
    "- CSV and Excel Files\n",
    "- JSON and Structured Data\n",
    "- Web Scraping\n",
    "- Databases (SQL)\n",
    "- Audio and Video Transcripts\n",
    "- Advanced Techniques\n",
    "- Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884961ac",
   "metadata": {},
   "source": [
    "# Introduction To Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d033c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "import os\n",
    "# Import type hints for clearer function signatures and better editor support\n",
    "from typing import List, Dict, Any\n",
    "# Import pandas for data manipulation and analysis\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e6820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up Completed!\n"
     ]
    }
   ],
   "source": [
    "# Importing the Document class from LangChain Core\n",
    "# ------------------------------------------------\n",
    "# The `Document` class represents a single document (or text unit) in LangChain.\n",
    "# It typically contains two parts:\n",
    "#   1. page_content → the main text\n",
    "#   2. metadata → extra information (like file name, source, etc.)\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# Importing Text Splitter classes\n",
    "# -------------------------------\n",
    "# LangChain provides several text splitter utilities to break large texts into smaller chunks.\n",
    "# This helps overcome token limits and improves retrieval accuracy.\n",
    "from langchain.text_splitter import (\n",
    "    # Splits text while preserving semantic boundaries (best general-purpose splitter)\n",
    "    RecursiveCharacterTextSplitter,  \n",
    "    # Splits text based on character count or separators like '\\n'\n",
    "    CharacterTextSplitter,           \n",
    "    # Splits text based on token count (useful when working with LLM token limits)\n",
    "    TokenTextSplitter                \n",
    ")\n",
    "\n",
    "\n",
    "# Printing a confirmation message\n",
    "# -------------------------------\n",
    "# This line simply indicates that all imports and setup were successful.\n",
    "print(\"Set up Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d151ce3",
   "metadata": {},
   "source": [
    "## Understanding Document Structure In Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29ecf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Structure\n",
      "Content  : This is the main text content that will be embedded and searched.\n",
      "Metadata : {'source': 'example.txt', 'page': 1, 'author': 'Krish Naik', 'date_created': '2024-01-01', 'custom_field': 'any_value'}\n"
     ]
    }
   ],
   "source": [
    "# Import the Document class\n",
    "# --------------------------\n",
    "# The Document class is the fundamental data structure in LangChain used\n",
    "# to represent a piece of text (page_content) and its associated metadata.\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# Create a simple Document object\n",
    "# ----------------------------------\n",
    "# Here, we define a document with:\n",
    "# - `page_content`: The actual text to be processed or embedded.\n",
    "# - `metadata`: Additional context or attributes describing the text,\n",
    "#   such as the source file, author, date, or any custom info.\n",
    "\n",
    "doc = Document(\n",
    "    page_content=\"This is the main text content that will be embedded and searched.\",\n",
    "    metadata={\n",
    "        \"source\": \"example.txt\",        # File or data source name\n",
    "        \"page\": 1,                      # Page number or section ID\n",
    "        \"author\": \"Krish Naik\",         # Author or data origin\n",
    "        \"date_created\": \"2024-01-01\",   # Date of creation or ingestion\n",
    "        \"custom_field\": \"any_value\"     # Any custom metadata field\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Print statements to confirm document creation\n",
    "# ---------------------------------------------\n",
    "# These lines display the internal structure of the Document object.\n",
    "print(\"Document Structure\")\n",
    "print(f\"Content  : {doc.page_content}\")  # Displays the main text content\n",
    "print(f\"Metadata : {doc.metadata}\")      # Displays all metadata as a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592d84f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0543b7",
   "metadata": {},
   "source": [
    "## Text Files (.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a975659",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "import os\n",
    "os.makedirs(\"data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b14b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c552f4e",
   "metadata": {},
   "source": [
    "### TextLoader- Read Single File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c875d854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document\n",
      "Content preview: Python Programming Introduction\n",
      "\n",
      "Python is a high-level, interpreted programming language known for ...\n",
      "Metadata: {'source': 'data/text_files/python_intro.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Importing the TextLoader class\n",
    "# ------------------------------\n",
    "# There are two sources for loaders in LangChain:\n",
    "# - `langchain.document_loaders`: Older version (deprecated in some releases)\n",
    "# - `langchain_community.document_loaders`: Newer maintained version\n",
    "# You can safely use the second one.\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# Load a single text file\n",
    "# --------------------------\n",
    "# TextLoader reads plain text files (.txt) and converts their content\n",
    "# into LangChain Document objects.\n",
    "# Each file will be loaded as a list of one or more Document instances.\n",
    "loader = TextLoader(\"data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Load the file content\n",
    "# ---------------------\n",
    "# The .load() method reads the file, extracts its text,\n",
    "# and returns it as a list of Document objects.\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "# Display summary information\n",
    "# ----------------------------\n",
    "# Let's print how many documents were loaded and show a short preview\n",
    "# of the first document’s content and metadata.\n",
    "\n",
    "# Number of loaded docs\n",
    "print(f\"Loaded {len(documents)} document\")             \n",
    "# First 100 chars of the content         \n",
    "print(f\"Content preview: {documents[0].page_content[:100]}...\") \n",
    "# File details like path or source\n",
    "print(f\"Metadata: {documents[0].metadata}\")                      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79596d23",
   "metadata": {},
   "source": [
    "### DirectoryLoader- Multiple Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d625d3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1816.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loaded 2 documents\n",
      "\n",
      "Document 1:\n",
      "  Source: data\\text_files\\machine_learning.txt\n",
      "  Length: 575 characters\n",
      "\n",
      "Document 2:\n",
      "  Source: data\\text_files\\python_intro.txt\n",
      "  Length: 489 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing DirectoryLoader\n",
    "# --------------------------\n",
    "# DirectoryLoader helps to automatically scan a folder and load multiple files\n",
    "# into LangChain Document objects. It can recursively search through subdirectories,\n",
    "# apply file-matching patterns, and use any specific loader (like TextLoader or PyPDFLoader).\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader  # required for loader_cls\n",
    "\n",
    "\n",
    "# ✅ Load all text files from a directory\n",
    "# ---------------------------------------\n",
    "# DirectoryLoader parameters:\n",
    "# - \"data/text_files\": The root directory path.\n",
    "# - glob=\"**/*.txt\": Pattern to match all text files recursively.\n",
    "# - loader_cls=TextLoader: Defines how each file is loaded (here, as text).\n",
    "# - loader_kwargs={'encoding': 'utf-8'}: Extra parameters for the loader (e.g., encoding).\n",
    "# - show_progress=True: Displays a progress bar while loading files.\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"data/text_files\",\n",
    "    # Matches all .txt files inside folders and subfolders\n",
    "    glob=\"**/*.txt\",                    \n",
    "    # Use TextLoader for each file\n",
    "    loader_cls=TextLoader,    \n",
    "    # Pass UTF-8 encoding          \n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    # Show progress bar during file loading\n",
    "    show_progress=True                  \n",
    ")\n",
    "\n",
    "\n",
    "# Load all matched documents into a list\n",
    "# --------------------------------------\n",
    "# Each file is loaded as a Document object (text + metadata)\n",
    "documents = dir_loader.load()\n",
    "\n",
    "\n",
    "# Display summary information\n",
    "# -------------------------------\n",
    "# Print the total number of documents loaded and display details of each.\n",
    "print(f\"📁 Loaded {len(documents)} documents\")\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    # File path or source name\n",
    "    print(f\"  Source: {doc.metadata['source']}\")     \n",
    "    # Number of characters in content     \n",
    "    print(f\"  Length: {len(doc.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae22b9",
   "metadata": {},
   "source": [
    "| Component           | Description                                                       |\n",
    "| ------------------- | ----------------------------------------------------------------- |\n",
    "| **DirectoryLoader** | Scans a folder and loads all matching files as `Document` objects |\n",
    "| **glob=\"**/*.txt\"** | Recursively loads all `.txt` files inside the directory           |\n",
    "| **loader_cls**      | Defines which loader to use for each file                         |\n",
    "| **loader_kwargs**   | Extra parameters (e.g., encoding, options) passed to the loader   |\n",
    "| **show_progress**   | Displays progress bar while loading                               |\n",
    "| **documents**       | List of `Document` objects containing text and metadata           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb958cd",
   "metadata": {},
   "source": [
    "| Aspect          | Details                                                                                                                                                                                                                     |\n",
    "| --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| ✅ Advantages    | - Loads multiple files at once (efficient bulk loading) <br> - Supports glob patterns (e.g., `*.txt`, `*.pdf`) <br> - Progress tracking while loading <br> - Recursive directory scanning (loads files from nested folders) |\n",
    "| ❌ Disadvantages | - All files must be of the same type (one loader per run) <br> - Limited error handling per file (one failure may affect batch) <br> - Can be memory intensive for large directories                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c621c75",
   "metadata": {},
   "source": [
    "## Text Splitting Statergies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deaf6508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '), Document(metadata={'source': 'data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "# Importing different text splitter classes\n",
    "# -----------------------------------------\n",
    "# LangChain provides multiple text splitters to handle large documents\n",
    "# before embedding or retrieval. Each splitter has its own strategy:\n",
    "\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,           # Splits text by character count or separators (simple approach)\n",
    "    RecursiveCharacterTextSplitter,  # Recursively splits text by paragraphs, sentences, or sections (context-aware)\n",
    "    TokenTextSplitter                # Splits text based on LLM tokens instead of raw characters\n",
    ")\n",
    "\n",
    "# Display the loaded documents for reference\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a023063",
   "metadata": {},
   "source": [
    "### Character Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d098593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MEthod 1- Character Text Splitter\n",
    "text=documents[0].page_content\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e982f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1️⃣ CHARACTER TEXT SPLITTER\n",
      "Created 3 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Character-based splitting\n",
    "print(\"1️⃣ CHARACTER TEXT SPLITTER\")\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",  # Split on newlines\n",
    "    chunk_size=200,  # Max chunk size in characters\n",
    "    chunk_overlap=20,  # Overlap between chunks\n",
    "    length_function=len  # How to measure chunk size\n",
    ")\n",
    "\n",
    "char_chunks=char_splitter.split_text(text)\n",
    "print(f\"Created {len(char_chunks)} chunks\")\n",
    "print(f\"First chunk: {char_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d707e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "from experience without being explicitly programmed. It focuses on developing\n",
      "------------------\n",
      "on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "\n",
      "Types of Machine Learning:\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning:\n"
     ]
    }
   ],
   "source": [
    "print(char_chunks[0])\n",
    "print(\"------------------\")\n",
    "print(char_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d98de149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1️⃣ CHARACTER TEXT SPLITTER\n",
      "Created 4 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "Machine learning is a subset of artificial intelligence that enables systems...\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Character-based splitting\n",
    "print(\"1️⃣ CHARACTER TEXT SPLITTER\")\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",  # Split on newlines\n",
    "    chunk_size=200,  # Max chunk size in characters\n",
    "    chunk_overlap=20,  # Overlap between chunks\n",
    "    length_function=len  # How to measure chunk size\n",
    ")\n",
    "\n",
    "char_chunks=char_splitter.split_text(text)\n",
    "print(f\"Created {len(char_chunks)} chunks\")\n",
    "print(f\"First chunk: {char_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d25bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "-------------\n",
      "from experience without being explicitly programmed. It focuses on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "Types of Machine Learning:\n",
      "-------------\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
      "3. Reinforcement Learning: Learning through rewards and penalties\n"
     ]
    }
   ],
   "source": [
    "print(char_chunks[0])\n",
    "print(\"-------------\")\n",
    "print(char_chunks[1])\n",
    "print(\"-------------\")\n",
    "print(char_chunks[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411e75a",
   "metadata": {},
   "source": [
    "### Recursive Character Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc2177d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 4 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "# RECURSIVE CHARACTER TEXT SPLITTER (Recommended)\n",
    "# ---------------------------------------------------\n",
    "# This splitter is context-aware: it tries to split the text at logical boundaries\n",
    "# using the list of separators provided. It is ideal for long documents because\n",
    "# it preserves some context across chunks.\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Example text to split\n",
    "text = documents[0].page_content  # Using the first loaded document\n",
    "\n",
    "# Create a RecursiveCharacterTextSplitter instance\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Splits text using the first working separator (space in this case)\n",
    "    separators=[\" \"],  \n",
    "    # Maximum number of characters per chunk\n",
    "    chunk_size=200,    \n",
    "    # Number of characters to overlap between consecutive chunks\n",
    "    chunk_overlap=20,  \n",
    "    # Function to measure text length (here, number of characters)\n",
    "    length_function=len \n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "recursive_chunks = recursive_splitter.split_text(text)\n",
    "\n",
    "# Display summary information\n",
    "# Number of chunks created\n",
    "print(f\"Created {len(recursive_chunks)} chunks\")         \n",
    "# Show first 100 characters of the first chunk\n",
    "print(f\"First chunk: {recursive_chunks[0][:100]}...\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a70073f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "from experience without being explicitly programmed. It focuses on developing\n",
      "-----------------\n",
      "on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "\n",
      "Types of Machine Learning:\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning:\n",
      "------------------\n",
      "Learning: Finding patterns in unlabeled data\n",
      "3. Reinforcement Learning: Learning through rewards and penalties\n",
      "\n",
      "Applications include image recognition, speech processing, and recommendation\n"
     ]
    }
   ],
   "source": [
    "print(recursive_chunks[0])\n",
    "print(\"-----------------\")\n",
    "print(recursive_chunks[1])\n",
    "print(\"------------------\")\n",
    "print(recursive_chunks[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96364077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple text example - 4 chunks:\n",
      "\n",
      "Chunk 1: 'This is sentence one and it is quite long. This is sentence two and it is also'\n",
      "Chunk 2: 'two and it is also quite long. This is sentence three which is even longer than'\n",
      "\n",
      "Chunk 2: 'two and it is also quite long. This is sentence three which is even longer than'\n",
      "Chunk 3: 'is even longer than the others. This is sentence four. This is sentence five.'\n",
      "\n",
      "Chunk 3: 'is even longer than the others. This is sentence four. This is sentence five.'\n",
      "Chunk 4: 'is sentence five. This is sentence six.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Splitting text without natural break points\n",
    "# -----------------------------------------------------\n",
    "# Some texts don’t have clear paragraph or sentence boundaries (like logs or single-line content)\n",
    "# RecursiveCharacterTextSplitter can still split them based on specified separators and chunk sizes.\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Sample text (long sentences, no natural breaks)\n",
    "simple_text = (\n",
    "    \"This is sentence one and it is quite long. \"\n",
    "    \"This is sentence two and it is also quite long. \"\n",
    "    \"This is sentence three which is even longer than the others. \"\n",
    "    \"This is sentence four. This is sentence five. This is sentence six.\"\n",
    ")\n",
    "\n",
    "# Create a recursive character splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],  # Only split at spaces\n",
    "    chunk_size=80,     # Maximum characters per chunk\n",
    "    chunk_overlap=20,  # Overlap characters between consecutive chunks\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split the text\n",
    "chunks = splitter.split_text(simple_text)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nSimple text example - {len(chunks)} chunks:\\n\")\n",
    "\n",
    "for i in range(len(chunks) - 1):\n",
    "    print(f\"Chunk {i+1}: '{chunks[i]}'\")\n",
    "    print(f\"Chunk {i+2}: '{chunks[i+1]}'\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e190f14",
   "metadata": {},
   "source": [
    "### Token based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b368869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 3 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "# TOKEN TEXT SPLITTER\n",
    "# -----------------------\n",
    "# This splitter splits text based on tokens rather than characters.\n",
    "# Useful for LLM pipelines where token limits matter more than character count.\n",
    "\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Create a TokenTextSplitter instance\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,    # Maximum number of tokens per chunk\n",
    "    chunk_overlap=10  # Number of tokens to overlap between chunks\n",
    ")\n",
    "\n",
    "# Split the document text into token-based chunks\n",
    "token_chunks = token_splitter.split_text(text)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Created {len(token_chunks)} chunks\")\n",
    "print(f\"First chunk: {token_chunks[0][:100]}...\")  # Display first 100 characters of the first chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26bf90f",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfb1fa",
   "metadata": {},
   "source": [
    "| Method                             | Splitting Basis                                     | Chunk Size Type | Overlap    | Pros                                                                   | Cons                                                  | Best Use Case                                                         |\n",
    "| ---------------------------------- | --------------------------------------------------- | --------------- | ---------- | ---------------------------------------------------------------------- | ----------------------------------------------------- | --------------------------------------------------------------------- |\n",
    "| **CharacterTextSplitter**          | Characters or separators like spaces, newlines      | Characters      | Characters | Simple, fast, easy to implement                                        | May split sentences/words awkwardly, context loss     | Short/plain text or when token limits don’t matter                    |\n",
    "| **RecursiveCharacterTextSplitter** | Logical boundaries (paragraphs → sentences → words) | Characters      | Characters | Preserves semantic boundaries, context-aware, handles long text better | Slightly slower than simple character splitter        | Long documents, RAG pipelines, embedding for LLMs                     |\n",
    "| **TokenTextSplitter**              | Tokens (LLM tokenization)                           | Tokens          | Tokens     | Ensures chunks fit LLM token limits, overlap preserves context         | Needs tokenization, slower than character-based split | Embeddings, LLM processing, or any scenario where token limits matter |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-hello-world",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
