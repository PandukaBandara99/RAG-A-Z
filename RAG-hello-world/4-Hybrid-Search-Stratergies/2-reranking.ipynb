{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a23eff",
   "metadata": {},
   "source": [
    "### Reranking Hybrid Search Statergies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5fac5",
   "metadata": {},
   "source": [
    "Re-ranking is a second-stage filtering process in retrieval systems, especially in RAG pipelines, where we:\n",
    "\n",
    "1. First use a fast retriever (like BM25, FAISS, hybrid) to fetch top-k documents quickly.\n",
    "\n",
    "2. Then use a more accurate but slower model (like a cross-encoder or LLM) to re-score and reorder those documents by relevance to the query.\n",
    "\n",
    "üëâ It ensures that the most relevant documents appear at the top, improving the final answer from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5fd53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8243d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load text file\n",
    "loader=TextLoader(\"langchain_sample.txt\")\n",
    "raw_docs=loader.load()\n",
    "\n",
    "# Split text into document chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b203af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## user query\n",
    "query=\"How can i use langchain to build an application with memory and tools?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9720bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FAISS and Huggingface model Embeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(docs,embedding_model)\n",
    "retriever=vectorstore.as_retriever(search_kwargs={\"k\":8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332edeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OpenAI Embedding\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings=OpenAIEmbeddings()\n",
    "vectorstore_openai=FAISS.from_documents(docs,embeddings)\n",
    "retriever_openai=vectorstore_openai.as_retriever(search_kwargs={\"k\":8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7e52001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001BBFD92AC40>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49fc0a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001BBFD9729F0>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cd8773a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001BBFDAD0190>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001BBFDAD0690>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## prompt and use the llm\n",
    "from langchain.chat_models import init_chat_model\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "llm=init_chat_model(\"groq:gemma2-9b-it\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2b0c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user's question.\n",
    "\n",
    "User Question: \"{question}\"\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "- Think about the relevance of each document to the user's question.\n",
    "- Return a list of document indices in ranked order, starting from the most relevant.\n",
    "\n",
    "Output format: comma-separated document indices (e.g., 2,1,3,0,...)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8eed561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='709da153-eaee-411b-bd17-b5fc443be7da', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='a217edf2-c10c-40ff-9317-37c4e3388096', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='ec8fd256-9859-4aa8-becb-b505799be510', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='d075ce19-78fc-4392-bde0-0a6a8a45a7ec', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='86653dbf-1c3c-4621-81d3-5402b9e993bb', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(id='cb734e67-c32f-4873-848b-86ae0a156290', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs=retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ce0eeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Your task is to rank the following documents from most to least relevant to the user\\'s question.\\n\\nUser Question: \"{question}\"\\n\\nDocuments:\\n{documents}\\n\\nInstructions:\\n- Think about the relevance of each document to the user\\'s question.\\n- Return a list of document indices in ranked order, starting from the most relevant.\\n\\nOutput format: comma-separated document indices (e.g., 2,1,3,0,...)\\n')\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001BBFDAD0190>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001BBFDAD0690>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=prompt| llm | StrOutputParser()\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3278ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lines = [f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "formatted_docs = \"\\n\".join(doc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ecef652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.',\n",
       " '2. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.',\n",
       " '3. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.',\n",
       " '4. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.',\n",
       " '5. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.',\n",
       " '6. Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9882543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\\n2. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\\n3. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\\n4. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\\n5. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\\n6. Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7e76228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"2,1,4,5,6  \\n\\n\\n**Explanation:**\\n\\n* **Document 2** directly addresses the user's question by outlining LangChain's capabilities for building applications with LLMs, including memory and tools. \\n* **Document 1**  explains memory and tool integration within LangChain, which are key components of the user's query.\\n* **Document 4**  focuses on agents, a specific type of tool-using LangChain application, further relevant to the question.\\n* **Document 5** discusses Retrieval-Augmented Generation (RAG), another concept related to using tools and external knowledge within LangChain applications.\\n* **Document 6** touches on hybrid retrieval, which could be relevant to building applications that leverage tools for information retrieval.\\n\\n\\nThe other documents are less relevant as they focus on specific aspects of LangChain or related technologies without directly addressing the core question of building applications with memory and tools.\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"question\":query,\"documents\":formatted_docs})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3c0699d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 3, 4]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Parse and rerank\n",
    "indices = [int(x.strip()) - 1 for x in response.split(\",\") if x.strip().isdigit()]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "468339a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='709da153-eaee-411b-bd17-b5fc443be7da', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='a217edf2-c10c-40ff-9317-37c4e3388096', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='ec8fd256-9859-4aa8-becb-b505799be510', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='d075ce19-78fc-4392-bde0-0a6a8a45a7ec', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='86653dbf-1c3c-4621-81d3-5402b9e993bb', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'),\n",
       " Document(id='cb734e67-c32f-4873-848b-86ae0a156290', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\nLangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3d31bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a217edf2-c10c-40ff-9317-37c4e3388096', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='709da153-eaee-411b-bd17-b5fc443be7da', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\nMemory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='d075ce19-78fc-4392-bde0-0a6a8a45a7ec', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\nAgents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='86653dbf-1c3c-4621-81d3-5402b9e993bb', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\nBM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_docs = [retrieved_docs[i] for i in indices if 0 <= i < len(retrieved_docs)]\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e076f87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Reranked Results:\n",
      "\n",
      "Rank 1:\n",
      "LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "\n",
      "Rank 2:\n",
      "LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "\n",
      "Rank 3:\n",
      "FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\n",
      "Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n",
      "\n",
      "Rank 4:\n",
      "Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\n",
      "BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Show results\n",
    "print(\"\\nüìä Final Reranked Results:\")\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f\"\\nRank {i}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c96bf",
   "metadata": {},
   "source": [
    "## **Notes - Reranking in Information Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f3bde",
   "metadata": {},
   "source": [
    "> Reranking is an advanced retrieval technique used to **improve the relevance of search results** by applying a secondary ranking model to an initial set of candidate documents retrieved from a primary search system (such as TF-IDF, BM25, or a dense vector retriever like FAISS or Pinecone).\n",
    "It plays a crucial role in **Retrieval-Augmented Generation (RAG)** and **hybrid retrieval pipelines**, where initial recall is broad, and reranking helps refine precision.\n",
    "\n",
    "\n",
    "**1. What is Reranking?**\n",
    "\n",
    "Reranking is a **two-stage retrieval process**:\n",
    "\n",
    "1. **Initial Retrieval (Stage 1):**\n",
    "   A fast retriever (like BM25 or a dense vector retriever) fetches the *top N* most relevant documents based on similarity scores.\n",
    "\n",
    "2. **Reranking (Stage 2):**\n",
    "   A more sophisticated model (often transformer-based, such as **cross-encoders**) re-evaluates those N documents against the query to produce a **more accurate relevance score** and reorder them.\n",
    "\n",
    "This approach balances **efficiency and accuracy** ‚Äî the first stage is fast and broad, while the second stage is slow but precise.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Why Use Reranking?**\n",
    "\n",
    "* **Improves Precision:**\n",
    "  Filters out false positives from the first-stage retriever.\n",
    "\n",
    "* **Enhances Contextual Understanding:**\n",
    "  Neural rerankers (e.g., BERT-based models) understand semantic meaning beyond keyword overlap.\n",
    "\n",
    "* **Balances Performance:**\n",
    "  Running deep models on all documents is costly; reranking applies them only to a small candidate set.\n",
    "\n",
    "* **Improves RAG Outputs:**\n",
    "  For systems like LangChain RAG pipelines, reranking ensures only the most relevant context is passed to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Common Reranking Approaches**\n",
    "\n",
    "**a. Cross-Encoder Reranking**\n",
    "\n",
    "A **cross-encoder** takes both the query and document as joint input (concatenated) and computes a **single similarity score** through a transformer model.\n",
    "\n",
    "* **Example Models:**\n",
    "\n",
    "  * `cross-encoder/ms-marco-MiniLM-L-6-v2`\n",
    "  * `cross-encoder/ms-marco-TinyBERT-L-2-v2`\n",
    "  * `nli-deberta-v3-base`\n",
    "\n",
    "* **Advantages:**\n",
    "\n",
    "  * Highest accuracy\n",
    "  * Deep contextual understanding\n",
    "\n",
    "* **Disadvantages:**\n",
    "\n",
    "  * Computationally expensive\n",
    "  * Not scalable for large datasets\n",
    "\n",
    "---\n",
    "\n",
    "**b. Bi-Encoder + Cross-Encoder (Hybrid Reranking)**\n",
    "\n",
    "A **bi-encoder** first retrieves documents using vector embeddings (fast).\n",
    "Then, a **cross-encoder** reranks the top results.\n",
    "\n",
    "This hybrid approach is standard in modern systems like **ColBERT**, **Haystack**, or **LangChain retrievers**.\n",
    "\n",
    "* **Stage 1:** Dense retrieval using embeddings (e.g., BERT, OpenAI Embeddings)\n",
    "* **Stage 2:** Reranking using cross-encoder for top *k* results\n",
    "\n",
    "---\n",
    "\n",
    "**c. LLM-Based Reranking**\n",
    "\n",
    "Large Language Models (LLMs) such as **GPT-4**, **Claude**, or **Llama 3** can be used to rerank retrieved documents by **evaluating their semantic match** with the query.\n",
    "\n",
    "* **Prompt Example:**\n",
    "\n",
    "  ```\n",
    "  Given the query: \"Explain quantum entanglement\"\n",
    "  And the following documents, rank them in order of relevance.\n",
    "  ```\n",
    "\n",
    "* **Use Case:**\n",
    "  When explainability and reasoning-based ranking are required.\n",
    "\n",
    "* **Trade-off:**\n",
    "  Very high computational cost but superior contextual precision.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Reranking Metrics**\n",
    "\n",
    "When evaluating rerankers, common **information retrieval metrics** include:\n",
    "\n",
    "| Metric                                           | Description                                               |\n",
    "| ------------------------------------------------ | --------------------------------------------------------- |\n",
    "| **MRR (Mean Reciprocal Rank)**                   | Measures how high the first relevant document ranks.      |\n",
    "| **nDCG (Normalized Discounted Cumulative Gain)** | Evaluates ranking quality considering position relevance. |\n",
    "| **Precision@K**                                  | Fraction of top K retrieved items that are relevant.      |\n",
    "| **Recall@K**                                     | Fraction of relevant items retrieved among top K.         |\n",
    "\n",
    "---\n",
    "\n",
    "**5. Example Workflow in LangChain**\n",
    "\n",
    "```python\n",
    "from langchain.retrievers import BM25Retriever, CrossEncoderReranker\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Step 1: Initialize BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "\n",
    "# Step 2: Initialize cross-encoder reranker\n",
    "reranker = CrossEncoderReranker(model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "# Step 3: Wrap retriever with reranker\n",
    "hybrid_retriever = bm25_retriever | reranker  # LCEL pipeline-style\n",
    "\n",
    "# Step 4: Use in a RAG chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    retriever=hybrid_retriever,\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**6. Practical Applications**\n",
    "\n",
    "* **Search Engines:**\n",
    "  Rerank top 100 BM25 results using cross-encoder to improve click relevance.\n",
    "\n",
    "* **Question Answering (RAG):**\n",
    "  Improve retrieved document quality before passing to the LLM.\n",
    "\n",
    "* **E-commerce:**\n",
    "  Rerank product results based on semantic and behavioral similarity.\n",
    "\n",
    "* **Legal and Biomedical Search:**\n",
    "  Ensure retrieved documents are contextually aligned and domain-accurate.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Best Practices**\n",
    "\n",
    "* Use **dense retrievers for recall**, **cross-encoders for precision**.\n",
    "* Limit reranking to **top 50‚Äì200 candidates** to avoid latency spikes.\n",
    "* Precompute embeddings and cache intermediate results.\n",
    "* Evaluate performance regularly using **nDCG** and **Recall@K**.\n",
    "* Experiment with **hybrid rerankers** (dense + sparse + cross-encoder) for optimal balance.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Stage | Component | Model Type              | Purpose                                     |\n",
    "| ----- | --------- | ----------------------- | ------------------------------------------- |\n",
    "| 1Ô∏è‚É£   | Retriever | BM25 / Dense Embeddings | Fast candidate retrieval                    |\n",
    "| 2Ô∏è‚É£   | Reranker  | Cross-Encoder / LLM     | Accurate semantic ranking                   |\n",
    "| 3Ô∏è‚É£   | Generator | LLM / RAG               | Produces final answer using top-ranked docs |\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
